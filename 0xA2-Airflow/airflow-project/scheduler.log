nohup: ignoring input
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2023-09-10T12:32:59.874+0000[0m] {[34mexecutor_loader.py:[0m117} INFO[0m - Loaded executor: SequentialExecutor[0m
[2023-09-10 12:33:00 +0000] [10437] [INFO] Starting gunicorn 21.2.0
[2023-09-10 12:33:00 +0000] [10437] [INFO] Listening at: http://[::]:8793 (10437)
[2023-09-10 12:33:00 +0000] [10437] [INFO] Using worker: sync
[2023-09-10 12:33:00 +0000] [10438] [INFO] Booting worker with pid: 10438
[[34m2023-09-10T12:33:00.100+0000[0m] {[34mscheduler_job_runner.py:[0m798} INFO[0m - Starting the scheduler[0m
[[34m2023-09-10T12:33:00.103+0000[0m] {[34mscheduler_job_runner.py:[0m805} INFO[0m - Processing each file at most -1 times[0m
[2023-09-10 12:33:00 +0000] [10441] [INFO] Booting worker with pid: 10441
[[34m2023-09-10T12:33:00.112+0000[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 10442[0m
[[34m2023-09-10T12:33:00.116+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:33:00.124+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-09-10T12:33:00.195+0000] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-09-10T12:38:00.471+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:43:00.744+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:48:00.873+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:53:01.231+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:58:01.503+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:03:01.787+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:08:02.054+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:13:02.326+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:18:02.510+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:23:02.628+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:28:02.997+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:33:03.292+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:38:03.458+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:43:03.726+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:48:03.995+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:53:04.288+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:58:04.597+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:58:30.989+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 13:51:53+00:00: manual__2023-09-10T13:51:53+00:00, state:running, queued_at: 2023-09-10 13:51:53.445715+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T13:58:30.991+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 13:51:53+00:00, run_id=manual__2023-09-10T13:51:53+00:00, run_start_date=2023-09-10 13:58:30.936354+00:00, run_end_date=2023-09-10 13:58:30.990910+00:00, run_duration=0.054556, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 13:51:53+00:00, data_interval_end=2023-09-10 13:51:53+00:00, dag_hash=293888ec3e25412de58e13b922f598df[0m
[[34m2023-09-10T13:58:30.995+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T13:51:53+00:00, run_after=2023-09-11T13:51:53+00:00[0m
[[34m2023-09-10T13:58:31.007+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 13:53:18+00:00: manual__2023-09-10T13:53:18+00:00, state:running, queued_at: 2023-09-10 13:53:18.806127+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T13:58:31.007+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 13:53:18+00:00, run_id=manual__2023-09-10T13:53:18+00:00, run_start_date=2023-09-10 13:58:30.936878+00:00, run_end_date=2023-09-10 13:58:31.007671+00:00, run_duration=0.070793, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 13:53:18+00:00, data_interval_end=2023-09-10 13:53:18+00:00, dag_hash=293888ec3e25412de58e13b922f598df[0m
[[34m2023-09-10T13:58:31.010+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T13:53:18+00:00, run_after=2023-09-11T13:53:18+00:00[0m
[[34m2023-09-10T14:00:51.810+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 14:00:50.536342+00:00: manual__2023-09-10T14:00:50.536342+00:00, state:running, queued_at: 2023-09-10 14:00:50.595656+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T14:00:51.810+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 14:00:50.536342+00:00, run_id=manual__2023-09-10T14:00:50.536342+00:00, run_start_date=2023-09-10 14:00:51.795208+00:00, run_end_date=2023-09-10 14:00:51.810792+00:00, run_duration=0.015584, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 14:00:50.536342+00:00, data_interval_end=2023-09-10 14:00:50.536342+00:00, dag_hash=293888ec3e25412de58e13b922f598df[0m
[[34m2023-09-10T14:00:51.813+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T14:00:50.536342+00:00, run_after=2023-09-11T14:00:50.536342+00:00[0m
[[34m2023-09-10T14:01:14.394+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 14:01:13.650498+00:00: manual__2023-09-10T14:01:13.650498+00:00, state:running, queued_at: 2023-09-10 14:01:13.681643+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T14:01:14.395+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 14:01:13.650498+00:00, run_id=manual__2023-09-10T14:01:13.650498+00:00, run_start_date=2023-09-10 14:01:14.344663+00:00, run_end_date=2023-09-10 14:01:14.394954+00:00, run_duration=0.050291, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 14:01:13.650498+00:00, data_interval_end=2023-09-10 14:01:13.650498+00:00, dag_hash=293888ec3e25412de58e13b922f598df[0m
[[34m2023-09-10T14:01:14.398+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T14:01:13.650498+00:00, run_after=2023-09-11T14:01:13.650498+00:00[0m
[[34m2023-09-10T14:03:04.875+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:08:05.174+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:13:05.281+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-10T14:17:59.579+0000] {manager.py:543} INFO - DAG etl_workflow is missing and will be deactivated.
[2023-09-10T14:17:59.587+0000] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2023-09-10T14:17:59.591+0000] {manager.py:557} INFO - Deleted DAG etl_workflow in serialized_dag table
[[34m2023-09-10T14:18:05.616+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:23:05.779+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:28:05.951+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:33:06.155+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:36:46.212+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 14:36:45.940783+00:00: manual__2023-09-10T14:36:45.940783+00:00, state:running, queued_at: 2023-09-10 14:36:45.989405+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T14:36:46.213+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 14:36:45.940783+00:00, run_id=manual__2023-09-10T14:36:45.940783+00:00, run_start_date=2023-09-10 14:36:46.196161+00:00, run_end_date=2023-09-10 14:36:46.213120+00:00, run_duration=0.016959, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 14:36:45.940783+00:00, data_interval_end=2023-09-10 14:36:45.940783+00:00, dag_hash=79b760373eec0d82222a80c0bd784dc3[0m
[[34m2023-09-10T14:36:46.217+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00[0m
[[34m2023-09-10T14:38:06.446+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-10 16:10:17 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:10438)
[2023-09-10 16:10:17 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:10441)
[2023-09-10 16:10:17 +0000] [10438] [INFO] Worker exiting (pid: 10438)
[2023-09-10 16:10:17 +0000] [10441] [INFO] Worker exiting (pid: 10441)
[2023-09-10 16:10:17 +0000] [10437] [ERROR] Worker (pid:10438) exited with code 1
[2023-09-10 16:10:17 +0000] [10437] [ERROR] Worker (pid:10438) exited with code 1.
[2023-09-10 16:10:17 +0000] [10437] [ERROR] Worker (pid:10441) exited with code 1
[2023-09-10 16:10:17 +0000] [10437] [ERROR] Worker (pid:10441) exited with code 1.
[2023-09-10 16:10:17 +0000] [23714] [INFO] Booting worker with pid: 23714
[2023-09-10 16:10:17 +0000] [23716] [INFO] Booting worker with pid: 23716
[[34m2023-09-10T16:10:53.960+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:15:54.100+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:20:54.256+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:25:54.541+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:30:54.810+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:35:55.087+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:40:55.135+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:45:55.449+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:50:55.631+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:55:55.922+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:00:56.230+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:05:56.631+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:10:56.977+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:15:57.406+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:20:57.826+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:25:57.994+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:30:58.432+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:35:58.724+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:40:59.084+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:45:59.351+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:50:59.418+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:56:00.116+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:01:00.264+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:06:00.461+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:11:01.049+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:16:01.200+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:21:02.100+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:26:02.386+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:31:02.558+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:36:02.881+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-11 06:26:29 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:23714)
[2023-09-11 06:26:29 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:23716)
[2023-09-11 06:26:29 +0000] [23714] [INFO] Worker exiting (pid: 23714)
[2023-09-11 06:26:29 +0000] [23716] [INFO] Worker exiting (pid: 23716)
[2023-09-11 06:26:29 +0000] [10437] [ERROR] Worker (pid:23716) exited with code 1
[2023-09-11 06:26:29 +0000] [10437] [ERROR] Worker (pid:23716) exited with code 1.
[2023-09-11 06:26:29 +0000] [10437] [ERROR] Worker (pid:23714) exited with code 1
[2023-09-11 06:26:29 +0000] [10437] [ERROR] Worker (pid:23714) exited with code 1.
[2023-09-11 06:26:29 +0000] [38657] [INFO] Booting worker with pid: 38657
[2023-09-11 06:26:29 +0000] [38661] [INFO] Booting worker with pid: 38661
[2023-09-11T06:26:30.274+0000] {manager.py:543} INFO - DAG example_external_task_marker_parent is missing and will be deactivated.
[2023-09-11T06:26:30.275+0000] {manager.py:543} INFO - DAG example_external_task_marker_child is missing and will be deactivated.
[2023-09-11T06:26:30.329+0000] {manager.py:553} INFO - Deactivated 2 DAGs which are no longer present in file.
[2023-09-11T06:26:30.354+0000] {manager.py:557} INFO - Deleted DAG example_external_task_marker_parent in serialized_dag table
[2023-09-11T06:26:30.452+0000] {manager.py:557} INFO - Deleted DAG example_external_task_marker_child in serialized_dag table
[[34m2023-09-11T06:29:55.500+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:34:55.769+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:39:40.383+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-02T00:00:00+00:00, run_after=2023-01-03T00:00:00+00:00[0m
[[34m2023-09-11T06:39:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:40.441+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:40.442+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:40.442+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:40.445+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:42.360+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:39:42.501+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:42.680+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:42.713+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:43.190+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:39:43.190+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:43.259+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:39:43.314+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:39:44.106+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:39:44.123+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:39:43.470130+00:00, run_end_date=2023-09-11 06:39:43.695420+00:00, run_duration=0.22529, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:40.439366+00:00, queued_by_job_id=2, pid=40739[0m
[[34m2023-09-11T06:39:44.477+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00[0m
[[34m2023-09-11T06:39:44.510+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-01 00:00:00+00:00: scheduled__2023-01-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:40.375379+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:39:44.511+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-01 00:00:00+00:00, run_id=scheduled__2023-01-01T00:00:00+00:00, run_start_date=2023-09-11 06:39:40.406312+00:00, run_end_date=2023-09-11 06:39:44.511186+00:00, run_duration=4.104874, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-01 00:00:00+00:00, data_interval_end=2023-01-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:39:44.515+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-02T00:00:00+00:00, run_after=2023-01-03T00:00:00+00:00[0m
[[34m2023-09-11T06:39:44.529+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:44.530+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:44.530+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:44.532+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:44.533+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:44.533+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:44.536+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:46.492+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:39:46.627+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:46.798+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:46.832+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:47.299+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:39:47.300+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:47.373+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:39:47.435+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:39:48.235+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:39:48.246+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:39:47.531289+00:00, run_end_date=2023-09-11 06:39:47.764989+00:00, run_duration=0.2337, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:44.531224+00:00, queued_by_job_id=2, pid=40749[0m
[[34m2023-09-11T06:39:48.491+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00[0m
[[34m2023-09-11T06:39:48.513+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-02 00:00:00+00:00: scheduled__2023-01-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:44.472358+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:39:48.513+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-02 00:00:00+00:00, run_id=scheduled__2023-01-02T00:00:00+00:00, run_start_date=2023-09-11 06:39:44.489014+00:00, run_end_date=2023-09-11 06:39:48.513855+00:00, run_duration=4.024841, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-02 00:00:00+00:00, data_interval_end=2023-01-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:39:48.517+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00[0m
[[34m2023-09-11T06:39:49.551+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00[0m
[[34m2023-09-11T06:39:49.594+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:49.594+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:49.594+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:49.596+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:49.597+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:49.597+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:49.600+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:51.528+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:39:51.675+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:51.860+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:51.898+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:52.445+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:39:52.446+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:52.524+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:39:52.590+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:39:53.402+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:39:53.413+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:39:52.685003+00:00, run_end_date=2023-09-11 06:39:52.931325+00:00, run_duration=0.246322, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:49.595321+00:00, queued_by_job_id=2, pid=40762[0m
[[34m2023-09-11T06:39:53.575+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-05T00:00:00+00:00, run_after=2023-01-06T00:00:00+00:00[0m
[[34m2023-09-11T06:39:53.613+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-03 00:00:00+00:00: scheduled__2023-01-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:49.546924+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:39:53.614+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-03 00:00:00+00:00, run_id=scheduled__2023-01-03T00:00:00+00:00, run_start_date=2023-09-11 06:39:49.562931+00:00, run_end_date=2023-09-11 06:39:53.614463+00:00, run_duration=4.051532, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-03 00:00:00+00:00, data_interval_end=2023-01-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:39:53.619+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00[0m
[[34m2023-09-11T06:39:53.635+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:53.635+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:53.635+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:53.638+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:53.638+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:53.639+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:53.641+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:55.788+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:39:55.946+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:56.150+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:56.187+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:56.684+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:39:56.685+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:56.759+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:39:56.819+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:39:57.593+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:39:57.605+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:39:56.904209+00:00, run_end_date=2023-09-11 06:39:57.158652+00:00, run_duration=0.254443, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:53.636711+00:00, queued_by_job_id=2, pid=40771[0m
[[34m2023-09-11T06:39:57.626+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:39:57.876+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-05T00:00:00+00:00, run_after=2023-01-06T00:00:00+00:00[0m
[[34m2023-09-11T06:39:57.899+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-04 00:00:00+00:00: scheduled__2023-01-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:53.570301+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:39:57.899+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-04 00:00:00+00:00, run_id=scheduled__2023-01-04T00:00:00+00:00, run_start_date=2023-09-11 06:39:53.588110+00:00, run_end_date=2023-09-11 06:39:57.899457+00:00, run_duration=4.311347, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-04 00:00:00+00:00, data_interval_end=2023-01-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:39:57.903+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-05T00:00:00+00:00, run_after=2023-01-06T00:00:00+00:00[0m
[[34m2023-09-11T06:39:58.587+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-06T00:00:00+00:00, run_after=2023-01-07T00:00:00+00:00[0m
[[34m2023-09-11T06:39:58.661+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:58.661+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:58.662+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:58.663+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:58.664+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:58.664+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:58.667+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:00.630+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:00.774+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:00.969+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:01.006+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:01.528+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:01.529+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:01.609+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:01.672+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:02.482+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:02.495+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:01.765217+00:00, run_end_date=2023-09-11 06:40:02.029368+00:00, run_duration=0.264151, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:58.662661+00:00, queued_by_job_id=2, pid=40784[0m
[[34m2023-09-11T06:40:02.662+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-07T00:00:00+00:00, run_after=2023-01-08T00:00:00+00:00[0m
[[34m2023-09-11T06:40:02.705+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-05 00:00:00+00:00: scheduled__2023-01-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:58.582250+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:02.705+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-05 00:00:00+00:00, run_id=scheduled__2023-01-05T00:00:00+00:00, run_start_date=2023-09-11 06:39:58.627868+00:00, run_end_date=2023-09-11 06:40:02.705422+00:00, run_duration=4.077554, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-05 00:00:00+00:00, data_interval_end=2023-01-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:02.709+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-06T00:00:00+00:00, run_after=2023-01-07T00:00:00+00:00[0m
[[34m2023-09-11T06:40:02.727+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:02.728+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:02.728+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:02.731+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:02.731+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:02.731+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:02.734+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:04.758+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:04.894+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:05.058+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:05.090+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:05.552+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:05.553+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:05.623+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:05.680+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:06.446+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:06.457+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:05.766569+00:00, run_end_date=2023-09-11 06:40:06.013298+00:00, run_duration=0.246729, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:02.729258+00:00, queued_by_job_id=2, pid=40791[0m
[[34m2023-09-11T06:40:06.659+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-07T00:00:00+00:00, run_after=2023-01-08T00:00:00+00:00[0m
[[34m2023-09-11T06:40:06.695+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-06 00:00:00+00:00: scheduled__2023-01-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:02.656912+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:06.696+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-06 00:00:00+00:00, run_id=scheduled__2023-01-06T00:00:00+00:00, run_start_date=2023-09-11 06:40:02.679593+00:00, run_end_date=2023-09-11 06:40:06.696035+00:00, run_duration=4.016442, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-06 00:00:00+00:00, data_interval_end=2023-01-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:06.699+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-07T00:00:00+00:00, run_after=2023-01-08T00:00:00+00:00[0m
[[34m2023-09-11T06:40:06.714+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number manual__2023-09-11T06:40:03+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:06.714+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:06.714+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number manual__2023-09-11T06:40:03+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:06.716+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:06.717+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='manual__2023-09-11T06:40:03+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:06.717+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'manual__2023-09-11T06:40:03+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:06.720+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'manual__2023-09-11T06:40:03+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:08.554+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:08.681+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:08.851+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:08.883+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:09.372+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:09.373+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:09.447+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=manual__2023-09-11T06:40:03+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:09.506+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number manual__2023-09-11T06:40:03+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:10.227+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='manual__2023-09-11T06:40:03+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:10.238+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=manual__2023-09-11T06:40:03+00:00, map_index=-1, run_start_date=2023-09-11 06:40:09.590795+00:00, run_end_date=2023-09-11 06:40:09.809267+00:00, run_duration=0.218472, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:06.715432+00:00, queued_by_job_id=2, pid=40798[0m
[[34m2023-09-11T06:40:10.452+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-08T00:00:00+00:00, run_after=2023-01-09T00:00:00+00:00[0m
[[34m2023-09-11T06:40:10.486+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-11 06:40:03+00:00: manual__2023-09-11T06:40:03+00:00, state:running, queued_at: 2023-09-11 06:40:03.754595+00:00. externally triggered: True> successful[0m
[[34m2023-09-11T06:40:10.487+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-11 06:40:03+00:00, run_id=manual__2023-09-11T06:40:03+00:00, run_start_date=2023-09-11 06:40:06.672701+00:00, run_end_date=2023-09-11 06:40:10.487222+00:00, run_duration=3.814521, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-10 06:40:03+00:00, data_interval_end=2023-09-11 06:40:03+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:10.490+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-11T06:40:03+00:00, run_after=2023-09-12T06:40:03+00:00[0m
[[34m2023-09-11T06:40:10.506+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:10.507+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:10.507+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:10.509+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:10.510+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:10.510+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:10.512+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:12.345+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:12.480+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:12.655+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:12.688+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:13.152+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:13.153+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:13.224+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:13.282+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:14.046+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:14.057+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:13.369343+00:00, run_end_date=2023-09-11 06:40:13.607271+00:00, run_duration=0.237928, state=success, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:10.508093+00:00, queued_by_job_id=2, pid=40807[0m
[[34m2023-09-11T06:40:14.483+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-07 00:00:00+00:00: scheduled__2023-01-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:10.447471+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:14.483+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-07 00:00:00+00:00, run_id=scheduled__2023-01-07T00:00:00+00:00, run_start_date=2023-09-11 06:40:10.465538+00:00, run_end_date=2023-09-11 06:40:14.483586+00:00, run_duration=4.018048, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-07 00:00:00+00:00, data_interval_end=2023-01-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:14.487+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-08T00:00:00+00:00, run_after=2023-01-09T00:00:00+00:00[0m
[[34m2023-09-11T06:40:15.632+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-09T00:00:00+00:00, run_after=2023-01-10T00:00:00+00:00[0m
[[34m2023-09-11T06:40:15.709+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:15.709+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:15.710+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:15.713+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:15.713+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:15.714+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:15.721+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:17.748+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:17.875+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:18.046+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:18.078+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:18.548+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:18.549+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:18.621+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:18.681+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:19.418+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:19.428+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:18.766916+00:00, run_end_date=2023-09-11 06:40:18.982970+00:00, run_duration=0.216054, state=success, executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:15.711168+00:00, queued_by_job_id=2, pid=40817[0m
[[34m2023-09-11T06:40:19.710+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-10T00:00:00+00:00, run_after=2023-01-11T00:00:00+00:00[0m
[[34m2023-09-11T06:40:19.746+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-08 00:00:00+00:00: scheduled__2023-01-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:15.625747+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:19.747+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-08 00:00:00+00:00, run_id=scheduled__2023-01-08T00:00:00+00:00, run_start_date=2023-09-11 06:40:15.655804+00:00, run_end_date=2023-09-11 06:40:19.746987+00:00, run_duration=4.091183, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-08 00:00:00+00:00, data_interval_end=2023-01-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:19.750+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-09T00:00:00+00:00, run_after=2023-01-10T00:00:00+00:00[0m
[[34m2023-09-11T06:40:19.766+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:19.766+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:19.766+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:19.768+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:19.769+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:19.769+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:19.772+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:21.708+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:21.867+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:22.099+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:22.141+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:22.619+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:22.619+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:22.687+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:22.743+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:23.488+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:23.498+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:22.845132+00:00, run_end_date=2023-09-11 06:40:23.071824+00:00, run_duration=0.226692, state=success, executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:19.767329+00:00, queued_by_job_id=2, pid=40826[0m
[[34m2023-09-11T06:40:23.743+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-10T00:00:00+00:00, run_after=2023-01-11T00:00:00+00:00[0m
[[34m2023-09-11T06:40:23.765+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-09 00:00:00+00:00: scheduled__2023-01-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:19.703175+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:23.766+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-09 00:00:00+00:00, run_id=scheduled__2023-01-09T00:00:00+00:00, run_start_date=2023-09-11 06:40:19.723873+00:00, run_end_date=2023-09-11 06:40:23.766280+00:00, run_duration=4.042407, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-09 00:00:00+00:00, data_interval_end=2023-01-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:23.770+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-10T00:00:00+00:00, run_after=2023-01-11T00:00:00+00:00[0m
[[34m2023-09-11T06:40:24.723+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-11T00:00:00+00:00, run_after=2023-01-12T00:00:00+00:00[0m
[[34m2023-09-11T06:40:24.773+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:24.773+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:24.773+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:24.775+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:24.776+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:24.776+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:24.779+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:26.710+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:26.864+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:27.049+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:27.082+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:27.560+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:27.561+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:27.630+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:27.687+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:28.414+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:28.425+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:27.777781+00:00, run_end_date=2023-09-11 06:40:28.002600+00:00, run_duration=0.224819, state=success, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:24.774386+00:00, queued_by_job_id=2, pid=40836[0m
[[34m2023-09-11T06:40:28.688+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-12T00:00:00+00:00, run_after=2023-01-13T00:00:00+00:00[0m
[[34m2023-09-11T06:40:28.742+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-10 00:00:00+00:00: scheduled__2023-01-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:24.716530+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:28.743+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-10 00:00:00+00:00, run_id=scheduled__2023-01-10T00:00:00+00:00, run_start_date=2023-09-11 06:40:24.739065+00:00, run_end_date=2023-09-11 06:40:28.742946+00:00, run_duration=4.003881, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-10 00:00:00+00:00, data_interval_end=2023-01-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:28.746+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-11T00:00:00+00:00, run_after=2023-01-12T00:00:00+00:00[0m
[[34m2023-09-11T06:40:28.762+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:28.762+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:28.762+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:28.764+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:28.765+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:28.765+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:28.768+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:30.683+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:30.811+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:30.985+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:31.018+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:31.507+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:31.508+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:31.581+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:31.645+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:32.641+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:32.652+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:31.732479+00:00, run_end_date=2023-09-11 06:40:32.058965+00:00, run_duration=0.326486, state=success, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:28.763398+00:00, queued_by_job_id=2, pid=40845[0m
[[34m2023-09-11T06:40:32.908+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-12T00:00:00+00:00, run_after=2023-01-13T00:00:00+00:00[0m
[[34m2023-09-11T06:40:32.931+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-11 00:00:00+00:00: scheduled__2023-01-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:28.683710+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:32.932+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-11 00:00:00+00:00, run_id=scheduled__2023-01-11T00:00:00+00:00, run_start_date=2023-09-11 06:40:28.703038+00:00, run_end_date=2023-09-11 06:40:32.931946+00:00, run_duration=4.228908, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-11 00:00:00+00:00, data_interval_end=2023-01-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:32.935+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-12T00:00:00+00:00, run_after=2023-01-13T00:00:00+00:00[0m
[[34m2023-09-11T06:40:33.771+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-13T00:00:00+00:00, run_after=2023-01-14T00:00:00+00:00[0m
[[34m2023-09-11T06:40:33.821+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:33.821+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:33.822+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:33.824+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:33.824+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:33.825+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:33.828+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:35.727+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:35.853+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:36.030+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:36.063+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:36.590+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:36.591+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:36.674+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:36.735+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:37.585+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:37.596+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:36.837129+00:00, run_end_date=2023-09-11 06:40:37.080420+00:00, run_duration=0.243291, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:33.822666+00:00, queued_by_job_id=2, pid=40855[0m
[[34m2023-09-11T06:40:37.891+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-14T00:00:00+00:00, run_after=2023-01-15T00:00:00+00:00[0m
[[34m2023-09-11T06:40:37.957+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-12 00:00:00+00:00: scheduled__2023-01-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:33.764241+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:37.958+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-12 00:00:00+00:00, run_id=scheduled__2023-01-12T00:00:00+00:00, run_start_date=2023-09-11 06:40:33.786292+00:00, run_end_date=2023-09-11 06:40:37.957898+00:00, run_duration=4.171606, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-12 00:00:00+00:00, data_interval_end=2023-01-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:37.962+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-13T00:00:00+00:00, run_after=2023-01-14T00:00:00+00:00[0m
[[34m2023-09-11T06:40:37.982+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:37.983+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:37.983+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:37.986+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:37.987+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:37.987+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:37.990+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:40.219+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:40.353+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:40.541+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:40.574+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:41.077+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:41.078+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:41.161+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:41.228+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:42.071+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:42.082+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:41.363874+00:00, run_end_date=2023-09-11 06:40:41.614110+00:00, run_duration=0.250236, state=success, executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:37.985040+00:00, queued_by_job_id=2, pid=40862[0m
[[34m2023-09-11T06:40:42.327+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-14T00:00:00+00:00, run_after=2023-01-15T00:00:00+00:00[0m
[[34m2023-09-11T06:40:42.349+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-13 00:00:00+00:00: scheduled__2023-01-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:37.882307+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:42.350+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-13 00:00:00+00:00, run_id=scheduled__2023-01-13T00:00:00+00:00, run_start_date=2023-09-11 06:40:37.921144+00:00, run_end_date=2023-09-11 06:40:42.350165+00:00, run_duration=4.429021, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-13 00:00:00+00:00, data_interval_end=2023-01-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:42.353+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-14T00:00:00+00:00, run_after=2023-01-15T00:00:00+00:00[0m
[[34m2023-09-11T06:40:42.859+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-15T00:00:00+00:00, run_after=2023-01-16T00:00:00+00:00[0m
[[34m2023-09-11T06:40:42.904+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:42.904+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:42.904+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:42.906+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:42.907+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:42.907+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:42.910+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:44.764+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:44.938+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:45.183+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:45.231+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:45.769+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:45.770+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:45.875+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:45.937+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:46.762+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:46.773+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:46.028528+00:00, run_end_date=2023-09-11 06:40:46.265201+00:00, run_duration=0.236673, state=success, executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:42.905336+00:00, queued_by_job_id=2, pid=40872[0m
[[34m2023-09-11T06:40:47.034+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-16T00:00:00+00:00, run_after=2023-01-17T00:00:00+00:00[0m
[[34m2023-09-11T06:40:47.071+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-14 00:00:00+00:00: scheduled__2023-01-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:42.854457+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:47.072+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-14 00:00:00+00:00, run_id=scheduled__2023-01-14T00:00:00+00:00, run_start_date=2023-09-11 06:40:42.872587+00:00, run_end_date=2023-09-11 06:40:47.071990+00:00, run_duration=4.199403, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-14 00:00:00+00:00, data_interval_end=2023-01-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:47.075+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-15T00:00:00+00:00, run_after=2023-01-16T00:00:00+00:00[0m
[[34m2023-09-11T06:40:47.090+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:47.090+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:47.091+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:47.093+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:47.093+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:47.094+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:47.097+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:49.027+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:49.168+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:49.351+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:49.383+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:49.859+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:49.860+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:49.937+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:49.996+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:50.727+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:50.737+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:50.085209+00:00, run_end_date=2023-09-11 06:40:50.303685+00:00, run_duration=0.218476, state=success, executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:47.091668+00:00, queued_by_job_id=2, pid=40879[0m
[[34m2023-09-11T06:40:51.092+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-16T00:00:00+00:00, run_after=2023-01-17T00:00:00+00:00[0m
[[34m2023-09-11T06:40:51.116+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-15 00:00:00+00:00: scheduled__2023-01-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:47.029557+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:51.117+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-15 00:00:00+00:00, run_id=scheduled__2023-01-15T00:00:00+00:00, run_start_date=2023-09-11 06:40:47.047694+00:00, run_end_date=2023-09-11 06:40:51.117133+00:00, run_duration=4.069439, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-15 00:00:00+00:00, data_interval_end=2023-01-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:51.120+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-16T00:00:00+00:00, run_after=2023-01-17T00:00:00+00:00[0m
[[34m2023-09-11T06:40:52.117+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-17T00:00:00+00:00, run_after=2023-01-18T00:00:00+00:00[0m
[[34m2023-09-11T06:40:52.160+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:52.161+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:52.161+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:52.163+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:52.164+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:52.164+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:52.167+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:54.380+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:54.531+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:54.729+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:54.763+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:55.308+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:55.309+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:55.383+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:55.444+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:56.244+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:56.255+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:55.532361+00:00, run_end_date=2023-09-11 06:40:55.773782+00:00, run_duration=0.241421, state=success, executor_state=success, try_number=1, max_tries=0, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:52.162310+00:00, queued_by_job_id=2, pid=40889[0m
[[34m2023-09-11T06:40:56.594+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-18T00:00:00+00:00, run_after=2023-01-19T00:00:00+00:00[0m
[[34m2023-09-11T06:40:56.637+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-16 00:00:00+00:00: scheduled__2023-01-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:52.112800+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:56.638+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-16 00:00:00+00:00, run_id=scheduled__2023-01-16T00:00:00+00:00, run_start_date=2023-09-11 06:40:52.129610+00:00, run_end_date=2023-09-11 06:40:56.638185+00:00, run_duration=4.508575, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-16 00:00:00+00:00, data_interval_end=2023-01-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:56.643+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-17T00:00:00+00:00, run_after=2023-01-18T00:00:00+00:00[0m
[[34m2023-09-11T06:40:56.659+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:56.660+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:56.660+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:56.662+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:56.663+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:56.663+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:56.666+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:58.875+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:59.037+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:59.221+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:59.258+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:59.806+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:59.807+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:59.923+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:59.990+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:00.805+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:00.816+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:00.086493+00:00, run_end_date=2023-09-11 06:41:00.327629+00:00, run_duration=0.241136, state=success, executor_state=success, try_number=1, max_tries=0, job_id=21, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:56.661219+00:00, queued_by_job_id=2, pid=40898[0m
[[34m2023-09-11T06:41:00.970+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-18T00:00:00+00:00, run_after=2023-01-19T00:00:00+00:00[0m
[[34m2023-09-11T06:41:00.998+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-17 00:00:00+00:00: scheduled__2023-01-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:56.587699+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:00.999+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-17 00:00:00+00:00, run_id=scheduled__2023-01-17T00:00:00+00:00, run_start_date=2023-09-11 06:40:56.609923+00:00, run_end_date=2023-09-11 06:41:00.999415+00:00, run_duration=4.389492, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-17 00:00:00+00:00, data_interval_end=2023-01-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:01.002+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-18T00:00:00+00:00, run_after=2023-01-19T00:00:00+00:00[0m
[[34m2023-09-11T06:41:01.600+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-19T00:00:00+00:00, run_after=2023-01-20T00:00:00+00:00[0m
[[34m2023-09-11T06:41:01.652+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:01.653+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:01.653+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:01.655+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:01.655+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:01.656+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:01.658+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:03.554+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:03.688+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:03.861+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:03.899+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:04.375+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:04.376+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:04.458+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:04.556+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:05.376+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:05.387+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:04.677219+00:00, run_end_date=2023-09-11 06:41:04.959552+00:00, run_duration=0.282333, state=success, executor_state=success, try_number=1, max_tries=0, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:01.654040+00:00, queued_by_job_id=2, pid=40908[0m
[[34m2023-09-11T06:41:05.663+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-20T00:00:00+00:00, run_after=2023-01-21T00:00:00+00:00[0m
[[34m2023-09-11T06:41:05.710+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-18 00:00:00+00:00: scheduled__2023-01-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:01.595195+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:05.711+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-18 00:00:00+00:00, run_id=scheduled__2023-01-18T00:00:00+00:00, run_start_date=2023-09-11 06:41:01.614044+00:00, run_end_date=2023-09-11 06:41:05.711324+00:00, run_duration=4.09728, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-18 00:00:00+00:00, data_interval_end=2023-01-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:05.718+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-19T00:00:00+00:00, run_after=2023-01-20T00:00:00+00:00[0m
[[34m2023-09-11T06:41:05.734+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:05.734+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:05.734+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:05.736+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:05.737+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:05.737+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:05.740+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:07.787+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:07.924+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:08.102+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:08.139+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:08.627+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:08.627+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:08.702+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:08.761+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:09.607+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:09.622+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:08.847537+00:00, run_end_date=2023-09-11 06:41:09.103358+00:00, run_duration=0.255821, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:05.735212+00:00, queued_by_job_id=2, pid=40917[0m
[[34m2023-09-11T06:41:09.778+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-20T00:00:00+00:00, run_after=2023-01-21T00:00:00+00:00[0m
[[34m2023-09-11T06:41:09.801+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-19 00:00:00+00:00: scheduled__2023-01-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:05.658376+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:09.801+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-19 00:00:00+00:00, run_id=scheduled__2023-01-19T00:00:00+00:00, run_start_date=2023-09-11 06:41:05.679056+00:00, run_end_date=2023-09-11 06:41:09.801776+00:00, run_duration=4.12272, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-19 00:00:00+00:00, data_interval_end=2023-01-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:09.805+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-20T00:00:00+00:00, run_after=2023-01-21T00:00:00+00:00[0m
[[34m2023-09-11T06:41:10.627+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-21T00:00:00+00:00, run_after=2023-01-22T00:00:00+00:00[0m
[[34m2023-09-11T06:41:10.686+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:10.687+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:10.687+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:10.690+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:10.691+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:10.691+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:10.694+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:13.896+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:14.151+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:14.624+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:14.813+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:15.748+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:15.750+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:15.879+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:15.996+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:16.930+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:16.941+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:16.096727+00:00, run_end_date=2023-09-11 06:41:16.471293+00:00, run_duration=0.374566, state=success, executor_state=success, try_number=1, max_tries=0, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:10.688292+00:00, queued_by_job_id=2, pid=40929[0m
[[34m2023-09-11T06:41:17.183+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-22T00:00:00+00:00, run_after=2023-01-23T00:00:00+00:00[0m
[[34m2023-09-11T06:41:17.224+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-20 00:00:00+00:00: scheduled__2023-01-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:10.623020+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:17.225+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-20 00:00:00+00:00, run_id=scheduled__2023-01-20T00:00:00+00:00, run_start_date=2023-09-11 06:41:10.643249+00:00, run_end_date=2023-09-11 06:41:17.224922+00:00, run_duration=6.581673, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-20 00:00:00+00:00, data_interval_end=2023-01-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:17.228+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-21T00:00:00+00:00, run_after=2023-01-22T00:00:00+00:00[0m
[[34m2023-09-11T06:41:17.244+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:17.244+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:17.244+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:17.249+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:17.250+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:17.250+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:17.252+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:19.885+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:20.062+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:20.278+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:20.321+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:21.530+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:21.532+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:21.633+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:21.698+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:22.751+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:22.762+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:21.799723+00:00, run_end_date=2023-09-11 06:41:22.050770+00:00, run_duration=0.251047, state=success, executor_state=success, try_number=1, max_tries=0, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:17.245608+00:00, queued_by_job_id=2, pid=40940[0m
[[34m2023-09-11T06:41:23.046+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-22T00:00:00+00:00, run_after=2023-01-23T00:00:00+00:00[0m
[[34m2023-09-11T06:41:23.075+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-21 00:00:00+00:00: scheduled__2023-01-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:17.178757+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:23.076+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-21 00:00:00+00:00, run_id=scheduled__2023-01-21T00:00:00+00:00, run_start_date=2023-09-11 06:41:17.197039+00:00, run_end_date=2023-09-11 06:41:23.076388+00:00, run_duration=5.879349, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-21 00:00:00+00:00, data_interval_end=2023-01-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:23.083+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-22T00:00:00+00:00, run_after=2023-01-23T00:00:00+00:00[0m
[[34m2023-09-11T06:41:24.359+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-23T00:00:00+00:00, run_after=2023-01-24T00:00:00+00:00[0m
[[34m2023-09-11T06:41:24.407+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:24.408+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:24.408+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:24.410+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:24.411+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:24.411+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:24.414+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:27.511+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:27.701+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:27.918+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:27.981+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:28.570+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:28.571+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:28.669+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:28.778+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:29.611+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:29.627+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:28.886801+00:00, run_end_date=2023-09-11 06:41:29.144110+00:00, run_duration=0.257309, state=success, executor_state=success, try_number=1, max_tries=0, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:24.409225+00:00, queued_by_job_id=2, pid=40950[0m
[[34m2023-09-11T06:41:29.907+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-24T00:00:00+00:00, run_after=2023-01-25T00:00:00+00:00[0m
[[34m2023-09-11T06:41:29.945+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-22 00:00:00+00:00: scheduled__2023-01-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:24.355111+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:29.945+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-22 00:00:00+00:00, run_id=scheduled__2023-01-22T00:00:00+00:00, run_start_date=2023-09-11 06:41:24.373918+00:00, run_end_date=2023-09-11 06:41:29.945858+00:00, run_duration=5.57194, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-22 00:00:00+00:00, data_interval_end=2023-01-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:29.949+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-23T00:00:00+00:00, run_after=2023-01-24T00:00:00+00:00[0m
[[34m2023-09-11T06:41:29.964+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:29.965+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:29.965+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:29.967+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:29.967+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:29.968+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:29.970+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:32.080+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:32.214+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:32.390+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:32.442+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:32.927+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:32.928+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:32.999+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:33.057+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:33.909+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:33.923+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:33.179532+00:00, run_end_date=2023-09-11 06:41:33.424112+00:00, run_duration=0.24458, state=success, executor_state=success, try_number=1, max_tries=0, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:29.966056+00:00, queued_by_job_id=2, pid=40959[0m
[[34m2023-09-11T06:41:34.217+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-24T00:00:00+00:00, run_after=2023-01-25T00:00:00+00:00[0m
[[34m2023-09-11T06:41:34.239+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-23 00:00:00+00:00: scheduled__2023-01-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:29.902803+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:34.240+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-23 00:00:00+00:00, run_id=scheduled__2023-01-23T00:00:00+00:00, run_start_date=2023-09-11 06:41:29.920133+00:00, run_end_date=2023-09-11 06:41:34.240345+00:00, run_duration=4.320212, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-23 00:00:00+00:00, data_interval_end=2023-01-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:34.243+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-24T00:00:00+00:00, run_after=2023-01-25T00:00:00+00:00[0m
[[34m2023-09-11T06:41:35.015+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-25T00:00:00+00:00, run_after=2023-01-26T00:00:00+00:00[0m
[[34m2023-09-11T06:41:35.064+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:35.065+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:35.065+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:35.069+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:35.070+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:35.071+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:35.074+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:38.099+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:38.360+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:38.737+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:38.809+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:39.851+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:39.852+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:39.945+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:40.145+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:42.077+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:42.089+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:40.512111+00:00, run_end_date=2023-09-11 06:41:41.228737+00:00, run_duration=0.716626, state=success, executor_state=success, try_number=1, max_tries=0, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:35.067432+00:00, queued_by_job_id=2, pid=40970[0m
[[34m2023-09-11T06:41:42.382+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-26T00:00:00+00:00, run_after=2023-01-27T00:00:00+00:00[0m
[[34m2023-09-11T06:41:42.416+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-24 00:00:00+00:00: scheduled__2023-01-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:34.997933+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:42.417+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-24 00:00:00+00:00, run_id=scheduled__2023-01-24T00:00:00+00:00, run_start_date=2023-09-11 06:41:35.027137+00:00, run_end_date=2023-09-11 06:41:42.417244+00:00, run_duration=7.390107, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-24 00:00:00+00:00, data_interval_end=2023-01-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:42.420+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-25T00:00:00+00:00, run_after=2023-01-26T00:00:00+00:00[0m
[[34m2023-09-11T06:41:42.442+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:42.442+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:42.442+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:42.445+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:42.445+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:42.445+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:42.448+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:44.706+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:44.993+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:45.341+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:45.386+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:46.188+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:46.189+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:46.385+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:46.458+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:47.424+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:47.437+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:46.557435+00:00, run_end_date=2023-09-11 06:41:46.929410+00:00, run_duration=0.371975, state=success, executor_state=success, try_number=1, max_tries=0, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:42.443606+00:00, queued_by_job_id=2, pid=40979[0m
[[34m2023-09-11T06:41:47.703+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-26T00:00:00+00:00, run_after=2023-01-27T00:00:00+00:00[0m
[[34m2023-09-11T06:41:47.742+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-25 00:00:00+00:00: scheduled__2023-01-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:42.377250+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:47.742+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-25 00:00:00+00:00, run_id=scheduled__2023-01-25T00:00:00+00:00, run_start_date=2023-09-11 06:41:42.394345+00:00, run_end_date=2023-09-11 06:41:47.742731+00:00, run_duration=5.348386, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-25 00:00:00+00:00, data_interval_end=2023-01-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:47.749+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-26T00:00:00+00:00, run_after=2023-01-27T00:00:00+00:00[0m
[[34m2023-09-11T06:41:49.017+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-27T00:00:00+00:00, run_after=2023-01-28T00:00:00+00:00[0m
[[34m2023-09-11T06:41:49.062+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:49.062+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:49.063+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:49.065+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:49.066+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:49.066+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:49.069+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:51.604+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:51.771+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:52.025+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:52.061+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:52.905+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:52.906+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:53.021+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:53.154+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:54.701+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:54.722+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:53.358728+00:00, run_end_date=2023-09-11 06:41:53.934015+00:00, run_duration=0.575287, state=success, executor_state=success, try_number=1, max_tries=0, job_id=30, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:49.063971+00:00, queued_by_job_id=2, pid=40991[0m
[[34m2023-09-11T06:41:55.073+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-28T00:00:00+00:00, run_after=2023-01-29T00:00:00+00:00[0m
[[34m2023-09-11T06:41:55.128+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-26 00:00:00+00:00: scheduled__2023-01-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:49.010833+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:55.129+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-26 00:00:00+00:00, run_id=scheduled__2023-01-26T00:00:00+00:00, run_start_date=2023-09-11 06:41:49.029645+00:00, run_end_date=2023-09-11 06:41:55.129060+00:00, run_duration=6.099415, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-26 00:00:00+00:00, data_interval_end=2023-01-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:55.135+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-27T00:00:00+00:00, run_after=2023-01-28T00:00:00+00:00[0m
[[34m2023-09-11T06:41:55.161+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:55.161+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:55.162+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:55.166+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:55.167+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:55.167+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:55.183+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:57.373+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:57.500+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:57.676+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:57.708+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:58.226+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:58.227+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:58.295+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:58.350+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:59.122+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:59.134+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:58.446104+00:00, run_end_date=2023-09-11 06:41:58.664908+00:00, run_duration=0.218804, state=success, executor_state=success, try_number=1, max_tries=0, job_id=31, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:55.163634+00:00, queued_by_job_id=2, pid=40999[0m
[[34m2023-09-11T06:41:59.382+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-28T00:00:00+00:00, run_after=2023-01-29T00:00:00+00:00[0m
[[34m2023-09-11T06:41:59.405+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-27 00:00:00+00:00: scheduled__2023-01-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:55.067059+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:59.406+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-27 00:00:00+00:00, run_id=scheduled__2023-01-27T00:00:00+00:00, run_start_date=2023-09-11 06:41:55.089089+00:00, run_end_date=2023-09-11 06:41:59.406421+00:00, run_duration=4.317332, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-27 00:00:00+00:00, data_interval_end=2023-01-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:59.409+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-28T00:00:00+00:00, run_after=2023-01-29T00:00:00+00:00[0m
[[34m2023-09-11T06:42:00.000+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-29T00:00:00+00:00, run_after=2023-01-30T00:00:00+00:00[0m
[[34m2023-09-11T06:42:00.051+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:00.051+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:00.051+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:00.053+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:00.054+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:00.054+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:00.057+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:01.884+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:02.015+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:02.223+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:02.255+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:02.714+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:02.715+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:02.783+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:02.839+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:03.587+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:03.598+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:02.928341+00:00, run_end_date=2023-09-11 06:42:03.142399+00:00, run_duration=0.214058, state=success, executor_state=success, try_number=1, max_tries=0, job_id=32, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:00.052514+00:00, queued_by_job_id=2, pid=41009[0m
[[34m2023-09-11T06:42:03.882+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-30T00:00:00+00:00, run_after=2023-01-31T00:00:00+00:00[0m
[[34m2023-09-11T06:42:03.919+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-28 00:00:00+00:00: scheduled__2023-01-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:59.994997+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:03.919+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-28 00:00:00+00:00, run_id=scheduled__2023-01-28T00:00:00+00:00, run_start_date=2023-09-11 06:42:00.018150+00:00, run_end_date=2023-09-11 06:42:03.919884+00:00, run_duration=3.901734, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-28 00:00:00+00:00, data_interval_end=2023-01-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:03.923+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-29T00:00:00+00:00, run_after=2023-01-30T00:00:00+00:00[0m
[[34m2023-09-11T06:42:03.939+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:03.939+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:03.940+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:03.942+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:03.942+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:03.943+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:03.946+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:05.878+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:06.020+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:06.197+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:06.230+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:06.706+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:06.707+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:06.780+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:06.841+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:07.551+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:07.562+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:06.925034+00:00, run_end_date=2023-09-11 06:42:07.144433+00:00, run_duration=0.219399, state=success, executor_state=success, try_number=1, max_tries=0, job_id=33, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:03.940814+00:00, queued_by_job_id=2, pid=41018[0m
[[34m2023-09-11T06:42:07.905+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-30T00:00:00+00:00, run_after=2023-01-31T00:00:00+00:00[0m
[[34m2023-09-11T06:42:07.930+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-29 00:00:00+00:00: scheduled__2023-01-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:03.877201+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:07.931+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-29 00:00:00+00:00, run_id=scheduled__2023-01-29T00:00:00+00:00, run_start_date=2023-09-11 06:42:03.895457+00:00, run_end_date=2023-09-11 06:42:07.931315+00:00, run_duration=4.035858, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-29 00:00:00+00:00, data_interval_end=2023-01-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:07.935+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-30T00:00:00+00:00, run_after=2023-01-31T00:00:00+00:00[0m
[[34m2023-09-11T06:42:08.956+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-31T00:00:00+00:00, run_after=2023-02-01T00:00:00+00:00[0m
[[34m2023-09-11T06:42:09.002+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:09.003+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:09.003+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:09.005+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:09.006+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:09.006+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:09.009+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:10.923+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:11.051+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:11.223+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:11.254+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:11.701+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:11.701+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:11.770+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:11.827+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:12.605+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:12.616+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:11.913963+00:00, run_end_date=2023-09-11 06:42:12.149309+00:00, run_duration=0.235346, state=success, executor_state=success, try_number=1, max_tries=0, job_id=34, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:09.004025+00:00, queued_by_job_id=2, pid=41028[0m
[[34m2023-09-11T06:42:12.953+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-01T00:00:00+00:00, run_after=2023-02-02T00:00:00+00:00[0m
[[34m2023-09-11T06:42:12.991+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-30 00:00:00+00:00: scheduled__2023-01-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:08.952142+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:12.991+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-30 00:00:00+00:00, run_id=scheduled__2023-01-30T00:00:00+00:00, run_start_date=2023-09-11 06:42:08.970005+00:00, run_end_date=2023-09-11 06:42:12.991710+00:00, run_duration=4.021705, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-30 00:00:00+00:00, data_interval_end=2023-01-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:12.995+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-31T00:00:00+00:00, run_after=2023-02-01T00:00:00+00:00[0m
[[34m2023-09-11T06:42:13.010+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:13.010+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:13.011+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:13.013+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:13.014+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:13.014+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:13.017+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:14.976+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:15.116+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:15.297+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:15.329+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:15.818+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:15.819+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:15.890+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:15.950+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:16.725+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:16.736+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:16.053204+00:00, run_end_date=2023-09-11 06:42:16.316770+00:00, run_duration=0.263566, state=success, executor_state=success, try_number=1, max_tries=0, job_id=35, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:13.011872+00:00, queued_by_job_id=2, pid=41035[0m
[[34m2023-09-11T06:42:16.941+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-01T00:00:00+00:00, run_after=2023-02-02T00:00:00+00:00[0m
[[34m2023-09-11T06:42:16.965+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-31 00:00:00+00:00: scheduled__2023-01-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:12.948874+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:16.965+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-31 00:00:00+00:00, run_id=scheduled__2023-01-31T00:00:00+00:00, run_start_date=2023-09-11 06:42:12.968102+00:00, run_end_date=2023-09-11 06:42:16.965847+00:00, run_duration=3.997745, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-31 00:00:00+00:00, data_interval_end=2023-02-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:16.969+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-01T00:00:00+00:00, run_after=2023-02-02T00:00:00+00:00[0m
[[34m2023-09-11T06:42:18.006+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-02T00:00:00+00:00, run_after=2023-02-03T00:00:00+00:00[0m
[[34m2023-09-11T06:42:18.048+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:18.049+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:18.049+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:18.051+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:18.052+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:18.052+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:18.055+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:20.003+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:20.134+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:20.316+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:20.349+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:20.890+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:20.890+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:20.961+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:21.020+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:21.867+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:21.878+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:21.105362+00:00, run_end_date=2023-09-11 06:42:21.361849+00:00, run_duration=0.256487, state=success, executor_state=success, try_number=1, max_tries=0, job_id=36, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:18.050281+00:00, queued_by_job_id=2, pid=41045[0m
[[34m2023-09-11T06:42:22.146+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-03T00:00:00+00:00, run_after=2023-02-04T00:00:00+00:00[0m
[[34m2023-09-11T06:42:22.192+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-01 00:00:00+00:00: scheduled__2023-02-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:18.002053+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:22.192+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-01 00:00:00+00:00, run_id=scheduled__2023-02-01T00:00:00+00:00, run_start_date=2023-09-11 06:42:18.018116+00:00, run_end_date=2023-09-11 06:42:22.192408+00:00, run_duration=4.174292, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-01 00:00:00+00:00, data_interval_end=2023-02-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:22.196+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-02T00:00:00+00:00, run_after=2023-02-03T00:00:00+00:00[0m
[[34m2023-09-11T06:42:22.211+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:22.211+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:22.212+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:22.214+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:22.215+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:22.215+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:22.218+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:24.219+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:24.349+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:24.518+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:24.553+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:25.029+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:25.029+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:25.105+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:25.172+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:25.901+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:25.912+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:25.263161+00:00, run_end_date=2023-09-11 06:42:25.489605+00:00, run_duration=0.226444, state=success, executor_state=success, try_number=1, max_tries=0, job_id=37, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:22.212924+00:00, queued_by_job_id=2, pid=41052[0m
[[34m2023-09-11T06:42:26.154+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-03T00:00:00+00:00, run_after=2023-02-04T00:00:00+00:00[0m
[[34m2023-09-11T06:42:26.177+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-02 00:00:00+00:00: scheduled__2023-02-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:22.141646+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:26.177+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-02 00:00:00+00:00, run_id=scheduled__2023-02-02T00:00:00+00:00, run_start_date=2023-09-11 06:42:22.167776+00:00, run_end_date=2023-09-11 06:42:26.177786+00:00, run_duration=4.01001, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-02 00:00:00+00:00, data_interval_end=2023-02-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:26.181+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-03T00:00:00+00:00, run_after=2023-02-04T00:00:00+00:00[0m
[[34m2023-09-11T06:42:27.145+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-04T00:00:00+00:00, run_after=2023-02-05T00:00:00+00:00[0m
[[34m2023-09-11T06:42:27.191+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:27.191+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:27.192+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:27.194+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:27.194+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:27.195+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:27.212+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:29.210+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:29.339+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:29.507+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:29.539+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:30.009+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:30.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:30.080+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:30.139+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:31.003+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:31.014+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:30.224526+00:00, run_end_date=2023-09-11 06:42:30.461543+00:00, run_duration=0.237017, state=success, executor_state=success, try_number=1, max_tries=0, job_id=38, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:27.192837+00:00, queued_by_job_id=2, pid=41064[0m
[[34m2023-09-11T06:42:31.273+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-05T00:00:00+00:00, run_after=2023-02-06T00:00:00+00:00[0m
[[34m2023-09-11T06:42:31.309+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-03 00:00:00+00:00: scheduled__2023-02-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:27.140335+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:31.309+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-03 00:00:00+00:00, run_id=scheduled__2023-02-03T00:00:00+00:00, run_start_date=2023-09-11 06:42:27.158627+00:00, run_end_date=2023-09-11 06:42:31.309853+00:00, run_duration=4.151226, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-03 00:00:00+00:00, data_interval_end=2023-02-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:31.313+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-04T00:00:00+00:00, run_after=2023-02-05T00:00:00+00:00[0m
[[34m2023-09-11T06:42:31.328+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:31.328+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:31.328+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:31.331+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:31.331+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:31.331+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:31.334+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:33.197+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:33.330+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:33.527+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:33.593+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:34.085+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:34.086+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:34.157+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:34.215+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:34.921+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:34.931+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:34.298812+00:00, run_end_date=2023-09-11 06:42:34.522586+00:00, run_duration=0.223774, state=success, executor_state=success, try_number=1, max_tries=0, job_id=39, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:31.329664+00:00, queued_by_job_id=2, pid=41073[0m
[[34m2023-09-11T06:42:35.180+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-05T00:00:00+00:00, run_after=2023-02-06T00:00:00+00:00[0m
[[34m2023-09-11T06:42:35.218+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-04 00:00:00+00:00: scheduled__2023-02-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:31.267921+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:35.218+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-04 00:00:00+00:00, run_id=scheduled__2023-02-04T00:00:00+00:00, run_start_date=2023-09-11 06:42:31.285914+00:00, run_end_date=2023-09-11 06:42:35.218707+00:00, run_duration=3.932793, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-04 00:00:00+00:00, data_interval_end=2023-02-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:35.222+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-05T00:00:00+00:00, run_after=2023-02-06T00:00:00+00:00[0m
[[34m2023-09-11T06:42:36.372+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-06T00:00:00+00:00, run_after=2023-02-07T00:00:00+00:00[0m
[[34m2023-09-11T06:42:36.417+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:36.417+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:36.417+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:36.419+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:36.420+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:36.420+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:36.422+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:38.341+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:38.473+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:38.657+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:38.690+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:39.154+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:39.155+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:39.259+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:39.357+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:40.102+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:40.113+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:39.450772+00:00, run_end_date=2023-09-11 06:42:39.691135+00:00, run_duration=0.240363, state=success, executor_state=success, try_number=1, max_tries=0, job_id=40, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:36.418466+00:00, queued_by_job_id=2, pid=41083[0m
[[34m2023-09-11T06:42:40.382+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-07T00:00:00+00:00, run_after=2023-02-08T00:00:00+00:00[0m
[[34m2023-09-11T06:42:40.418+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-05 00:00:00+00:00: scheduled__2023-02-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:36.368113+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:40.419+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-05 00:00:00+00:00, run_id=scheduled__2023-02-05T00:00:00+00:00, run_start_date=2023-09-11 06:42:36.386302+00:00, run_end_date=2023-09-11 06:42:40.419167+00:00, run_duration=4.032865, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-05 00:00:00+00:00, data_interval_end=2023-02-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:40.422+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-06T00:00:00+00:00, run_after=2023-02-07T00:00:00+00:00[0m
[[34m2023-09-11T06:42:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:40.439+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:40.441+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:40.441+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:40.442+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:40.444+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:42.274+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:42.409+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:42.580+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:42.613+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:43.072+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:43.073+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:43.143+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:43.202+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:43.957+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:43.968+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:43.287128+00:00, run_end_date=2023-09-11 06:42:43.527439+00:00, run_duration=0.240311, state=success, executor_state=success, try_number=1, max_tries=0, job_id=41, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:40.439818+00:00, queued_by_job_id=2, pid=41092[0m
[[34m2023-09-11T06:42:44.218+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-07T00:00:00+00:00, run_after=2023-02-08T00:00:00+00:00[0m
[[34m2023-09-11T06:42:44.241+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-06 00:00:00+00:00: scheduled__2023-02-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:40.377685+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:44.242+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-06 00:00:00+00:00, run_id=scheduled__2023-02-06T00:00:00+00:00, run_start_date=2023-09-11 06:42:40.395668+00:00, run_end_date=2023-09-11 06:42:44.242005+00:00, run_duration=3.846337, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-06 00:00:00+00:00, data_interval_end=2023-02-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:44.245+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-07T00:00:00+00:00, run_after=2023-02-08T00:00:00+00:00[0m
[[34m2023-09-11T06:42:45.186+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-08T00:00:00+00:00, run_after=2023-02-09T00:00:00+00:00[0m
[[34m2023-09-11T06:42:45.229+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:45.230+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:45.230+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:45.232+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:45.232+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:45.233+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:45.236+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:47.196+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:47.333+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:47.514+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:47.549+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:48.025+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:48.026+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:48.103+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:48.164+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:48.906+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:48.917+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:48.248876+00:00, run_end_date=2023-09-11 06:42:48.479508+00:00, run_duration=0.230632, state=success, executor_state=success, try_number=1, max_tries=0, job_id=42, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:45.230955+00:00, queued_by_job_id=2, pid=41102[0m
[[34m2023-09-11T06:42:49.090+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-09T00:00:00+00:00, run_after=2023-02-10T00:00:00+00:00[0m
[[34m2023-09-11T06:42:49.138+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-07 00:00:00+00:00: scheduled__2023-02-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:45.181935+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:49.138+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-07 00:00:00+00:00, run_id=scheduled__2023-02-07T00:00:00+00:00, run_start_date=2023-09-11 06:42:45.198417+00:00, run_end_date=2023-09-11 06:42:49.138814+00:00, run_duration=3.940397, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-07 00:00:00+00:00, data_interval_end=2023-02-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:49.142+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-08T00:00:00+00:00, run_after=2023-02-09T00:00:00+00:00[0m
[[34m2023-09-11T06:42:49.159+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:49.159+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:49.160+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:49.163+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:49.163+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:49.163+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:49.166+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:51.148+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:51.277+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:51.462+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:51.494+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:51.989+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:51.990+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:52.071+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:52.133+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:52.933+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:52.944+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:52.247340+00:00, run_end_date=2023-09-11 06:42:52.495153+00:00, run_duration=0.247813, state=success, executor_state=success, try_number=1, max_tries=0, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:49.161628+00:00, queued_by_job_id=2, pid=41111[0m
[[34m2023-09-11T06:42:53.301+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-09T00:00:00+00:00, run_after=2023-02-10T00:00:00+00:00[0m
[[34m2023-09-11T06:42:53.324+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-08 00:00:00+00:00: scheduled__2023-02-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:49.083771+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:53.325+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-08 00:00:00+00:00, run_id=scheduled__2023-02-08T00:00:00+00:00, run_start_date=2023-09-11 06:42:49.106755+00:00, run_end_date=2023-09-11 06:42:53.324931+00:00, run_duration=4.218176, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-08 00:00:00+00:00, data_interval_end=2023-02-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:53.328+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-09T00:00:00+00:00, run_after=2023-02-10T00:00:00+00:00[0m
[[34m2023-09-11T06:42:54.183+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-10T00:00:00+00:00, run_after=2023-02-11T00:00:00+00:00[0m
[[34m2023-09-11T06:42:54.226+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:54.226+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:54.227+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:54.229+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:54.230+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:54.230+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:54.233+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:56.193+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:56.323+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:56.497+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:56.530+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:57.020+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:57.021+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:57.092+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:57.165+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:57.891+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:57.902+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:57.255303+00:00, run_end_date=2023-09-11 06:42:57.473040+00:00, run_duration=0.217737, state=success, executor_state=success, try_number=1, max_tries=0, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:54.227750+00:00, queued_by_job_id=2, pid=41121[0m
[[34m2023-09-11T06:42:58.159+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-11T00:00:00+00:00, run_after=2023-02-12T00:00:00+00:00[0m
[[34m2023-09-11T06:42:58.198+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-09 00:00:00+00:00: scheduled__2023-02-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:54.178331+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:58.198+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-09 00:00:00+00:00, run_id=scheduled__2023-02-09T00:00:00+00:00, run_start_date=2023-09-11 06:42:54.194943+00:00, run_end_date=2023-09-11 06:42:58.198707+00:00, run_duration=4.003764, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-09 00:00:00+00:00, data_interval_end=2023-02-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:58.202+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-10T00:00:00+00:00, run_after=2023-02-11T00:00:00+00:00[0m
[[34m2023-09-11T06:42:58.219+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:58.219+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:58.220+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:58.222+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:58.222+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:58.223+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:58.225+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:00.149+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:00.287+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:00.474+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:00.508+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:00.989+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:00.989+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:01.059+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:01.122+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:01.830+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:01.840+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:01.208874+00:00, run_end_date=2023-09-11 06:43:01.435342+00:00, run_duration=0.226468, state=success, executor_state=success, try_number=1, max_tries=0, job_id=45, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:58.220747+00:00, queued_by_job_id=2, pid=41128[0m
[[34m2023-09-11T06:43:02.082+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-11T00:00:00+00:00, run_after=2023-02-12T00:00:00+00:00[0m
[[34m2023-09-11T06:43:02.105+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-10 00:00:00+00:00: scheduled__2023-02-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:58.154390+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:02.106+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-10 00:00:00+00:00, run_id=scheduled__2023-02-10T00:00:00+00:00, run_start_date=2023-09-11 06:42:58.171830+00:00, run_end_date=2023-09-11 06:43:02.106104+00:00, run_duration=3.934274, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-10 00:00:00+00:00, data_interval_end=2023-02-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:02.110+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-11T00:00:00+00:00, run_after=2023-02-12T00:00:00+00:00[0m
[[34m2023-09-11T06:43:03.165+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-12T00:00:00+00:00, run_after=2023-02-13T00:00:00+00:00[0m
[[34m2023-09-11T06:43:03.210+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:03.211+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:03.211+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:03.213+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:03.214+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:03.214+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:03.216+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:05.054+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:05.184+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:05.355+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:05.388+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:05.855+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:05.856+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:05.940+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:06.002+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:06.753+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:06.764+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:06.090282+00:00, run_end_date=2023-09-11 06:43:06.318182+00:00, run_duration=0.2279, state=success, executor_state=success, try_number=1, max_tries=0, job_id=46, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:03.212004+00:00, queued_by_job_id=2, pid=41138[0m
[[34m2023-09-11T06:43:06.919+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-13T00:00:00+00:00, run_after=2023-02-14T00:00:00+00:00[0m
[[34m2023-09-11T06:43:06.969+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-11 00:00:00+00:00: scheduled__2023-02-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:03.158711+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:06.969+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-11 00:00:00+00:00, run_id=scheduled__2023-02-11T00:00:00+00:00, run_start_date=2023-09-11 06:43:03.180002+00:00, run_end_date=2023-09-11 06:43:06.969797+00:00, run_duration=3.789795, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-11 00:00:00+00:00, data_interval_end=2023-02-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:06.973+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-12T00:00:00+00:00, run_after=2023-02-13T00:00:00+00:00[0m
[[34m2023-09-11T06:43:06.988+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:06.988+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:06.988+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:06.990+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:06.991+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:06.991+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:06.994+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:08.853+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:08.992+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:09.165+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:09.196+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:09.678+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:09.679+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:09.753+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:09.811+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:10.514+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:10.527+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:09.903075+00:00, run_end_date=2023-09-11 06:43:10.126387+00:00, run_duration=0.223312, state=success, executor_state=success, try_number=1, max_tries=0, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:06.989483+00:00, queued_by_job_id=2, pid=41146[0m
[[34m2023-09-11T06:43:10.772+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-13T00:00:00+00:00, run_after=2023-02-14T00:00:00+00:00[0m
[[34m2023-09-11T06:43:10.794+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-12 00:00:00+00:00: scheduled__2023-02-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:06.914723+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:10.795+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-12 00:00:00+00:00, run_id=scheduled__2023-02-12T00:00:00+00:00, run_start_date=2023-09-11 06:43:06.933504+00:00, run_end_date=2023-09-11 06:43:10.794887+00:00, run_duration=3.861383, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-12 00:00:00+00:00, data_interval_end=2023-02-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:10.798+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-13T00:00:00+00:00, run_after=2023-02-14T00:00:00+00:00[0m
[[34m2023-09-11T06:43:11.936+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-14T00:00:00+00:00, run_after=2023-02-15T00:00:00+00:00[0m
[[34m2023-09-11T06:43:11.980+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:11.980+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:11.981+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:11.983+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:11.983+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:11.984+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:11.986+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:14.101+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:14.276+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:14.467+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:14.504+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:15.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:15.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:15.087+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:15.149+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:15.893+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:15.903+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:15.234046+00:00, run_end_date=2023-09-11 06:43:15.473106+00:00, run_duration=0.23906, state=success, executor_state=success, try_number=1, max_tries=0, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:11.981895+00:00, queued_by_job_id=2, pid=41156[0m
[[34m2023-09-11T06:43:16.087+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-15T00:00:00+00:00, run_after=2023-02-16T00:00:00+00:00[0m
[[34m2023-09-11T06:43:16.125+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-13 00:00:00+00:00: scheduled__2023-02-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:11.931891+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:16.125+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-13 00:00:00+00:00, run_id=scheduled__2023-02-13T00:00:00+00:00, run_start_date=2023-09-11 06:43:11.949056+00:00, run_end_date=2023-09-11 06:43:16.125807+00:00, run_duration=4.176751, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-13 00:00:00+00:00, data_interval_end=2023-02-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:16.129+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-14T00:00:00+00:00, run_after=2023-02-15T00:00:00+00:00[0m
[[34m2023-09-11T06:43:16.144+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:16.145+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:16.145+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:16.147+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:16.147+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:16.147+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:16.150+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:18.285+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:18.455+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:18.674+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:18.706+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:19.170+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:19.171+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:19.243+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:19.307+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:20.027+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:20.037+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:19.400549+00:00, run_end_date=2023-09-11 06:43:19.619740+00:00, run_duration=0.219191, state=success, executor_state=success, try_number=1, max_tries=0, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:16.145901+00:00, queued_by_job_id=2, pid=41165[0m
[[34m2023-09-11T06:43:20.174+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-15T00:00:00+00:00, run_after=2023-02-16T00:00:00+00:00[0m
[[34m2023-09-11T06:43:20.197+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-14 00:00:00+00:00: scheduled__2023-02-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:16.083069+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:20.198+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-14 00:00:00+00:00, run_id=scheduled__2023-02-14T00:00:00+00:00, run_start_date=2023-09-11 06:43:16.100863+00:00, run_end_date=2023-09-11 06:43:20.198225+00:00, run_duration=4.097362, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-14 00:00:00+00:00, data_interval_end=2023-02-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:20.201+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-15T00:00:00+00:00, run_after=2023-02-16T00:00:00+00:00[0m
[[34m2023-09-11T06:43:21.083+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-16T00:00:00+00:00, run_after=2023-02-17T00:00:00+00:00[0m
[[34m2023-09-11T06:43:21.129+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:21.130+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:21.130+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:21.132+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:21.133+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:21.133+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:21.136+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:23.026+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:23.157+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:23.324+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:23.358+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:23.828+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:23.829+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:23.908+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:23.967+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:24.856+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:24.867+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:24.052237+00:00, run_end_date=2023-09-11 06:43:24.421215+00:00, run_duration=0.368978, state=success, executor_state=success, try_number=1, max_tries=0, job_id=50, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:21.131100+00:00, queued_by_job_id=2, pid=41175[0m
[[34m2023-09-11T06:43:25.125+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-17T00:00:00+00:00, run_after=2023-02-18T00:00:00+00:00[0m
[[34m2023-09-11T06:43:25.179+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-15 00:00:00+00:00: scheduled__2023-02-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:21.078556+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:25.180+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-15 00:00:00+00:00, run_id=scheduled__2023-02-15T00:00:00+00:00, run_start_date=2023-09-11 06:43:21.095678+00:00, run_end_date=2023-09-11 06:43:25.180247+00:00, run_duration=4.084569, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-15 00:00:00+00:00, data_interval_end=2023-02-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:25.183+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-16T00:00:00+00:00, run_after=2023-02-17T00:00:00+00:00[0m
[[34m2023-09-11T06:43:25.198+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:25.198+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:25.198+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:25.201+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:25.201+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:25.201+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:25.204+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:27.169+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:27.311+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:27.531+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:27.567+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:28.040+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:28.041+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:28.116+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:28.185+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:28.933+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:28.944+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:28.270249+00:00, run_end_date=2023-09-11 06:43:28.513418+00:00, run_duration=0.243169, state=success, executor_state=success, try_number=1, max_tries=0, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:25.199573+00:00, queued_by_job_id=2, pid=41184[0m
[[34m2023-09-11T06:43:29.197+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-17T00:00:00+00:00, run_after=2023-02-18T00:00:00+00:00[0m
[[34m2023-09-11T06:43:29.218+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-16 00:00:00+00:00: scheduled__2023-02-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:25.120323+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:29.219+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-16 00:00:00+00:00, run_id=scheduled__2023-02-16T00:00:00+00:00, run_start_date=2023-09-11 06:43:25.157344+00:00, run_end_date=2023-09-11 06:43:29.219219+00:00, run_duration=4.061875, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-16 00:00:00+00:00, data_interval_end=2023-02-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:29.222+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-17T00:00:00+00:00, run_after=2023-02-18T00:00:00+00:00[0m
[[34m2023-09-11T06:43:30.125+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-18T00:00:00+00:00, run_after=2023-02-19T00:00:00+00:00[0m
[[34m2023-09-11T06:43:30.169+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:30.170+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:30.170+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:30.172+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:30.173+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:30.173+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:30.177+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:32.258+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:32.430+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:32.659+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:32.701+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:33.452+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:33.453+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:33.530+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:33.592+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:34.333+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:34.345+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:33.705298+00:00, run_end_date=2023-09-11 06:43:33.936476+00:00, run_duration=0.231178, state=success, executor_state=success, try_number=1, max_tries=0, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:30.171053+00:00, queued_by_job_id=2, pid=41195[0m
[[34m2023-09-11T06:43:34.721+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-19T00:00:00+00:00, run_after=2023-02-20T00:00:00+00:00[0m
[[34m2023-09-11T06:43:34.757+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-17 00:00:00+00:00: scheduled__2023-02-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:30.120756+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:34.757+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-17 00:00:00+00:00, run_id=scheduled__2023-02-17T00:00:00+00:00, run_start_date=2023-09-11 06:43:30.136956+00:00, run_end_date=2023-09-11 06:43:34.757646+00:00, run_duration=4.62069, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-17 00:00:00+00:00, data_interval_end=2023-02-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:34.761+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-18T00:00:00+00:00, run_after=2023-02-19T00:00:00+00:00[0m
[[34m2023-09-11T06:43:34.778+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:34.778+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:34.778+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:34.781+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:34.782+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:34.782+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:34.784+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:37.270+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:37.416+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:37.624+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:37.706+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:38.282+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:38.283+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:38.357+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:38.420+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:39.200+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:39.212+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:38.509890+00:00, run_end_date=2023-09-11 06:43:38.750638+00:00, run_duration=0.240748, state=success, executor_state=success, try_number=1, max_tries=0, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:34.779800+00:00, queued_by_job_id=2, pid=41204[0m
[[34m2023-09-11T06:43:39.489+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-19T00:00:00+00:00, run_after=2023-02-20T00:00:00+00:00[0m
[[34m2023-09-11T06:43:39.515+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-18 00:00:00+00:00: scheduled__2023-02-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:34.717087+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:39.515+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-18 00:00:00+00:00, run_id=scheduled__2023-02-18T00:00:00+00:00, run_start_date=2023-09-11 06:43:34.734103+00:00, run_end_date=2023-09-11 06:43:39.515734+00:00, run_duration=4.781631, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-18 00:00:00+00:00, data_interval_end=2023-02-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:39.519+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-19T00:00:00+00:00, run_after=2023-02-20T00:00:00+00:00[0m
[[34m2023-09-11T06:43:40.790+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-20T00:00:00+00:00, run_after=2023-02-21T00:00:00+00:00[0m
[[34m2023-09-11T06:43:40.837+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:40.837+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:40.837+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:40.839+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:40.840+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:40.840+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:40.843+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:42.822+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:42.963+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:43.147+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:43.181+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:43.681+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:43.682+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:43.764+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:43.826+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:44.601+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:44.613+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:43.926206+00:00, run_end_date=2023-09-11 06:43:44.162456+00:00, run_duration=0.23625, state=success, executor_state=success, try_number=1, max_tries=0, job_id=54, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:40.838412+00:00, queued_by_job_id=2, pid=41214[0m
[[34m2023-09-11T06:43:44.952+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-21T00:00:00+00:00, run_after=2023-02-22T00:00:00+00:00[0m
[[34m2023-09-11T06:43:44.987+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-19 00:00:00+00:00: scheduled__2023-02-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:40.784475+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:44.987+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-19 00:00:00+00:00, run_id=scheduled__2023-02-19T00:00:00+00:00, run_start_date=2023-09-11 06:43:40.802792+00:00, run_end_date=2023-09-11 06:43:44.987779+00:00, run_duration=4.184987, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-19 00:00:00+00:00, data_interval_end=2023-02-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:44.991+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-20T00:00:00+00:00, run_after=2023-02-21T00:00:00+00:00[0m
[[34m2023-09-11T06:43:45.005+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:45.006+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:45.006+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:45.008+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:45.009+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:45.009+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:45.012+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:46.886+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:47.012+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:47.179+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:47.210+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:47.670+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:47.670+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:47.740+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:47.798+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:48.539+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:48.549+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:47.883377+00:00, run_end_date=2023-09-11 06:43:48.118864+00:00, run_duration=0.235487, state=success, executor_state=success, try_number=1, max_tries=0, job_id=55, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:45.007177+00:00, queued_by_job_id=2, pid=41223[0m
[[34m2023-09-11T06:43:48.920+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-21T00:00:00+00:00, run_after=2023-02-22T00:00:00+00:00[0m
[[34m2023-09-11T06:43:48.948+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-20 00:00:00+00:00: scheduled__2023-02-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:44.947518+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:48.948+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-20 00:00:00+00:00, run_id=scheduled__2023-02-20T00:00:00+00:00, run_start_date=2023-09-11 06:43:44.965088+00:00, run_end_date=2023-09-11 06:43:48.948512+00:00, run_duration=3.983424, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-20 00:00:00+00:00, data_interval_end=2023-02-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:48.952+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-21T00:00:00+00:00, run_after=2023-02-22T00:00:00+00:00[0m
[[34m2023-09-11T06:43:49.595+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-22T00:00:00+00:00, run_after=2023-02-23T00:00:00+00:00[0m
[[34m2023-09-11T06:43:49.656+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:49.657+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:49.657+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:49.661+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:49.662+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:49.662+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:49.665+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:51.651+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:51.795+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:51.982+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:52.018+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:52.515+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:52.515+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:52.597+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:52.659+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:53.384+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:53.396+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:52.747990+00:00, run_end_date=2023-09-11 06:43:52.977066+00:00, run_duration=0.229076, state=success, executor_state=success, try_number=1, max_tries=0, job_id=56, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:49.658613+00:00, queued_by_job_id=2, pid=41233[0m
[[34m2023-09-11T06:43:53.717+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-23T00:00:00+00:00, run_after=2023-02-24T00:00:00+00:00[0m
[[34m2023-09-11T06:43:53.755+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-21 00:00:00+00:00: scheduled__2023-02-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:49.586777+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:53.756+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-21 00:00:00+00:00, run_id=scheduled__2023-02-21T00:00:00+00:00, run_start_date=2023-09-11 06:43:49.614001+00:00, run_end_date=2023-09-11 06:43:53.756433+00:00, run_duration=4.142432, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-21 00:00:00+00:00, data_interval_end=2023-02-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:53.760+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-22T00:00:00+00:00, run_after=2023-02-23T00:00:00+00:00[0m
[[34m2023-09-11T06:43:53.776+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:53.776+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:53.777+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:53.779+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:53.779+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:53.780+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:53.783+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:55.848+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:55.986+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:56.175+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:56.208+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:56.695+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:56.695+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:56.768+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:56.826+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:57.631+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:57.642+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:56.914459+00:00, run_end_date=2023-09-11 06:43:57.160494+00:00, run_duration=0.246035, state=success, executor_state=success, try_number=1, max_tries=0, job_id=57, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:53.777868+00:00, queued_by_job_id=2, pid=41241[0m
[[34m2023-09-11T06:43:57.989+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-23T00:00:00+00:00, run_after=2023-02-24T00:00:00+00:00[0m
[[34m2023-09-11T06:43:58.014+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-22 00:00:00+00:00: scheduled__2023-02-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:53.712868+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:58.015+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-22 00:00:00+00:00, run_id=scheduled__2023-02-22T00:00:00+00:00, run_start_date=2023-09-11 06:43:53.731388+00:00, run_end_date=2023-09-11 06:43:58.014990+00:00, run_duration=4.283602, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-22 00:00:00+00:00, data_interval_end=2023-02-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:58.018+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-23T00:00:00+00:00, run_after=2023-02-24T00:00:00+00:00[0m
[[34m2023-09-11T06:43:58.723+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-24T00:00:00+00:00, run_after=2023-02-25T00:00:00+00:00[0m
[[34m2023-09-11T06:43:58.771+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:58.771+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:58.771+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:58.774+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:58.775+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:58.775+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:58.778+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:00.770+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:00.901+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:01.079+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:01.116+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:01.604+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:01.604+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:01.679+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:01.739+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:02.464+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:02.475+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:01.833431+00:00, run_end_date=2023-09-11 06:44:02.058760+00:00, run_duration=0.225329, state=success, executor_state=success, try_number=1, max_tries=0, job_id=58, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:58.772709+00:00, queued_by_job_id=2, pid=41251[0m
[[34m2023-09-11T06:44:02.854+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-25T00:00:00+00:00, run_after=2023-02-26T00:00:00+00:00[0m
[[34m2023-09-11T06:44:02.893+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-23 00:00:00+00:00: scheduled__2023-02-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:58.718777+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:02.894+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-23 00:00:00+00:00, run_id=scheduled__2023-02-23T00:00:00+00:00, run_start_date=2023-09-11 06:43:58.737093+00:00, run_end_date=2023-09-11 06:44:02.894107+00:00, run_duration=4.157014, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-23 00:00:00+00:00, data_interval_end=2023-02-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:02.897+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-24T00:00:00+00:00, run_after=2023-02-25T00:00:00+00:00[0m
[[34m2023-09-11T06:44:02.912+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:02.913+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:02.913+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:02.915+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:02.915+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:02.916+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:02.918+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:05.122+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:05.255+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:05.435+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:05.470+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:05.985+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:05.985+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:06.064+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:06.130+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:06.873+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:06.884+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:06.224407+00:00, run_end_date=2023-09-11 06:44:06.460022+00:00, run_duration=0.235615, state=success, executor_state=success, try_number=1, max_tries=0, job_id=59, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:02.913982+00:00, queued_by_job_id=2, pid=41258[0m
[[34m2023-09-11T06:44:07.133+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-25T00:00:00+00:00, run_after=2023-02-26T00:00:00+00:00[0m
[[34m2023-09-11T06:44:07.157+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-24 00:00:00+00:00: scheduled__2023-02-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:02.849127+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:07.157+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-24 00:00:00+00:00, run_id=scheduled__2023-02-24T00:00:00+00:00, run_start_date=2023-09-11 06:44:02.867174+00:00, run_end_date=2023-09-11 06:44:07.157709+00:00, run_duration=4.290535, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-24 00:00:00+00:00, data_interval_end=2023-02-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:07.161+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-25T00:00:00+00:00, run_after=2023-02-26T00:00:00+00:00[0m
[[34m2023-09-11T06:44:07.904+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-26T00:00:00+00:00, run_after=2023-02-27T00:00:00+00:00[0m
[[34m2023-09-11T06:44:07.951+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:07.951+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:07.951+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:07.953+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:07.954+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:07.955+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:07.957+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:09.914+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:10.051+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:10.247+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:10.282+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:10.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:10.775+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:10.851+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:10.913+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:11.691+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:11.702+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:11.004601+00:00, run_end_date=2023-09-11 06:44:11.273765+00:00, run_duration=0.269164, state=success, executor_state=success, try_number=1, max_tries=0, job_id=60, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:07.952358+00:00, queued_by_job_id=2, pid=41268[0m
[[34m2023-09-11T06:44:11.957+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-27T00:00:00+00:00, run_after=2023-02-28T00:00:00+00:00[0m
[[34m2023-09-11T06:44:11.994+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-25 00:00:00+00:00: scheduled__2023-02-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:07.899652+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:11.995+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-25 00:00:00+00:00, run_id=scheduled__2023-02-25T00:00:00+00:00, run_start_date=2023-09-11 06:44:07.918048+00:00, run_end_date=2023-09-11 06:44:11.995303+00:00, run_duration=4.077255, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-25 00:00:00+00:00, data_interval_end=2023-02-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:11.998+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-26T00:00:00+00:00, run_after=2023-02-27T00:00:00+00:00[0m
[[34m2023-09-11T06:44:12.019+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:12.020+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:12.020+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:12.023+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:12.025+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:12.025+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:12.029+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:14.016+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:14.157+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:14.344+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:14.380+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:14.879+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:14.879+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:14.954+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:15.014+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:15.758+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:15.769+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:15.102756+00:00, run_end_date=2023-09-11 06:44:15.337869+00:00, run_duration=0.235113, state=success, executor_state=success, try_number=1, max_tries=0, job_id=61, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:12.021654+00:00, queued_by_job_id=2, pid=41277[0m
[[34m2023-09-11T06:44:16.015+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-27T00:00:00+00:00, run_after=2023-02-28T00:00:00+00:00[0m
[[34m2023-09-11T06:44:16.041+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-26 00:00:00+00:00: scheduled__2023-02-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:11.951328+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:16.041+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-26 00:00:00+00:00, run_id=scheduled__2023-02-26T00:00:00+00:00, run_start_date=2023-09-11 06:44:11.969377+00:00, run_end_date=2023-09-11 06:44:16.041406+00:00, run_duration=4.072029, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-26 00:00:00+00:00, data_interval_end=2023-02-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:16.045+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-27T00:00:00+00:00, run_after=2023-02-28T00:00:00+00:00[0m
[[34m2023-09-11T06:44:17.061+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-28T00:00:00+00:00, run_after=2023-03-01T00:00:00+00:00[0m
[[34m2023-09-11T06:44:17.107+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:17.107+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:17.108+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:17.110+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:17.111+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:17.111+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:17.114+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:19.011+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:19.145+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:19.320+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:19.355+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:19.844+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:19.845+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:19.921+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:19.980+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:20.710+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:20.721+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:20.081163+00:00, run_end_date=2023-09-11 06:44:20.305638+00:00, run_duration=0.224475, state=success, executor_state=success, try_number=1, max_tries=0, job_id=62, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:17.108990+00:00, queued_by_job_id=2, pid=41288[0m
[[34m2023-09-11T06:44:20.998+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-01T00:00:00+00:00, run_after=2023-03-02T00:00:00+00:00[0m
[[34m2023-09-11T06:44:21.035+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-27 00:00:00+00:00: scheduled__2023-02-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:17.056518+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:21.036+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-27 00:00:00+00:00, run_id=scheduled__2023-02-27T00:00:00+00:00, run_start_date=2023-09-11 06:44:17.073053+00:00, run_end_date=2023-09-11 06:44:21.035994+00:00, run_duration=3.962941, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-27 00:00:00+00:00, data_interval_end=2023-02-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:21.040+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-28T00:00:00+00:00, run_after=2023-03-01T00:00:00+00:00[0m
[[34m2023-09-11T06:44:21.055+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:21.055+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:21.055+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:21.058+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:21.058+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:21.059+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:21.061+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:22.947+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:23.079+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:23.263+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:23.300+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:23.800+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:23.801+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:23.874+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:23.935+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:24.676+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:24.687+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:24.026863+00:00, run_end_date=2023-09-11 06:44:24.256542+00:00, run_duration=0.229679, state=success, executor_state=success, try_number=1, max_tries=0, job_id=63, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:21.056559+00:00, queued_by_job_id=2, pid=41297[0m
[[34m2023-09-11T06:44:24.924+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-01T00:00:00+00:00, run_after=2023-03-02T00:00:00+00:00[0m
[[34m2023-09-11T06:44:24.949+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-28 00:00:00+00:00: scheduled__2023-02-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:20.993710+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:24.950+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-28 00:00:00+00:00, run_id=scheduled__2023-02-28T00:00:00+00:00, run_start_date=2023-09-11 06:44:21.011906+00:00, run_end_date=2023-09-11 06:44:24.949938+00:00, run_duration=3.938032, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-28 00:00:00+00:00, data_interval_end=2023-03-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:24.953+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-01T00:00:00+00:00, run_after=2023-03-02T00:00:00+00:00[0m
[[34m2023-09-11T06:44:26.055+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-02T00:00:00+00:00, run_after=2023-03-03T00:00:00+00:00[0m
[[34m2023-09-11T06:44:26.100+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:26.100+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:26.101+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:26.103+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:26.104+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:26.104+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:26.107+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:28.061+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:28.202+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:28.381+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:28.415+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:28.895+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:28.895+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:28.970+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:29.030+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:29.751+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:29.762+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:29.120449+00:00, run_end_date=2023-09-11 06:44:29.348920+00:00, run_duration=0.228471, state=success, executor_state=success, try_number=1, max_tries=0, job_id=64, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:26.101873+00:00, queued_by_job_id=2, pid=41307[0m
[[34m2023-09-11T06:44:30.025+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-03T00:00:00+00:00, run_after=2023-03-04T00:00:00+00:00[0m
[[34m2023-09-11T06:44:30.059+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-01 00:00:00+00:00: scheduled__2023-03-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:26.050381+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:30.060+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-01 00:00:00+00:00, run_id=scheduled__2023-03-01T00:00:00+00:00, run_start_date=2023-09-11 06:44:26.068331+00:00, run_end_date=2023-09-11 06:44:30.060425+00:00, run_duration=3.992094, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-01 00:00:00+00:00, data_interval_end=2023-03-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:30.063+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-02T00:00:00+00:00, run_after=2023-03-03T00:00:00+00:00[0m
[[34m2023-09-11T06:44:30.080+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:30.080+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:30.080+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:30.082+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:30.083+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:30.083+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:30.086+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:32.024+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:32.180+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:32.391+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:32.429+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:32.926+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:32.927+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:33.002+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:33.064+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:33.802+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:33.813+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:33.159265+00:00, run_end_date=2023-09-11 06:44:33.389438+00:00, run_duration=0.230173, state=success, executor_state=success, try_number=1, max_tries=0, job_id=65, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:30.081369+00:00, queued_by_job_id=2, pid=41316[0m
[[34m2023-09-11T06:44:34.063+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-03T00:00:00+00:00, run_after=2023-03-04T00:00:00+00:00[0m
[[34m2023-09-11T06:44:34.088+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-02 00:00:00+00:00: scheduled__2023-03-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:30.019756+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:34.089+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-02 00:00:00+00:00, run_id=scheduled__2023-03-02T00:00:00+00:00, run_start_date=2023-09-11 06:44:30.037004+00:00, run_end_date=2023-09-11 06:44:34.089213+00:00, run_duration=4.052209, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-02 00:00:00+00:00, data_interval_end=2023-03-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:34.093+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-03T00:00:00+00:00, run_after=2023-03-04T00:00:00+00:00[0m
[[34m2023-09-11T06:44:35.020+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-04T00:00:00+00:00, run_after=2023-03-05T00:00:00+00:00[0m
[[34m2023-09-11T06:44:35.064+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:35.064+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:35.064+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:35.066+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:35.067+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:35.067+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:35.070+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:36.933+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:37.064+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:37.240+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:37.273+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:37.742+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:37.743+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:37.812+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:37.870+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:38.583+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:38.594+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:37.959929+00:00, run_end_date=2023-09-11 06:44:38.176959+00:00, run_duration=0.21703, state=success, executor_state=success, try_number=1, max_tries=0, job_id=66, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:35.065477+00:00, queued_by_job_id=2, pid=41326[0m
[[34m2023-09-11T06:44:38.854+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-05T00:00:00+00:00, run_after=2023-03-06T00:00:00+00:00[0m
[[34m2023-09-11T06:44:38.898+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-03 00:00:00+00:00: scheduled__2023-03-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:35.015712+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:38.899+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-03 00:00:00+00:00, run_id=scheduled__2023-03-03T00:00:00+00:00, run_start_date=2023-09-11 06:44:35.032079+00:00, run_end_date=2023-09-11 06:44:38.899137+00:00, run_duration=3.867058, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-03 00:00:00+00:00, data_interval_end=2023-03-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:38.902+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-04T00:00:00+00:00, run_after=2023-03-05T00:00:00+00:00[0m
[[34m2023-09-11T06:44:38.917+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:38.918+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:38.918+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:38.920+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:38.921+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:38.921+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:38.924+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:40.810+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:40.947+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:41.129+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:41.163+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:41.655+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:41.656+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:41.729+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:41.790+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:42.528+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:42.539+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:41.880312+00:00, run_end_date=2023-09-11 06:44:42.125643+00:00, run_duration=0.245331, state=success, executor_state=success, try_number=1, max_tries=0, job_id=67, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:38.918990+00:00, queued_by_job_id=2, pid=41333[0m
[[34m2023-09-11T06:44:42.779+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-05T00:00:00+00:00, run_after=2023-03-06T00:00:00+00:00[0m
[[34m2023-09-11T06:44:42.804+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-04 00:00:00+00:00: scheduled__2023-03-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:38.850011+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:42.805+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-04 00:00:00+00:00, run_id=scheduled__2023-03-04T00:00:00+00:00, run_start_date=2023-09-11 06:44:38.866325+00:00, run_end_date=2023-09-11 06:44:42.805560+00:00, run_duration=3.939235, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-04 00:00:00+00:00, data_interval_end=2023-03-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:42.810+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-05T00:00:00+00:00, run_after=2023-03-06T00:00:00+00:00[0m
[[34m2023-09-11T06:44:43.849+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-06T00:00:00+00:00, run_after=2023-03-07T00:00:00+00:00[0m
[[34m2023-09-11T06:44:43.895+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:43.895+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:43.895+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:43.898+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:43.898+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:43.898+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:43.901+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:45.794+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:45.930+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:46.108+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:46.149+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:46.642+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:46.642+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:46.740+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:46.814+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:47.528+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:47.539+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:46.902050+00:00, run_end_date=2023-09-11 06:44:47.130287+00:00, run_duration=0.228237, state=success, executor_state=success, try_number=1, max_tries=0, job_id=68, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:43.896689+00:00, queued_by_job_id=2, pid=41343[0m
[[34m2023-09-11T06:44:47.804+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-07T00:00:00+00:00, run_after=2023-03-08T00:00:00+00:00[0m
[[34m2023-09-11T06:44:47.840+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-05 00:00:00+00:00: scheduled__2023-03-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:43.845119+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:47.841+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-05 00:00:00+00:00, run_id=scheduled__2023-03-05T00:00:00+00:00, run_start_date=2023-09-11 06:44:43.862372+00:00, run_end_date=2023-09-11 06:44:47.840926+00:00, run_duration=3.978554, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-05 00:00:00+00:00, data_interval_end=2023-03-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:47.844+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-06T00:00:00+00:00, run_after=2023-03-07T00:00:00+00:00[0m
[[34m2023-09-11T06:44:47.859+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:47.860+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:47.860+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:47.862+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:47.863+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:47.863+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:47.879+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:49.770+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:49.904+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:50.082+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:50.116+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:50.590+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:50.591+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:50.664+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:50.723+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:51.462+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:51.474+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:50.812880+00:00, run_end_date=2023-09-11 06:44:51.046731+00:00, run_duration=0.233851, state=success, executor_state=success, try_number=1, max_tries=0, job_id=69, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:47.861385+00:00, queued_by_job_id=2, pid=41350[0m
[[34m2023-09-11T06:44:51.729+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-07T00:00:00+00:00, run_after=2023-03-08T00:00:00+00:00[0m
[[34m2023-09-11T06:44:51.755+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-06 00:00:00+00:00: scheduled__2023-03-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:47.799151+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:51.756+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-06 00:00:00+00:00, run_id=scheduled__2023-03-06T00:00:00+00:00, run_start_date=2023-09-11 06:44:47.817105+00:00, run_end_date=2023-09-11 06:44:51.756274+00:00, run_duration=3.939169, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-06 00:00:00+00:00, data_interval_end=2023-03-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:51.761+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-07T00:00:00+00:00, run_after=2023-03-08T00:00:00+00:00[0m
[[34m2023-09-11T06:44:52.800+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-08T00:00:00+00:00, run_after=2023-03-09T00:00:00+00:00[0m
[[34m2023-09-11T06:44:52.845+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:52.845+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:52.845+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:52.847+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:52.848+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:52.848+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:52.851+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:54.765+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:54.898+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:55.107+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:55.144+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:55.675+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:55.676+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:55.750+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:55.813+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:56.593+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:56.604+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:55.907753+00:00, run_end_date=2023-09-11 06:44:56.180103+00:00, run_duration=0.27235, state=success, executor_state=success, try_number=1, max_tries=0, job_id=70, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:52.846511+00:00, queued_by_job_id=2, pid=41361[0m
[[34m2023-09-11T06:44:56.869+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-09T00:00:00+00:00, run_after=2023-03-10T00:00:00+00:00[0m
[[34m2023-09-11T06:44:56.905+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-07 00:00:00+00:00: scheduled__2023-03-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:52.795832+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:56.905+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-07 00:00:00+00:00, run_id=scheduled__2023-03-07T00:00:00+00:00, run_start_date=2023-09-11 06:44:52.813089+00:00, run_end_date=2023-09-11 06:44:56.905467+00:00, run_duration=4.092378, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-07 00:00:00+00:00, data_interval_end=2023-03-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:56.909+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-08T00:00:00+00:00, run_after=2023-03-09T00:00:00+00:00[0m
[[34m2023-09-11T06:44:56.924+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:56.924+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:56.925+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:56.927+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:56.927+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:56.927+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:56.930+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:58.942+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:59.074+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:59.255+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:59.288+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:59.772+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:59.773+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:59.848+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:59.921+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:00.645+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:00.662+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:00.014981+00:00, run_end_date=2023-09-11 06:45:00.244499+00:00, run_duration=0.229518, state=success, executor_state=success, try_number=1, max_tries=0, job_id=71, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:56.925792+00:00, queued_by_job_id=2, pid=41370[0m
[[34m2023-09-11T06:45:00.679+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:45:00.812+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-09T00:00:00+00:00, run_after=2023-03-10T00:00:00+00:00[0m
[[34m2023-09-11T06:45:00.835+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-08 00:00:00+00:00: scheduled__2023-03-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:56.864443+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:00.835+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-08 00:00:00+00:00, run_id=scheduled__2023-03-08T00:00:00+00:00, run_start_date=2023-09-11 06:44:56.882951+00:00, run_end_date=2023-09-11 06:45:00.835621+00:00, run_duration=3.95267, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-08 00:00:00+00:00, data_interval_end=2023-03-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:00.839+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-09T00:00:00+00:00, run_after=2023-03-10T00:00:00+00:00[0m
[[34m2023-09-11T06:45:01.878+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-10T00:00:00+00:00, run_after=2023-03-11T00:00:00+00:00[0m
[[34m2023-09-11T06:45:01.928+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:01.929+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:01.929+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:01.931+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:01.932+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:01.932+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:01.935+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:03.919+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:04.050+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:04.225+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:04.257+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:04.781+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:04.781+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:04.852+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:04.916+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:05.662+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:05.672+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:05.001782+00:00, run_end_date=2023-09-11 06:45:05.236739+00:00, run_duration=0.234957, state=success, executor_state=success, try_number=1, max_tries=0, job_id=72, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:01.929994+00:00, queued_by_job_id=2, pid=41397[0m
[[34m2023-09-11T06:45:05.825+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-11T00:00:00+00:00, run_after=2023-03-12T00:00:00+00:00[0m
[[34m2023-09-11T06:45:05.862+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-09 00:00:00+00:00: scheduled__2023-03-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:01.873855+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:05.862+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-09 00:00:00+00:00, run_id=scheduled__2023-03-09T00:00:00+00:00, run_start_date=2023-09-11 06:45:01.897463+00:00, run_end_date=2023-09-11 06:45:05.862807+00:00, run_duration=3.965344, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-09 00:00:00+00:00, data_interval_end=2023-03-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:05.866+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-10T00:00:00+00:00, run_after=2023-03-11T00:00:00+00:00[0m
[[34m2023-09-11T06:45:05.881+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:05.882+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:05.882+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:05.884+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:05.885+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:05.885+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:05.899+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:07.842+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:07.978+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:08.159+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:08.192+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:08.675+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:08.675+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:08.744+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:08.802+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:09.552+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:09.564+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:08.890309+00:00, run_end_date=2023-09-11 06:45:09.125780+00:00, run_duration=0.235471, state=success, executor_state=success, try_number=1, max_tries=0, job_id=73, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:05.883396+00:00, queued_by_job_id=2, pid=41406[0m
[[34m2023-09-11T06:45:09.713+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-11T00:00:00+00:00, run_after=2023-03-12T00:00:00+00:00[0m
[[34m2023-09-11T06:45:09.741+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-10 00:00:00+00:00: scheduled__2023-03-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:05.819823+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:09.741+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-10 00:00:00+00:00, run_id=scheduled__2023-03-10T00:00:00+00:00, run_start_date=2023-09-11 06:45:05.839577+00:00, run_end_date=2023-09-11 06:45:09.741447+00:00, run_duration=3.90187, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-10 00:00:00+00:00, data_interval_end=2023-03-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:09.745+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-11T00:00:00+00:00, run_after=2023-03-12T00:00:00+00:00[0m
[[34m2023-09-11T06:45:10.812+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-12T00:00:00+00:00, run_after=2023-03-13T00:00:00+00:00[0m
[[34m2023-09-11T06:45:10.860+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:10.860+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:10.861+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:10.863+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:10.863+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:10.864+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:10.866+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:12.769+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:12.898+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:13.071+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:13.103+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:13.564+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:13.564+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:13.635+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:13.694+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:14.402+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:14.413+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:13.779460+00:00, run_end_date=2023-09-11 06:45:13.996424+00:00, run_duration=0.216964, state=success, executor_state=success, try_number=1, max_tries=0, job_id=74, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:10.861835+00:00, queued_by_job_id=2, pid=41416[0m
[[34m2023-09-11T06:45:14.575+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-13T00:00:00+00:00, run_after=2023-03-14T00:00:00+00:00[0m
[[34m2023-09-11T06:45:14.623+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-11 00:00:00+00:00: scheduled__2023-03-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:10.807038+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:14.623+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-11 00:00:00+00:00, run_id=scheduled__2023-03-11T00:00:00+00:00, run_start_date=2023-09-11 06:45:10.825530+00:00, run_end_date=2023-09-11 06:45:14.623656+00:00, run_duration=3.798126, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-11 00:00:00+00:00, data_interval_end=2023-03-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:14.626+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-12T00:00:00+00:00, run_after=2023-03-13T00:00:00+00:00[0m
[[34m2023-09-11T06:45:14.642+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:14.642+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:14.642+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:14.645+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:14.645+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:14.646+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:14.649+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:16.519+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:16.650+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:16.851+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:16.890+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:17.455+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:17.456+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:17.528+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:17.608+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:18.428+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:18.440+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:17.729668+00:00, run_end_date=2023-09-11 06:45:17.981491+00:00, run_duration=0.251823, state=success, executor_state=success, try_number=1, max_tries=0, job_id=75, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:14.643546+00:00, queued_by_job_id=2, pid=41425[0m
[[34m2023-09-11T06:45:18.687+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-13T00:00:00+00:00, run_after=2023-03-14T00:00:00+00:00[0m
[[34m2023-09-11T06:45:18.709+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-12 00:00:00+00:00: scheduled__2023-03-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:14.569848+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:18.710+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-12 00:00:00+00:00, run_id=scheduled__2023-03-12T00:00:00+00:00, run_start_date=2023-09-11 06:45:14.599264+00:00, run_end_date=2023-09-11 06:45:18.710408+00:00, run_duration=4.111144, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-12 00:00:00+00:00, data_interval_end=2023-03-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:18.713+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-13T00:00:00+00:00, run_after=2023-03-14T00:00:00+00:00[0m
[[34m2023-09-11T06:45:19.678+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-14T00:00:00+00:00, run_after=2023-03-15T00:00:00+00:00[0m
[[34m2023-09-11T06:45:19.732+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:19.733+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:19.733+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:19.735+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:19.735+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:19.736+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:19.739+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:21.701+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:21.838+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:22.016+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:22.050+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:22.546+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:22.546+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:22.619+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:22.683+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:23.458+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:23.471+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:22.775767+00:00, run_end_date=2023-09-11 06:45:23.011261+00:00, run_duration=0.235494, state=success, executor_state=success, try_number=1, max_tries=0, job_id=76, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:19.734095+00:00, queued_by_job_id=2, pid=41435[0m
[[34m2023-09-11T06:45:23.739+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-15T00:00:00+00:00, run_after=2023-03-16T00:00:00+00:00[0m
[[34m2023-09-11T06:45:23.778+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-13 00:00:00+00:00: scheduled__2023-03-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:19.673486+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:23.778+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-13 00:00:00+00:00, run_id=scheduled__2023-03-13T00:00:00+00:00, run_start_date=2023-09-11 06:45:19.699760+00:00, run_end_date=2023-09-11 06:45:23.778575+00:00, run_duration=4.078815, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-13 00:00:00+00:00, data_interval_end=2023-03-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:23.782+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-14T00:00:00+00:00, run_after=2023-03-15T00:00:00+00:00[0m
[[34m2023-09-11T06:45:23.799+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:23.799+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:23.799+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:23.802+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:23.803+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:23.804+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:23.807+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:25.775+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:25.911+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:26.090+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:26.125+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:26.608+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:26.609+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:26.683+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:26.743+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:27.497+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:27.509+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:26.832833+00:00, run_end_date=2023-09-11 06:45:27.076075+00:00, run_duration=0.243242, state=success, executor_state=success, try_number=1, max_tries=0, job_id=77, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:23.801328+00:00, queued_by_job_id=2, pid=41442[0m
[[34m2023-09-11T06:45:27.784+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-15T00:00:00+00:00, run_after=2023-03-16T00:00:00+00:00[0m
[[34m2023-09-11T06:45:27.808+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-14 00:00:00+00:00: scheduled__2023-03-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:23.733999+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:27.809+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-14 00:00:00+00:00, run_id=scheduled__2023-03-14T00:00:00+00:00, run_start_date=2023-09-11 06:45:23.754053+00:00, run_end_date=2023-09-11 06:45:27.808901+00:00, run_duration=4.054848, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-14 00:00:00+00:00, data_interval_end=2023-03-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:27.812+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-15T00:00:00+00:00, run_after=2023-03-16T00:00:00+00:00[0m
[[34m2023-09-11T06:45:28.804+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-16T00:00:00+00:00, run_after=2023-03-17T00:00:00+00:00[0m
[[34m2023-09-11T06:45:28.849+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:28.850+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:28.850+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:28.852+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:28.853+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:28.853+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:28.856+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:30.811+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:30.960+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:31.152+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:31.192+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:31.709+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:31.710+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:31.783+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:31.848+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:32.627+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:32.638+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:31.945613+00:00, run_end_date=2023-09-11 06:45:32.182979+00:00, run_duration=0.237366, state=success, executor_state=success, try_number=1, max_tries=0, job_id=78, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:28.850939+00:00, queued_by_job_id=2, pid=41452[0m
[[34m2023-09-11T06:45:32.911+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00[0m
[[34m2023-09-11T06:45:32.949+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-15 00:00:00+00:00: scheduled__2023-03-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:28.799554+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:32.950+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-15 00:00:00+00:00, run_id=scheduled__2023-03-15T00:00:00+00:00, run_start_date=2023-09-11 06:45:28.816592+00:00, run_end_date=2023-09-11 06:45:32.949879+00:00, run_duration=4.133287, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-15 00:00:00+00:00, data_interval_end=2023-03-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:32.954+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-16T00:00:00+00:00, run_after=2023-03-17T00:00:00+00:00[0m
[[34m2023-09-11T06:45:32.970+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:32.970+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:32.970+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:32.972+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:32.973+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:32.973+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:32.976+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:34.964+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:35.114+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:35.309+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:35.345+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:35.842+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:35.843+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:35.923+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:35.984+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:36.772+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:36.782+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:36.073198+00:00, run_end_date=2023-09-11 06:45:36.323834+00:00, run_duration=0.250636, state=success, executor_state=success, try_number=1, max_tries=0, job_id=79, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:32.971486+00:00, queued_by_job_id=2, pid=41459[0m
[[34m2023-09-11T06:45:37.029+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00[0m
[[34m2023-09-11T06:45:37.052+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-16 00:00:00+00:00: scheduled__2023-03-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:32.906784+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:37.052+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-16 00:00:00+00:00, run_id=scheduled__2023-03-16T00:00:00+00:00, run_start_date=2023-09-11 06:45:32.925811+00:00, run_end_date=2023-09-11 06:45:37.052740+00:00, run_duration=4.126929, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-16 00:00:00+00:00, data_interval_end=2023-03-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:37.056+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00[0m
[[34m2023-09-11T06:45:37.907+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00[0m
[[34m2023-09-11T06:45:37.952+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:37.952+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:37.953+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:37.955+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:37.956+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:37.956+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:37.959+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:39.990+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:40.131+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:40.315+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:40.351+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:40.909+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:40.909+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:40.982+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:41.046+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:41.873+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:41.885+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:41.138558+00:00, run_end_date=2023-09-11 06:45:41.405071+00:00, run_duration=0.266513, state=success, executor_state=success, try_number=1, max_tries=0, job_id=80, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:37.953814+00:00, queued_by_job_id=2, pid=41469[0m
[[34m2023-09-11T06:45:42.152+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-19T00:00:00+00:00, run_after=2023-03-20T00:00:00+00:00[0m
[[34m2023-09-11T06:45:42.190+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-17 00:00:00+00:00: scheduled__2023-03-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:37.903035+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:42.190+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-17 00:00:00+00:00, run_id=scheduled__2023-03-17T00:00:00+00:00, run_start_date=2023-09-11 06:45:37.921033+00:00, run_end_date=2023-09-11 06:45:42.190535+00:00, run_duration=4.269502, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-17 00:00:00+00:00, data_interval_end=2023-03-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:42.194+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00[0m
[[34m2023-09-11T06:45:42.210+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:42.210+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:42.210+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:42.212+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:42.213+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:42.213+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:42.229+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:44.337+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:44.479+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:44.662+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:44.696+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:45.208+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:45.209+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:45.290+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:45.357+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:46.154+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:46.166+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:45.452206+00:00, run_end_date=2023-09-11 06:45:45.704319+00:00, run_duration=0.252113, state=success, executor_state=success, try_number=1, max_tries=0, job_id=81, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:42.211328+00:00, queued_by_job_id=2, pid=41478[0m
[[34m2023-09-11T06:45:46.410+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-19T00:00:00+00:00, run_after=2023-03-20T00:00:00+00:00[0m
[[34m2023-09-11T06:45:46.435+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-18 00:00:00+00:00: scheduled__2023-03-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:42.147405+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:46.435+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-18 00:00:00+00:00, run_id=scheduled__2023-03-18T00:00:00+00:00, run_start_date=2023-09-11 06:45:42.165319+00:00, run_end_date=2023-09-11 06:45:46.435517+00:00, run_duration=4.270198, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-18 00:00:00+00:00, data_interval_end=2023-03-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:46.439+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-19T00:00:00+00:00, run_after=2023-03-20T00:00:00+00:00[0m
[[34m2023-09-11T06:45:47.210+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-20T00:00:00+00:00, run_after=2023-03-21T00:00:00+00:00[0m
[[34m2023-09-11T06:45:47.258+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:47.258+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:47.258+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:47.261+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:47.261+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:47.262+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:47.264+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:49.214+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:49.355+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:49.541+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:49.574+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:50.057+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:50.058+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:50.137+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:50.197+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:50.974+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:50.986+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:50.286266+00:00, run_end_date=2023-09-11 06:45:50.533361+00:00, run_duration=0.247095, state=success, executor_state=success, try_number=1, max_tries=0, job_id=82, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:47.259573+00:00, queued_by_job_id=2, pid=41488[0m
[[34m2023-09-11T06:45:51.255+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-21T00:00:00+00:00, run_after=2023-03-22T00:00:00+00:00[0m
[[34m2023-09-11T06:45:51.295+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-19 00:00:00+00:00: scheduled__2023-03-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:47.205427+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:51.296+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-19 00:00:00+00:00, run_id=scheduled__2023-03-19T00:00:00+00:00, run_start_date=2023-09-11 06:45:47.223455+00:00, run_end_date=2023-09-11 06:45:51.296681+00:00, run_duration=4.073226, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-19 00:00:00+00:00, data_interval_end=2023-03-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:51.301+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-20T00:00:00+00:00, run_after=2023-03-21T00:00:00+00:00[0m
[[34m2023-09-11T06:45:51.317+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:51.317+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:51.317+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:51.320+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:51.321+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:51.321+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:51.324+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:53.305+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:53.443+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:53.627+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:53.661+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:54.175+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:54.176+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:54.255+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:54.317+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:55.053+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:55.063+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:54.413512+00:00, run_end_date=2023-09-11 06:45:54.650214+00:00, run_duration=0.236702, state=success, executor_state=success, try_number=1, max_tries=0, job_id=83, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:51.318642+00:00, queued_by_job_id=2, pid=41497[0m
[[34m2023-09-11T06:45:55.317+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-21T00:00:00+00:00, run_after=2023-03-22T00:00:00+00:00[0m
[[34m2023-09-11T06:45:55.341+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-20 00:00:00+00:00: scheduled__2023-03-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:51.249310+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:55.342+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-20 00:00:00+00:00, run_id=scheduled__2023-03-20T00:00:00+00:00, run_start_date=2023-09-11 06:45:51.269833+00:00, run_end_date=2023-09-11 06:45:55.342154+00:00, run_duration=4.072321, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-20 00:00:00+00:00, data_interval_end=2023-03-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:55.345+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-21T00:00:00+00:00, run_after=2023-03-22T00:00:00+00:00[0m
[[34m2023-09-11T06:45:56.248+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-22T00:00:00+00:00, run_after=2023-03-23T00:00:00+00:00[0m
[[34m2023-09-11T06:45:56.292+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:56.293+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:56.293+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:56.295+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:56.295+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:56.296+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:56.298+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:58.338+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:58.489+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:58.671+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:58.705+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:59.195+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:59.195+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:59.272+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:59.333+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:00.143+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:00.155+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:59.430363+00:00, run_end_date=2023-09-11 06:45:59.669784+00:00, run_duration=0.239421, state=success, executor_state=success, try_number=1, max_tries=0, job_id=84, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:56.294090+00:00, queued_by_job_id=2, pid=41507[0m
[[34m2023-09-11T06:46:00.666+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-23T00:00:00+00:00, run_after=2023-03-24T00:00:00+00:00[0m
[[34m2023-09-11T06:46:00.703+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-21 00:00:00+00:00: scheduled__2023-03-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:56.244066+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:00.704+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-21 00:00:00+00:00, run_id=scheduled__2023-03-21T00:00:00+00:00, run_start_date=2023-09-11 06:45:56.260488+00:00, run_end_date=2023-09-11 06:46:00.704267+00:00, run_duration=4.443779, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-21 00:00:00+00:00, data_interval_end=2023-03-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:00.707+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-22T00:00:00+00:00, run_after=2023-03-23T00:00:00+00:00[0m
[[34m2023-09-11T06:46:00.723+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:00.723+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:00.723+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:00.726+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:00.726+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:00.727+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:00.730+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:02.667+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:02.804+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:02.994+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:03.027+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:03.514+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:03.514+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:03.587+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:03.647+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:04.371+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:04.382+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:03.736927+00:00, run_end_date=2023-09-11 06:46:03.964867+00:00, run_duration=0.22794, state=success, executor_state=success, try_number=1, max_tries=0, job_id=85, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:00.724649+00:00, queued_by_job_id=2, pid=41516[0m
[[34m2023-09-11T06:46:04.835+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-23T00:00:00+00:00, run_after=2023-03-24T00:00:00+00:00[0m
[[34m2023-09-11T06:46:04.859+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-22 00:00:00+00:00: scheduled__2023-03-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:00.660801+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:04.860+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-22 00:00:00+00:00, run_id=scheduled__2023-03-22T00:00:00+00:00, run_start_date=2023-09-11 06:46:00.679349+00:00, run_end_date=2023-09-11 06:46:04.860410+00:00, run_duration=4.181061, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-22 00:00:00+00:00, data_interval_end=2023-03-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:04.863+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-23T00:00:00+00:00, run_after=2023-03-24T00:00:00+00:00[0m
[[34m2023-09-11T06:46:05.421+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-24T00:00:00+00:00, run_after=2023-03-25T00:00:00+00:00[0m
[[34m2023-09-11T06:46:05.480+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:05.480+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:05.480+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:05.482+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:05.483+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:05.483+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:05.486+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:07.408+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:07.540+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:07.722+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:07.757+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:08.261+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:08.261+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:08.333+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:08.394+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:09.108+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:09.123+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:08.483415+00:00, run_end_date=2023-09-11 06:46:08.713286+00:00, run_duration=0.229871, state=success, executor_state=success, try_number=1, max_tries=0, job_id=86, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:05.481529+00:00, queued_by_job_id=2, pid=41526[0m
[[34m2023-09-11T06:46:09.702+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-25T00:00:00+00:00, run_after=2023-03-26T00:00:00+00:00[0m
[[34m2023-09-11T06:46:09.739+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-23 00:00:00+00:00: scheduled__2023-03-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:05.415374+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:09.740+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-23 00:00:00+00:00, run_id=scheduled__2023-03-23T00:00:00+00:00, run_start_date=2023-09-11 06:46:05.433221+00:00, run_end_date=2023-09-11 06:46:09.740299+00:00, run_duration=4.307078, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-23 00:00:00+00:00, data_interval_end=2023-03-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:09.744+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-24T00:00:00+00:00, run_after=2023-03-25T00:00:00+00:00[0m
[[34m2023-09-11T06:46:09.760+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:09.760+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:09.761+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:09.763+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:09.763+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:09.764+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:09.767+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:11.693+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:11.829+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:12.020+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:12.053+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:12.538+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:12.538+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:12.612+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:12.674+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:13.587+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:13.601+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:12.766227+00:00, run_end_date=2023-09-11 06:46:13.006359+00:00, run_duration=0.240132, state=success, executor_state=success, try_number=1, max_tries=0, job_id=87, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:09.761917+00:00, queued_by_job_id=2, pid=41535[0m
[[34m2023-09-11T06:46:14.114+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-25T00:00:00+00:00, run_after=2023-03-26T00:00:00+00:00[0m
[[34m2023-09-11T06:46:14.139+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-24 00:00:00+00:00: scheduled__2023-03-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:09.696883+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:14.140+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-24 00:00:00+00:00, run_id=scheduled__2023-03-24T00:00:00+00:00, run_start_date=2023-09-11 06:46:09.716263+00:00, run_end_date=2023-09-11 06:46:14.140344+00:00, run_duration=4.424081, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-24 00:00:00+00:00, data_interval_end=2023-03-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:14.144+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-25T00:00:00+00:00, run_after=2023-03-26T00:00:00+00:00[0m
[[34m2023-09-11T06:46:14.655+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-26T00:00:00+00:00, run_after=2023-03-27T00:00:00+00:00[0m
[[34m2023-09-11T06:46:14.699+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:14.699+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:14.700+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:14.702+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:14.703+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:14.703+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:14.706+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:16.637+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:16.772+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:16.966+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:17.012+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:17.557+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:17.557+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:17.630+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:17.689+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:18.482+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:18.493+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:17.777701+00:00, run_end_date=2023-09-11 06:46:18.012112+00:00, run_duration=0.234411, state=success, executor_state=success, try_number=1, max_tries=0, job_id=88, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:14.701015+00:00, queued_by_job_id=2, pid=41545[0m
[[34m2023-09-11T06:46:18.769+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-27T00:00:00+00:00, run_after=2023-03-28T00:00:00+00:00[0m
[[34m2023-09-11T06:46:18.809+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-25 00:00:00+00:00: scheduled__2023-03-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:14.650689+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:18.809+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-25 00:00:00+00:00, run_id=scheduled__2023-03-25T00:00:00+00:00, run_start_date=2023-09-11 06:46:14.667716+00:00, run_end_date=2023-09-11 06:46:18.809550+00:00, run_duration=4.141834, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-25 00:00:00+00:00, data_interval_end=2023-03-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:18.813+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-26T00:00:00+00:00, run_after=2023-03-27T00:00:00+00:00[0m
[[34m2023-09-11T06:46:18.829+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:18.829+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:18.830+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:18.832+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:18.833+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:18.833+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:18.836+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:20.823+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:20.974+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:21.169+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:21.204+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:21.717+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:21.717+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:21.795+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:21.861+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:22.840+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:22.851+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:21.957416+00:00, run_end_date=2023-09-11 06:46:22.411353+00:00, run_duration=0.453937, state=success, executor_state=success, try_number=1, max_tries=0, job_id=89, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:18.830822+00:00, queued_by_job_id=2, pid=41552[0m
[[34m2023-09-11T06:46:23.399+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-27T00:00:00+00:00, run_after=2023-03-28T00:00:00+00:00[0m
[[34m2023-09-11T06:46:23.426+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-26 00:00:00+00:00: scheduled__2023-03-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:18.763890+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:23.427+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-26 00:00:00+00:00, run_id=scheduled__2023-03-26T00:00:00+00:00, run_start_date=2023-09-11 06:46:18.782612+00:00, run_end_date=2023-09-11 06:46:23.427227+00:00, run_duration=4.644615, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-26 00:00:00+00:00, data_interval_end=2023-03-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:23.430+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-27T00:00:00+00:00, run_after=2023-03-28T00:00:00+00:00[0m
[[34m2023-09-11T06:46:24.252+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-28T00:00:00+00:00, run_after=2023-03-29T00:00:00+00:00[0m
[[34m2023-09-11T06:46:24.331+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:24.331+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:24.332+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:24.334+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:24.335+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:24.336+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:24.339+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:26.243+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:26.378+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:26.565+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:26.599+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:27.082+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:27.082+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:27.184+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:27.244+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:27.974+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:27.989+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:27.331666+00:00, run_end_date=2023-09-11 06:46:27.566152+00:00, run_duration=0.234486, state=success, executor_state=success, try_number=1, max_tries=0, job_id=90, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:24.332741+00:00, queued_by_job_id=2, pid=41562[0m
[[34m2023-09-11T06:46:28.496+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-29T00:00:00+00:00, run_after=2023-03-30T00:00:00+00:00[0m
[[34m2023-09-11T06:46:28.548+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-27 00:00:00+00:00: scheduled__2023-03-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:24.246822+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:28.549+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-27 00:00:00+00:00, run_id=scheduled__2023-03-27T00:00:00+00:00, run_start_date=2023-09-11 06:46:24.297233+00:00, run_end_date=2023-09-11 06:46:28.549332+00:00, run_duration=4.252099, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-27 00:00:00+00:00, data_interval_end=2023-03-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:28.553+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-28T00:00:00+00:00, run_after=2023-03-29T00:00:00+00:00[0m
[[34m2023-09-11T06:46:28.583+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:28.583+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:28.584+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:28.586+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:28.587+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:28.587+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:28.590+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:30.629+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:30.781+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:31.024+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:31.062+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:31.585+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:31.588+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:31.699+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:31.768+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:32.550+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:32.561+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:31.870545+00:00, run_end_date=2023-09-11 06:46:32.106138+00:00, run_duration=0.235593, state=success, executor_state=success, try_number=1, max_tries=0, job_id=91, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:28.585216+00:00, queued_by_job_id=2, pid=41571[0m
[[34m2023-09-11T06:46:33.006+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-29T00:00:00+00:00, run_after=2023-03-30T00:00:00+00:00[0m
[[34m2023-09-11T06:46:33.029+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-28 00:00:00+00:00: scheduled__2023-03-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:28.491486+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:33.030+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-28 00:00:00+00:00, run_id=scheduled__2023-03-28T00:00:00+00:00, run_start_date=2023-09-11 06:46:28.509598+00:00, run_end_date=2023-09-11 06:46:33.029973+00:00, run_duration=4.520375, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-28 00:00:00+00:00, data_interval_end=2023-03-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:33.033+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-29T00:00:00+00:00, run_after=2023-03-30T00:00:00+00:00[0m
[[34m2023-09-11T06:46:34.362+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-30T00:00:00+00:00, run_after=2023-03-31T00:00:00+00:00[0m
[[34m2023-09-11T06:46:34.416+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:34.416+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:34.417+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:34.419+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:34.420+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:34.420+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:34.423+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:36.435+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:36.568+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:36.742+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:36.775+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:37.263+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:37.263+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:37.337+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:37.402+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:38.204+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:38.215+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:37.497233+00:00, run_end_date=2023-09-11 06:46:37.746668+00:00, run_duration=0.249435, state=success, executor_state=success, try_number=1, max_tries=0, job_id=92, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:34.417960+00:00, queued_by_job_id=2, pid=41581[0m
[[34m2023-09-11T06:46:38.372+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-31T00:00:00+00:00, run_after=2023-04-01T00:00:00+00:00[0m
[[34m2023-09-11T06:46:38.410+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-29 00:00:00+00:00: scheduled__2023-03-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:34.357982+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:38.410+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-29 00:00:00+00:00, run_id=scheduled__2023-03-29T00:00:00+00:00, run_start_date=2023-09-11 06:46:34.375764+00:00, run_end_date=2023-09-11 06:46:38.410682+00:00, run_duration=4.034918, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-29 00:00:00+00:00, data_interval_end=2023-03-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:38.414+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-30T00:00:00+00:00, run_after=2023-03-31T00:00:00+00:00[0m
[[34m2023-09-11T06:46:38.429+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:38.430+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:38.430+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:38.432+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:38.433+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:38.433+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:38.436+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:40.435+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:40.578+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:40.761+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:40.797+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:41.362+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:41.363+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:41.493+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:41.596+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:42.838+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:42.869+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:41.732139+00:00, run_end_date=2023-09-11 06:46:42.202900+00:00, run_duration=0.470761, state=success, executor_state=success, try_number=1, max_tries=0, job_id=93, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:38.431244+00:00, queued_by_job_id=2, pid=41588[0m
[[34m2023-09-11T06:46:43.326+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-31T00:00:00+00:00, run_after=2023-04-01T00:00:00+00:00[0m
[[34m2023-09-11T06:46:43.353+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-30 00:00:00+00:00: scheduled__2023-03-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:38.365817+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:43.354+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-30 00:00:00+00:00, run_id=scheduled__2023-03-30T00:00:00+00:00, run_start_date=2023-09-11 06:46:38.386109+00:00, run_end_date=2023-09-11 06:46:43.353806+00:00, run_duration=4.967697, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-30 00:00:00+00:00, data_interval_end=2023-03-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:43.357+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-31T00:00:00+00:00, run_after=2023-04-01T00:00:00+00:00[0m
[[34m2023-09-11T06:46:44.649+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-01T00:00:00+00:00, run_after=2023-04-02T00:00:00+00:00[0m
[[34m2023-09-11T06:46:44.701+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:44.701+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:44.702+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:44.705+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:44.705+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:44.706+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:44.708+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:46.864+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:47.024+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:47.275+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:47.311+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:47.839+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:47.840+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:47.921+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:47.986+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:48.749+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:48.759+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:48.079553+00:00, run_end_date=2023-09-11 06:46:48.351322+00:00, run_duration=0.271769, state=success, executor_state=success, try_number=1, max_tries=0, job_id=94, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:44.703184+00:00, queued_by_job_id=2, pid=41600[0m
[[34m2023-09-11T06:46:49.260+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-02T00:00:00+00:00, run_after=2023-04-03T00:00:00+00:00[0m
[[34m2023-09-11T06:46:49.297+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-31 00:00:00+00:00: scheduled__2023-03-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:44.644358+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:49.297+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-31 00:00:00+00:00, run_id=scheduled__2023-03-31T00:00:00+00:00, run_start_date=2023-09-11 06:46:44.662211+00:00, run_end_date=2023-09-11 06:46:49.297401+00:00, run_duration=4.63519, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-31 00:00:00+00:00, data_interval_end=2023-04-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:49.301+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-01T00:00:00+00:00, run_after=2023-04-02T00:00:00+00:00[0m
[[34m2023-09-11T06:46:49.319+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:49.319+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:49.319+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:49.322+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:49.322+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:49.322+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:49.325+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:51.158+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:51.286+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:51.457+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:51.489+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:51.960+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:51.961+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:52.038+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:52.098+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:52.808+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:52.819+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:52.184516+00:00, run_end_date=2023-09-11 06:46:52.418701+00:00, run_duration=0.234185, state=success, executor_state=success, try_number=1, max_tries=0, job_id=95, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:49.320688+00:00, queued_by_job_id=2, pid=41607[0m
[[34m2023-09-11T06:46:53.071+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-02T00:00:00+00:00, run_after=2023-04-03T00:00:00+00:00[0m
[[34m2023-09-11T06:46:53.096+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-01 00:00:00+00:00: scheduled__2023-04-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:49.255687+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:53.097+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-01 00:00:00+00:00, run_id=scheduled__2023-04-01T00:00:00+00:00, run_start_date=2023-09-11 06:46:49.274699+00:00, run_end_date=2023-09-11 06:46:53.097132+00:00, run_duration=3.822433, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-01 00:00:00+00:00, data_interval_end=2023-04-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:53.100+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-02T00:00:00+00:00, run_after=2023-04-03T00:00:00+00:00[0m
[[34m2023-09-11T06:46:53.678+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-03T00:00:00+00:00, run_after=2023-04-04T00:00:00+00:00[0m
[[34m2023-09-11T06:46:53.723+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:53.723+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:53.723+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:53.725+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:53.726+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:53.726+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:53.729+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:55.587+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:55.715+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:55.887+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:55.920+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:56.391+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:56.392+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:56.463+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:56.520+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:57.282+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:57.293+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:56.606519+00:00, run_end_date=2023-09-11 06:46:56.824096+00:00, run_duration=0.217577, state=success, executor_state=success, try_number=1, max_tries=0, job_id=96, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:53.724427+00:00, queued_by_job_id=2, pid=41615[0m
[[34m2023-09-11T06:46:57.451+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-04T00:00:00+00:00, run_after=2023-04-05T00:00:00+00:00[0m
[[34m2023-09-11T06:46:57.488+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-02 00:00:00+00:00: scheduled__2023-04-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:53.673738+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:57.488+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-02 00:00:00+00:00, run_id=scheduled__2023-04-02T00:00:00+00:00, run_start_date=2023-09-11 06:46:53.690714+00:00, run_end_date=2023-09-11 06:46:57.488884+00:00, run_duration=3.79817, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-02 00:00:00+00:00, data_interval_end=2023-04-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:57.492+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-03T00:00:00+00:00, run_after=2023-04-04T00:00:00+00:00[0m
[[34m2023-09-11T06:46:57.508+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:57.508+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:57.509+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:57.511+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:57.511+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:57.512+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:57.515+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:59.396+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:59.528+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:59.739+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:59.771+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:00.257+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:00.258+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:00.335+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:00.395+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:01.105+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:01.117+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:00.481559+00:00, run_end_date=2023-09-11 06:47:00.700484+00:00, run_duration=0.218925, state=success, executor_state=success, try_number=1, max_tries=0, job_id=97, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:57.509839+00:00, queued_by_job_id=2, pid=41624[0m
[[34m2023-09-11T06:47:01.371+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-04T00:00:00+00:00, run_after=2023-04-05T00:00:00+00:00[0m
[[34m2023-09-11T06:47:01.408+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-03 00:00:00+00:00: scheduled__2023-04-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:57.446831+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:01.408+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-03 00:00:00+00:00, run_id=scheduled__2023-04-03T00:00:00+00:00, run_start_date=2023-09-11 06:46:57.465004+00:00, run_end_date=2023-09-11 06:47:01.408692+00:00, run_duration=3.943688, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-03 00:00:00+00:00, data_interval_end=2023-04-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:01.412+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-04T00:00:00+00:00, run_after=2023-04-05T00:00:00+00:00[0m
[[34m2023-09-11T06:47:02.541+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-05T00:00:00+00:00, run_after=2023-04-06T00:00:00+00:00[0m
[[34m2023-09-11T06:47:02.584+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:02.585+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:02.585+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:02.587+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:02.588+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:02.588+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:02.591+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:04.432+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:04.561+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:04.744+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:04.778+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:05.247+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:05.248+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:05.317+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:05.374+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:06.082+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:06.093+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:05.458716+00:00, run_end_date=2023-09-11 06:47:05.681532+00:00, run_duration=0.222816, state=success, executor_state=success, try_number=1, max_tries=0, job_id=98, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:02.586090+00:00, queued_by_job_id=2, pid=41634[0m
[[34m2023-09-11T06:47:06.260+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-06T00:00:00+00:00, run_after=2023-04-07T00:00:00+00:00[0m
[[34m2023-09-11T06:47:06.296+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-04 00:00:00+00:00: scheduled__2023-04-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:02.536847+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:06.297+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-04 00:00:00+00:00, run_id=scheduled__2023-04-04T00:00:00+00:00, run_start_date=2023-09-11 06:47:02.553575+00:00, run_end_date=2023-09-11 06:47:06.297204+00:00, run_duration=3.743629, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-04 00:00:00+00:00, data_interval_end=2023-04-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:06.300+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-05T00:00:00+00:00, run_after=2023-04-06T00:00:00+00:00[0m
[[34m2023-09-11T06:47:06.315+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:06.316+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:06.316+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:06.318+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:06.319+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:06.319+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:06.322+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:08.166+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:08.299+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:08.466+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:08.498+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:08.999+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:09.000+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:09.068+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:09.125+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:09.830+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:09.841+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:09.212598+00:00, run_end_date=2023-09-11 06:47:09.442642+00:00, run_duration=0.230044, state=success, executor_state=success, try_number=1, max_tries=0, job_id=99, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:06.317148+00:00, queued_by_job_id=2, pid=41643[0m
[[34m2023-09-11T06:47:09.988+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-06T00:00:00+00:00, run_after=2023-04-07T00:00:00+00:00[0m
[[34m2023-09-11T06:47:10.011+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-05 00:00:00+00:00: scheduled__2023-04-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:06.255673+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:10.011+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-05 00:00:00+00:00, run_id=scheduled__2023-04-05T00:00:00+00:00, run_start_date=2023-09-11 06:47:06.273911+00:00, run_end_date=2023-09-11 06:47:10.011721+00:00, run_duration=3.73781, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-05 00:00:00+00:00, data_interval_end=2023-04-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:10.015+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-06T00:00:00+00:00, run_after=2023-04-07T00:00:00+00:00[0m
[[34m2023-09-11T06:47:11.222+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-07T00:00:00+00:00, run_after=2023-04-08T00:00:00+00:00[0m
[[34m2023-09-11T06:47:11.267+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:11.267+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:11.267+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:11.270+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:11.270+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:11.270+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:11.273+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:13.106+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:13.238+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:13.412+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:13.444+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:13.902+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:13.902+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:13.971+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:14.028+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:14.746+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:14.756+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:14.119332+00:00, run_end_date=2023-09-11 06:47:14.339902+00:00, run_duration=0.22057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=100, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:11.268317+00:00, queued_by_job_id=2, pid=41653[0m
[[34m2023-09-11T06:47:15.013+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-08T00:00:00+00:00, run_after=2023-04-09T00:00:00+00:00[0m
[[34m2023-09-11T06:47:15.048+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-06 00:00:00+00:00: scheduled__2023-04-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:11.217913+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:15.049+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-06 00:00:00+00:00, run_id=scheduled__2023-04-06T00:00:00+00:00, run_start_date=2023-09-11 06:47:11.234900+00:00, run_end_date=2023-09-11 06:47:15.048856+00:00, run_duration=3.813956, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-06 00:00:00+00:00, data_interval_end=2023-04-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:15.052+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-07T00:00:00+00:00, run_after=2023-04-08T00:00:00+00:00[0m
[[34m2023-09-11T06:47:15.067+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:15.067+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:15.067+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:15.070+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:15.070+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:15.071+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:15.074+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:16.915+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:17.043+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:17.252+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:17.292+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:17.760+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:17.760+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:17.831+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:17.887+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:18.614+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:18.625+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:17.983053+00:00, run_end_date=2023-09-11 06:47:18.221688+00:00, run_duration=0.238635, state=success, executor_state=success, try_number=1, max_tries=0, job_id=101, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:15.068720+00:00, queued_by_job_id=2, pid=41660[0m
[[34m2023-09-11T06:47:18.878+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-08T00:00:00+00:00, run_after=2023-04-09T00:00:00+00:00[0m
[[34m2023-09-11T06:47:18.905+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-07 00:00:00+00:00: scheduled__2023-04-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:15.008522+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:18.905+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-07 00:00:00+00:00, run_id=scheduled__2023-04-07T00:00:00+00:00, run_start_date=2023-09-11 06:47:15.025732+00:00, run_end_date=2023-09-11 06:47:18.905453+00:00, run_duration=3.879721, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-07 00:00:00+00:00, data_interval_end=2023-04-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:18.908+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-08T00:00:00+00:00, run_after=2023-04-09T00:00:00+00:00[0m
[[34m2023-09-11T06:47:20.011+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-09T00:00:00+00:00, run_after=2023-04-10T00:00:00+00:00[0m
[[34m2023-09-11T06:47:20.055+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:20.055+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:20.055+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:20.057+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:20.058+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:20.058+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:20.061+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:21.912+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:22.042+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:22.218+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:22.251+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:22.722+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:22.723+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:22.793+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:22.850+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:23.559+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:23.570+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:22.940284+00:00, run_end_date=2023-09-11 06:47:23.162963+00:00, run_duration=0.222679, state=success, executor_state=success, try_number=1, max_tries=0, job_id=102, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:20.056323+00:00, queued_by_job_id=2, pid=41670[0m
[[34m2023-09-11T06:47:23.832+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-10T00:00:00+00:00, run_after=2023-04-11T00:00:00+00:00[0m
[[34m2023-09-11T06:47:23.868+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-08 00:00:00+00:00: scheduled__2023-04-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:20.006555+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:23.868+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-08 00:00:00+00:00, run_id=scheduled__2023-04-08T00:00:00+00:00, run_start_date=2023-09-11 06:47:20.023114+00:00, run_end_date=2023-09-11 06:47:23.868782+00:00, run_duration=3.845668, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-08 00:00:00+00:00, data_interval_end=2023-04-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:23.872+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-09T00:00:00+00:00, run_after=2023-04-10T00:00:00+00:00[0m
[[34m2023-09-11T06:47:23.887+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:23.898+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:23.898+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:23.901+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:23.901+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:23.902+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:23.918+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:25.790+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:25.927+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:26.097+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:26.135+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:26.600+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:26.601+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:26.671+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:26.731+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:27.437+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:27.448+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:26.815218+00:00, run_end_date=2023-09-11 06:47:27.046976+00:00, run_duration=0.231758, state=success, executor_state=success, try_number=1, max_tries=0, job_id=103, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:23.899626+00:00, queued_by_job_id=2, pid=41677[0m
[[34m2023-09-11T06:47:27.685+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-10T00:00:00+00:00, run_after=2023-04-11T00:00:00+00:00[0m
[[34m2023-09-11T06:47:27.708+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-09 00:00:00+00:00: scheduled__2023-04-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:23.827235+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:27.708+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-09 00:00:00+00:00, run_id=scheduled__2023-04-09T00:00:00+00:00, run_start_date=2023-09-11 06:47:23.845782+00:00, run_end_date=2023-09-11 06:47:27.708721+00:00, run_duration=3.862939, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-09 00:00:00+00:00, data_interval_end=2023-04-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:27.712+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-10T00:00:00+00:00, run_after=2023-04-11T00:00:00+00:00[0m
[[34m2023-09-11T06:47:28.802+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-11T00:00:00+00:00, run_after=2023-04-12T00:00:00+00:00[0m
[[34m2023-09-11T06:47:28.845+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:28.845+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:28.846+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:28.848+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:28.849+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:28.850+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:28.853+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:30.814+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:30.945+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:31.122+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:31.155+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:31.607+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:31.608+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:31.681+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:31.739+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:32.443+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:32.454+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:31.825062+00:00, run_end_date=2023-09-11 06:47:32.052237+00:00, run_duration=0.227175, state=success, executor_state=success, try_number=1, max_tries=0, job_id=104, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:28.846778+00:00, queued_by_job_id=2, pid=41687[0m
[[34m2023-09-11T06:47:32.728+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-12T00:00:00+00:00, run_after=2023-04-13T00:00:00+00:00[0m
[[34m2023-09-11T06:47:32.763+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-10 00:00:00+00:00: scheduled__2023-04-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:28.797709+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:32.764+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-10 00:00:00+00:00, run_id=scheduled__2023-04-10T00:00:00+00:00, run_start_date=2023-09-11 06:47:28.814521+00:00, run_end_date=2023-09-11 06:47:32.764071+00:00, run_duration=3.94955, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-10 00:00:00+00:00, data_interval_end=2023-04-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:32.767+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-11T00:00:00+00:00, run_after=2023-04-12T00:00:00+00:00[0m
[[34m2023-09-11T06:47:32.782+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:32.783+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:32.783+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:32.785+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:32.786+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:32.786+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:32.789+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:34.631+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:34.760+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:34.938+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:34.970+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:35.486+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:35.486+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:35.557+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:35.615+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:36.456+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:36.466+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:35.702321+00:00, run_end_date=2023-09-11 06:47:35.968966+00:00, run_duration=0.266645, state=success, executor_state=success, try_number=1, max_tries=0, job_id=105, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:32.784033+00:00, queued_by_job_id=2, pid=41696[0m
[[34m2023-09-11T06:47:36.716+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-12T00:00:00+00:00, run_after=2023-04-13T00:00:00+00:00[0m
[[34m2023-09-11T06:47:36.738+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-11 00:00:00+00:00: scheduled__2023-04-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:32.724186+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:36.739+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-11 00:00:00+00:00, run_id=scheduled__2023-04-11T00:00:00+00:00, run_start_date=2023-09-11 06:47:32.741461+00:00, run_end_date=2023-09-11 06:47:36.738968+00:00, run_duration=3.997507, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-11 00:00:00+00:00, data_interval_end=2023-04-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:36.742+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-12T00:00:00+00:00, run_after=2023-04-13T00:00:00+00:00[0m
[[34m2023-09-11T06:47:37.742+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-13T00:00:00+00:00, run_after=2023-04-14T00:00:00+00:00[0m
[[34m2023-09-11T06:47:37.785+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:37.785+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:37.785+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:37.787+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:37.788+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:37.788+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:37.791+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:39.887+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:40.041+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:40.368+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:40.410+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:40.955+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:40.957+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:41.074+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:41.156+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:41.940+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:41.952+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:41.251643+00:00, run_end_date=2023-09-11 06:47:41.529158+00:00, run_duration=0.277515, state=success, executor_state=success, try_number=1, max_tries=0, job_id=106, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:37.786419+00:00, queued_by_job_id=2, pid=41770[0m
[[34m2023-09-11T06:47:42.215+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-14T00:00:00+00:00, run_after=2023-04-15T00:00:00+00:00[0m
[[34m2023-09-11T06:47:42.251+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-12 00:00:00+00:00: scheduled__2023-04-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:37.737450+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:42.252+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-12 00:00:00+00:00, run_id=scheduled__2023-04-12T00:00:00+00:00, run_start_date=2023-09-11 06:47:37.754323+00:00, run_end_date=2023-09-11 06:47:42.252251+00:00, run_duration=4.497928, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-12 00:00:00+00:00, data_interval_end=2023-04-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:42.255+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-13T00:00:00+00:00, run_after=2023-04-14T00:00:00+00:00[0m
[[34m2023-09-11T06:47:42.270+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:42.271+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:42.271+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:42.273+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:42.274+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:42.274+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:42.276+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:44.319+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:44.453+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:44.633+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:44.666+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:45.153+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:45.154+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:45.227+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:45.289+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:46.044+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:46.055+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:45.376564+00:00, run_end_date=2023-09-11 06:47:45.611752+00:00, run_duration=0.235188, state=success, executor_state=success, try_number=1, max_tries=0, job_id=107, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:42.272017+00:00, queued_by_job_id=2, pid=41806[0m
[[34m2023-09-11T06:47:46.313+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-14T00:00:00+00:00, run_after=2023-04-15T00:00:00+00:00[0m
[[34m2023-09-11T06:47:46.336+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-13 00:00:00+00:00: scheduled__2023-04-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:42.210807+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:46.337+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-13 00:00:00+00:00, run_id=scheduled__2023-04-13T00:00:00+00:00, run_start_date=2023-09-11 06:47:42.228537+00:00, run_end_date=2023-09-11 06:47:46.337094+00:00, run_duration=4.108557, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-13 00:00:00+00:00, data_interval_end=2023-04-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:46.340+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-14T00:00:00+00:00, run_after=2023-04-15T00:00:00+00:00[0m
[[34m2023-09-11T06:47:46.762+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-15T00:00:00+00:00, run_after=2023-04-16T00:00:00+00:00[0m
[[34m2023-09-11T06:47:46.805+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:46.805+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:46.806+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:46.808+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:46.808+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:46.809+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:46.811+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:48.724+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:48.868+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:49.038+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:49.069+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:49.547+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:49.548+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:49.622+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:49.681+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:50.396+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:50.407+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:49.768485+00:00, run_end_date=2023-09-11 06:47:49.993459+00:00, run_duration=0.224974, state=success, executor_state=success, try_number=1, max_tries=0, job_id=108, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:46.806914+00:00, queued_by_job_id=2, pid=41816[0m
[[34m2023-09-11T06:47:50.674+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-16T00:00:00+00:00, run_after=2023-04-17T00:00:00+00:00[0m
[[34m2023-09-11T06:47:50.711+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-14 00:00:00+00:00: scheduled__2023-04-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:46.757740+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:50.712+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-14 00:00:00+00:00, run_id=scheduled__2023-04-14T00:00:00+00:00, run_start_date=2023-09-11 06:47:46.774458+00:00, run_end_date=2023-09-11 06:47:50.712433+00:00, run_duration=3.937975, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-14 00:00:00+00:00, data_interval_end=2023-04-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:50.716+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-15T00:00:00+00:00, run_after=2023-04-16T00:00:00+00:00[0m
[[34m2023-09-11T06:47:50.731+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:50.731+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:50.732+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:50.734+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:50.734+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:50.735+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:50.737+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:52.601+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:52.730+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:52.908+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:52.940+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:53.408+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:53.409+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:53.481+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:53.539+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:54.353+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:54.363+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:53.622716+00:00, run_end_date=2023-09-11 06:47:53.853126+00:00, run_duration=0.23041, state=success, executor_state=success, try_number=1, max_tries=0, job_id=109, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:50.732940+00:00, queued_by_job_id=2, pid=41826[0m
[[34m2023-09-11T06:47:54.621+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-16T00:00:00+00:00, run_after=2023-04-17T00:00:00+00:00[0m
[[34m2023-09-11T06:47:54.643+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-15 00:00:00+00:00: scheduled__2023-04-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:50.669448+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:54.643+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-15 00:00:00+00:00, run_id=scheduled__2023-04-15T00:00:00+00:00, run_start_date=2023-09-11 06:47:50.688733+00:00, run_end_date=2023-09-11 06:47:54.643625+00:00, run_duration=3.954892, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-15 00:00:00+00:00, data_interval_end=2023-04-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:54.647+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-16T00:00:00+00:00, run_after=2023-04-17T00:00:00+00:00[0m
[[34m2023-09-11T06:47:55.772+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-17T00:00:00+00:00, run_after=2023-04-18T00:00:00+00:00[0m
[[34m2023-09-11T06:47:55.817+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:55.817+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:55.817+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:55.819+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:55.820+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:55.820+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:55.823+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:57.767+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:57.893+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:58.061+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:58.094+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:58.587+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:58.588+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:58.672+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:58.733+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:59.453+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:59.464+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:58.824276+00:00, run_end_date=2023-09-11 06:47:59.047796+00:00, run_duration=0.22352, state=success, executor_state=success, try_number=1, max_tries=0, job_id=110, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:55.818315+00:00, queued_by_job_id=2, pid=41836[0m
[[34m2023-09-11T06:47:59.720+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-18T00:00:00+00:00, run_after=2023-04-19T00:00:00+00:00[0m
[[34m2023-09-11T06:47:59.755+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-16 00:00:00+00:00: scheduled__2023-04-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:55.768004+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:59.755+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-16 00:00:00+00:00, run_id=scheduled__2023-04-16T00:00:00+00:00, run_start_date=2023-09-11 06:47:55.785095+00:00, run_end_date=2023-09-11 06:47:59.755765+00:00, run_duration=3.97067, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-16 00:00:00+00:00, data_interval_end=2023-04-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:59.759+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-17T00:00:00+00:00, run_after=2023-04-18T00:00:00+00:00[0m
[[34m2023-09-11T06:47:59.800+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:59.801+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:59.801+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:59.803+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:59.803+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:59.804+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:59.819+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:01.768+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:01.914+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:02.086+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:02.119+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:02.599+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:02.600+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:02.670+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:02.741+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:03.483+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:03.494+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:02.828066+00:00, run_end_date=2023-09-11 06:48:03.052842+00:00, run_duration=0.224776, state=success, executor_state=success, try_number=1, max_tries=0, job_id=111, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:59.801869+00:00, queued_by_job_id=2, pid=41843[0m
[[34m2023-09-11T06:48:03.753+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-18T00:00:00+00:00, run_after=2023-04-19T00:00:00+00:00[0m
[[34m2023-09-11T06:48:03.777+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-17 00:00:00+00:00: scheduled__2023-04-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:59.715377+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:03.778+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-17 00:00:00+00:00, run_id=scheduled__2023-04-17T00:00:00+00:00, run_start_date=2023-09-11 06:47:59.733467+00:00, run_end_date=2023-09-11 06:48:03.777884+00:00, run_duration=4.044417, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-17 00:00:00+00:00, data_interval_end=2023-04-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:03.781+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-18T00:00:00+00:00, run_after=2023-04-19T00:00:00+00:00[0m
[[34m2023-09-11T06:48:04.823+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-19T00:00:00+00:00, run_after=2023-04-20T00:00:00+00:00[0m
[[34m2023-09-11T06:48:04.866+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:04.867+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:04.867+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:04.869+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:04.870+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:04.870+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:04.873+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:06.798+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:06.951+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:07.136+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:07.172+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:07.655+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:07.656+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:07.727+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:07.791+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:08.549+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:08.559+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:07.876990+00:00, run_end_date=2023-09-11 06:48:08.141079+00:00, run_duration=0.264089, state=success, executor_state=success, try_number=1, max_tries=0, job_id=112, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:04.867908+00:00, queued_by_job_id=2, pid=41853[0m
[[34m2023-09-11T06:48:08.824+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-20T00:00:00+00:00, run_after=2023-04-21T00:00:00+00:00[0m
[[34m2023-09-11T06:48:08.865+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-18 00:00:00+00:00: scheduled__2023-04-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:04.818320+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:08.865+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-18 00:00:00+00:00, run_id=scheduled__2023-04-18T00:00:00+00:00, run_start_date=2023-09-11 06:48:04.835138+00:00, run_end_date=2023-09-11 06:48:08.865602+00:00, run_duration=4.030464, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-18 00:00:00+00:00, data_interval_end=2023-04-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:08.869+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-19T00:00:00+00:00, run_after=2023-04-20T00:00:00+00:00[0m
[[34m2023-09-11T06:48:08.885+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:08.885+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:08.885+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:08.888+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:08.896+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:08.896+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:08.899+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:10.825+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:10.950+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:11.127+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:11.161+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:11.614+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:11.615+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:11.685+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:11.750+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:12.468+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:12.484+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:11.838211+00:00, run_end_date=2023-09-11 06:48:12.063945+00:00, run_duration=0.225734, state=success, executor_state=success, try_number=1, max_tries=0, job_id=113, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:08.886565+00:00, queued_by_job_id=2, pid=41860[0m
[[34m2023-09-11T06:48:12.628+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-20T00:00:00+00:00, run_after=2023-04-21T00:00:00+00:00[0m
[[34m2023-09-11T06:48:12.650+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-19 00:00:00+00:00: scheduled__2023-04-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:08.819089+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:12.650+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-19 00:00:00+00:00, run_id=scheduled__2023-04-19T00:00:00+00:00, run_start_date=2023-09-11 06:48:08.838532+00:00, run_end_date=2023-09-11 06:48:12.650639+00:00, run_duration=3.812107, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-19 00:00:00+00:00, data_interval_end=2023-04-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:12.653+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-20T00:00:00+00:00, run_after=2023-04-21T00:00:00+00:00[0m
[[34m2023-09-11T06:48:13.785+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-21T00:00:00+00:00, run_after=2023-04-22T00:00:00+00:00[0m
[[34m2023-09-11T06:48:13.837+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:13.838+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:13.838+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:13.840+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:13.841+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:13.842+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:13.844+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:15.850+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:15.980+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:16.180+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:16.225+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:16.708+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:16.708+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:16.778+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:16.850+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:17.649+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:17.660+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:16.944587+00:00, run_end_date=2023-09-11 06:48:17.169610+00:00, run_duration=0.225023, state=success, executor_state=success, try_number=1, max_tries=0, job_id=114, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:13.839308+00:00, queued_by_job_id=2, pid=41870[0m
[[34m2023-09-11T06:48:17.945+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-22T00:00:00+00:00, run_after=2023-04-23T00:00:00+00:00[0m
[[34m2023-09-11T06:48:17.984+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-20 00:00:00+00:00: scheduled__2023-04-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:13.778159+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:17.984+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-20 00:00:00+00:00, run_id=scheduled__2023-04-20T00:00:00+00:00, run_start_date=2023-09-11 06:48:13.799848+00:00, run_end_date=2023-09-11 06:48:17.984680+00:00, run_duration=4.184832, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-20 00:00:00+00:00, data_interval_end=2023-04-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:17.988+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-21T00:00:00+00:00, run_after=2023-04-22T00:00:00+00:00[0m
[[34m2023-09-11T06:48:18.005+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:18.005+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:18.006+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:18.008+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:18.008+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:18.009+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:18.011+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:20.089+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:20.254+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:20.454+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:20.494+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:21.058+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:21.058+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:21.129+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:21.187+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:21.903+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:21.914+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:21.271393+00:00, run_end_date=2023-09-11 06:48:21.495099+00:00, run_duration=0.223706, state=success, executor_state=success, try_number=1, max_tries=0, job_id=115, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:18.006728+00:00, queued_by_job_id=2, pid=41879[0m
[[34m2023-09-11T06:48:22.160+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-22T00:00:00+00:00, run_after=2023-04-23T00:00:00+00:00[0m
[[34m2023-09-11T06:48:22.196+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-21 00:00:00+00:00: scheduled__2023-04-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:17.940277+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:22.197+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-21 00:00:00+00:00, run_id=scheduled__2023-04-21T00:00:00+00:00, run_start_date=2023-09-11 06:48:17.959819+00:00, run_end_date=2023-09-11 06:48:22.197039+00:00, run_duration=4.23722, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-21 00:00:00+00:00, data_interval_end=2023-04-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:22.200+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-22T00:00:00+00:00, run_after=2023-04-23T00:00:00+00:00[0m
[[34m2023-09-11T06:48:22.958+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-23T00:00:00+00:00, run_after=2023-04-24T00:00:00+00:00[0m
[[34m2023-09-11T06:48:23.006+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:23.006+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:23.007+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:23.009+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:23.010+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:23.010+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:23.014+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:24.985+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:25.117+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:25.290+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:25.322+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:25.789+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:25.790+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:25.860+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:25.923+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:26.626+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:26.637+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:26.007667+00:00, run_end_date=2023-09-11 06:48:26.230050+00:00, run_duration=0.222383, state=success, executor_state=success, try_number=1, max_tries=0, job_id=116, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:23.007861+00:00, queued_by_job_id=2, pid=41889[0m
[[34m2023-09-11T06:48:26.904+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-24T00:00:00+00:00, run_after=2023-04-25T00:00:00+00:00[0m
[[34m2023-09-11T06:48:26.940+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-22 00:00:00+00:00: scheduled__2023-04-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:22.953765+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:26.941+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-22 00:00:00+00:00, run_id=scheduled__2023-04-22T00:00:00+00:00, run_start_date=2023-09-11 06:48:22.970873+00:00, run_end_date=2023-09-11 06:48:26.941361+00:00, run_duration=3.970488, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-22 00:00:00+00:00, data_interval_end=2023-04-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:26.944+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-23T00:00:00+00:00, run_after=2023-04-24T00:00:00+00:00[0m
[[34m2023-09-11T06:48:26.960+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:26.960+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:26.961+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:26.963+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:26.963+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:26.964+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:26.966+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:28.822+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:28.955+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:29.125+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:29.158+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:29.634+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:29.635+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:29.708+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:29.767+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:30.510+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:30.521+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:29.850728+00:00, run_end_date=2023-09-11 06:48:30.077431+00:00, run_duration=0.226703, state=success, executor_state=success, try_number=1, max_tries=0, job_id=117, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:26.961810+00:00, queued_by_job_id=2, pid=41898[0m
[[34m2023-09-11T06:48:30.767+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-24T00:00:00+00:00, run_after=2023-04-25T00:00:00+00:00[0m
[[34m2023-09-11T06:48:30.790+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-23 00:00:00+00:00: scheduled__2023-04-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:26.899461+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:30.790+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-23 00:00:00+00:00, run_id=scheduled__2023-04-23T00:00:00+00:00, run_start_date=2023-09-11 06:48:26.917969+00:00, run_end_date=2023-09-11 06:48:30.790840+00:00, run_duration=3.872871, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-23 00:00:00+00:00, data_interval_end=2023-04-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:30.794+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-24T00:00:00+00:00, run_after=2023-04-25T00:00:00+00:00[0m
[[34m2023-09-11T06:48:31.902+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-25T00:00:00+00:00, run_after=2023-04-26T00:00:00+00:00[0m
[[34m2023-09-11T06:48:31.945+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:31.945+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:31.945+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:31.948+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:31.948+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:31.949+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:31.951+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:33.790+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:33.916+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:34.109+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:34.142+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:34.620+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:34.620+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:34.691+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:34.747+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:35.465+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:35.475+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:34.835777+00:00, run_end_date=2023-09-11 06:48:35.065233+00:00, run_duration=0.229456, state=success, executor_state=success, try_number=1, max_tries=0, job_id=118, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:31.946582+00:00, queued_by_job_id=2, pid=41908[0m
[[34m2023-09-11T06:48:35.742+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-26T00:00:00+00:00, run_after=2023-04-27T00:00:00+00:00[0m
[[34m2023-09-11T06:48:35.778+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-24 00:00:00+00:00: scheduled__2023-04-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:31.898015+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:35.778+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-24 00:00:00+00:00, run_id=scheduled__2023-04-24T00:00:00+00:00, run_start_date=2023-09-11 06:48:31.914937+00:00, run_end_date=2023-09-11 06:48:35.778698+00:00, run_duration=3.863761, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-24 00:00:00+00:00, data_interval_end=2023-04-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:35.782+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-25T00:00:00+00:00, run_after=2023-04-26T00:00:00+00:00[0m
[[34m2023-09-11T06:48:35.797+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:35.797+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:35.798+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:35.800+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:35.801+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:35.801+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:35.804+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:37.658+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:37.793+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:37.975+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:38.009+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:38.484+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:38.485+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:38.557+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:38.615+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:39.330+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:39.340+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:38.700203+00:00, run_end_date=2023-09-11 06:48:38.947325+00:00, run_duration=0.247122, state=success, executor_state=success, try_number=1, max_tries=0, job_id=119, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:35.798948+00:00, queued_by_job_id=2, pid=41917[0m
[[34m2023-09-11T06:48:39.598+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-27T00:00:00+00:00, run_after=2023-04-28T00:00:00+00:00[0m
[[34m2023-09-11T06:48:39.634+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-25 00:00:00+00:00: scheduled__2023-04-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:35.737150+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:39.635+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-25 00:00:00+00:00, run_id=scheduled__2023-04-25T00:00:00+00:00, run_start_date=2023-09-11 06:48:35.755007+00:00, run_end_date=2023-09-11 06:48:39.635075+00:00, run_duration=3.880068, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-25 00:00:00+00:00, data_interval_end=2023-04-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:39.638+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-26T00:00:00+00:00, run_after=2023-04-27T00:00:00+00:00[0m
[[34m2023-09-11T06:48:39.652+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:39.653+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:39.653+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:39.655+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:39.656+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:39.656+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:39.659+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:41.502+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:41.632+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:41.806+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:41.838+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:42.317+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:42.318+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:42.387+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:42.452+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:43.153+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:43.164+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:42.535768+00:00, run_end_date=2023-09-11 06:48:42.756913+00:00, run_duration=0.221145, state=success, executor_state=success, try_number=1, max_tries=0, job_id=120, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:39.654146+00:00, queued_by_job_id=2, pid=41925[0m
[[34m2023-09-11T06:48:43.428+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-27T00:00:00+00:00, run_after=2023-04-28T00:00:00+00:00[0m
[[34m2023-09-11T06:48:43.450+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-26 00:00:00+00:00: scheduled__2023-04-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:39.593152+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:43.451+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-26 00:00:00+00:00, run_id=scheduled__2023-04-26T00:00:00+00:00, run_start_date=2023-09-11 06:48:39.612716+00:00, run_end_date=2023-09-11 06:48:43.451171+00:00, run_duration=3.838455, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-26 00:00:00+00:00, data_interval_end=2023-04-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:43.463+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-27T00:00:00+00:00, run_after=2023-04-28T00:00:00+00:00[0m
[[34m2023-09-11T06:48:44.735+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-28T00:00:00+00:00, run_after=2023-04-29T00:00:00+00:00[0m
[[34m2023-09-11T06:48:44.792+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:44.792+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:44.792+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:44.795+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:44.795+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:44.795+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:44.798+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:46.625+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:46.755+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:46.934+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:46.967+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:47.468+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:47.469+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:47.552+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:47.619+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:48.361+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:48.372+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:47.706966+00:00, run_end_date=2023-09-11 06:48:47.967308+00:00, run_duration=0.260342, state=success, executor_state=success, try_number=1, max_tries=0, job_id=121, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:44.793714+00:00, queued_by_job_id=2, pid=41935[0m
[[34m2023-09-11T06:48:48.685+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-29T00:00:00+00:00, run_after=2023-04-30T00:00:00+00:00[0m
[[34m2023-09-11T06:48:48.723+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-27 00:00:00+00:00: scheduled__2023-04-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:44.729524+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:48.723+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-27 00:00:00+00:00, run_id=scheduled__2023-04-27T00:00:00+00:00, run_start_date=2023-09-11 06:48:44.748010+00:00, run_end_date=2023-09-11 06:48:48.723381+00:00, run_duration=3.975371, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-27 00:00:00+00:00, data_interval_end=2023-04-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:48.727+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-28T00:00:00+00:00, run_after=2023-04-29T00:00:00+00:00[0m
[[34m2023-09-11T06:48:48.742+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:48.743+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:48.743+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:48.745+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:48.746+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:48.746+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:48.749+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:50.633+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:50.772+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:50.958+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:50.991+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:51.466+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:51.467+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:51.536+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:51.600+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:52.339+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:52.351+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:51.685337+00:00, run_end_date=2023-09-11 06:48:51.920050+00:00, run_duration=0.234713, state=success, executor_state=success, try_number=1, max_tries=0, job_id=122, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:48.744108+00:00, queued_by_job_id=2, pid=41942[0m
[[34m2023-09-11T06:48:52.511+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-29T00:00:00+00:00, run_after=2023-04-30T00:00:00+00:00[0m
[[34m2023-09-11T06:48:52.533+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-28 00:00:00+00:00: scheduled__2023-04-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:48.680011+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:52.534+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-28 00:00:00+00:00, run_id=scheduled__2023-04-28T00:00:00+00:00, run_start_date=2023-09-11 06:48:48.698791+00:00, run_end_date=2023-09-11 06:48:52.534144+00:00, run_duration=3.835353, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-28 00:00:00+00:00, data_interval_end=2023-04-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:52.537+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-29T00:00:00+00:00, run_after=2023-04-30T00:00:00+00:00[0m
[[34m2023-09-11T06:48:53.440+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-30T00:00:00+00:00, run_after=2023-05-01T00:00:00+00:00[0m
[[34m2023-09-11T06:48:53.490+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:53.490+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:53.491+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:53.493+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:53.494+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:53.494+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:53.496+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:55.420+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:55.597+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:55.769+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:55.802+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:56.278+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:56.279+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:56.351+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:56.410+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:57.253+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:57.264+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:56.501019+00:00, run_end_date=2023-09-11 06:48:56.730100+00:00, run_duration=0.229081, state=success, executor_state=success, try_number=1, max_tries=0, job_id=123, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:53.491749+00:00, queued_by_job_id=2, pid=41952[0m
[[34m2023-09-11T06:48:57.455+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-01T00:00:00+00:00, run_after=2023-05-02T00:00:00+00:00[0m
[[34m2023-09-11T06:48:57.491+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-29 00:00:00+00:00: scheduled__2023-04-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:53.435579+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:57.492+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-29 00:00:00+00:00, run_id=scheduled__2023-04-29T00:00:00+00:00, run_start_date=2023-09-11 06:48:53.452714+00:00, run_end_date=2023-09-11 06:48:57.492193+00:00, run_duration=4.039479, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-29 00:00:00+00:00, data_interval_end=2023-04-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:57.496+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-30T00:00:00+00:00, run_after=2023-05-01T00:00:00+00:00[0m
[[34m2023-09-11T06:48:57.511+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:57.512+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:57.512+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:57.514+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:57.515+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:57.515+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:57.518+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:59.353+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:59.486+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:59.681+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:59.714+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:00.184+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:00.185+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:00.261+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:00.318+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:01.027+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:01.038+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:00.403296+00:00, run_end_date=2023-09-11 06:49:00.633946+00:00, run_duration=0.23065, state=success, executor_state=success, try_number=1, max_tries=0, job_id=124, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:57.513246+00:00, queued_by_job_id=2, pid=41961[0m
[[34m2023-09-11T06:49:01.181+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-01T00:00:00+00:00, run_after=2023-05-02T00:00:00+00:00[0m
[[34m2023-09-11T06:49:01.203+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-30 00:00:00+00:00: scheduled__2023-04-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:57.450152+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:01.204+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-30 00:00:00+00:00, run_id=scheduled__2023-04-30T00:00:00+00:00, run_start_date=2023-09-11 06:48:57.468381+00:00, run_end_date=2023-09-11 06:49:01.203946+00:00, run_duration=3.735565, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-30 00:00:00+00:00, data_interval_end=2023-05-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:01.207+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-01T00:00:00+00:00, run_after=2023-05-02T00:00:00+00:00[0m
[[34m2023-09-11T06:49:02.701+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-02T00:00:00+00:00, run_after=2023-05-03T00:00:00+00:00[0m
[[34m2023-09-11T06:49:02.751+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:02.751+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:02.752+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:02.754+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:02.754+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:02.755+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:02.757+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:04.798+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:04.949+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:05.125+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:05.158+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:05.623+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:05.623+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:05.693+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:05.753+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:06.455+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:06.465+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:05.839629+00:00, run_end_date=2023-09-11 06:49:06.067037+00:00, run_duration=0.227408, state=success, executor_state=success, try_number=1, max_tries=0, job_id=125, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:02.752904+00:00, queued_by_job_id=2, pid=41971[0m
[[34m2023-09-11T06:49:06.631+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-03T00:00:00+00:00, run_after=2023-05-04T00:00:00+00:00[0m
[[34m2023-09-11T06:49:06.666+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-01 00:00:00+00:00: scheduled__2023-05-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:02.695390+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:06.666+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-01 00:00:00+00:00, run_id=scheduled__2023-05-01T00:00:00+00:00, run_start_date=2023-09-11 06:49:02.718034+00:00, run_end_date=2023-09-11 06:49:06.666656+00:00, run_duration=3.948622, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-01 00:00:00+00:00, data_interval_end=2023-05-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:06.670+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-02T00:00:00+00:00, run_after=2023-05-03T00:00:00+00:00[0m
[[34m2023-09-11T06:49:06.685+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:06.685+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:06.685+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:06.687+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:06.688+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:06.688+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:06.691+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:08.523+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:08.652+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:08.820+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:08.853+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:09.331+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:09.332+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:09.404+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:09.463+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:10.185+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:10.196+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:09.554222+00:00, run_end_date=2023-09-11 06:49:09.788598+00:00, run_duration=0.234376, state=success, executor_state=success, try_number=1, max_tries=0, job_id=126, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:06.686434+00:00, queued_by_job_id=2, pid=41980[0m
[[34m2023-09-11T06:49:10.447+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-03T00:00:00+00:00, run_after=2023-05-04T00:00:00+00:00[0m
[[34m2023-09-11T06:49:10.469+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-02 00:00:00+00:00: scheduled__2023-05-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:06.626026+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:10.470+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-02 00:00:00+00:00, run_id=scheduled__2023-05-02T00:00:00+00:00, run_start_date=2023-09-11 06:49:06.643503+00:00, run_end_date=2023-09-11 06:49:10.470258+00:00, run_duration=3.826755, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-02 00:00:00+00:00, data_interval_end=2023-05-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:10.473+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-03T00:00:00+00:00, run_after=2023-05-04T00:00:00+00:00[0m
[[34m2023-09-11T06:49:11.737+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-04T00:00:00+00:00, run_after=2023-05-05T00:00:00+00:00[0m
[[34m2023-09-11T06:49:11.782+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:11.782+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:11.783+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:11.785+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:11.786+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:11.786+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:11.789+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:13.642+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:13.774+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:13.953+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:13.990+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:14.447+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:14.447+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:14.516+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:14.571+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:15.276+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:15.287+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:14.656968+00:00, run_end_date=2023-09-11 06:49:14.876580+00:00, run_duration=0.219612, state=success, executor_state=success, try_number=1, max_tries=0, job_id=127, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:11.783876+00:00, queued_by_job_id=2, pid=41990[0m
[[34m2023-09-11T06:49:15.548+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-05T00:00:00+00:00, run_after=2023-05-06T00:00:00+00:00[0m
[[34m2023-09-11T06:49:15.592+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-03 00:00:00+00:00: scheduled__2023-05-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:11.732523+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:15.593+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-03 00:00:00+00:00, run_id=scheduled__2023-05-03T00:00:00+00:00, run_start_date=2023-09-11 06:49:11.750544+00:00, run_end_date=2023-09-11 06:49:15.592951+00:00, run_duration=3.842407, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-03 00:00:00+00:00, data_interval_end=2023-05-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:15.596+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-04T00:00:00+00:00, run_after=2023-05-05T00:00:00+00:00[0m
[[34m2023-09-11T06:49:15.613+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:15.613+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:15.613+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:15.615+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:15.616+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:15.616+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:15.619+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:17.442+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:17.609+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:17.800+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:17.832+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:18.316+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:18.317+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:18.385+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:18.448+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:19.163+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:19.173+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:18.536073+00:00, run_end_date=2023-09-11 06:49:18.752322+00:00, run_duration=0.216249, state=success, executor_state=success, try_number=1, max_tries=0, job_id=128, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:15.614500+00:00, queued_by_job_id=2, pid=41999[0m
[[34m2023-09-11T06:49:19.459+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-05T00:00:00+00:00, run_after=2023-05-06T00:00:00+00:00[0m
[[34m2023-09-11T06:49:19.483+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-04 00:00:00+00:00: scheduled__2023-05-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:15.542899+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:19.483+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-04 00:00:00+00:00, run_id=scheduled__2023-05-04T00:00:00+00:00, run_start_date=2023-09-11 06:49:15.569526+00:00, run_end_date=2023-09-11 06:49:19.483368+00:00, run_duration=3.913842, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-04 00:00:00+00:00, data_interval_end=2023-05-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:19.486+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-05T00:00:00+00:00, run_after=2023-05-06T00:00:00+00:00[0m
[[34m2023-09-11T06:49:20.459+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-06T00:00:00+00:00, run_after=2023-05-07T00:00:00+00:00[0m
[[34m2023-09-11T06:49:20.502+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:20.502+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:20.503+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:20.505+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:20.505+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:20.505+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:20.508+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:22.313+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:22.439+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:22.613+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:22.646+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:23.116+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:23.117+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:23.184+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:23.243+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:24.003+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:24.013+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:23.331490+00:00, run_end_date=2023-09-11 06:49:23.553595+00:00, run_duration=0.222105, state=success, executor_state=success, try_number=1, max_tries=0, job_id=129, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:20.503801+00:00, queued_by_job_id=2, pid=42007[0m
[[34m2023-09-11T06:49:24.331+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-07T00:00:00+00:00, run_after=2023-05-08T00:00:00+00:00[0m
[[34m2023-09-11T06:49:24.367+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-05 00:00:00+00:00: scheduled__2023-05-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:20.454440+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:24.367+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-05 00:00:00+00:00, run_id=scheduled__2023-05-05T00:00:00+00:00, run_start_date=2023-09-11 06:49:20.472450+00:00, run_end_date=2023-09-11 06:49:24.367362+00:00, run_duration=3.894912, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-05 00:00:00+00:00, data_interval_end=2023-05-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:24.371+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-06T00:00:00+00:00, run_after=2023-05-07T00:00:00+00:00[0m
[[34m2023-09-11T06:49:24.386+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:24.386+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:24.387+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:24.394+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:24.395+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:24.395+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:24.398+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:26.222+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:26.354+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:26.527+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:26.558+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:27.032+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:27.032+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:27.101+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:27.165+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:27.892+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:27.903+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:27.251609+00:00, run_end_date=2023-09-11 06:49:27.477876+00:00, run_duration=0.226267, state=success, executor_state=success, try_number=1, max_tries=0, job_id=130, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:24.387701+00:00, queued_by_job_id=2, pid=42014[0m
[[34m2023-09-11T06:49:28.044+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-07T00:00:00+00:00, run_after=2023-05-08T00:00:00+00:00[0m
[[34m2023-09-11T06:49:28.067+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-06 00:00:00+00:00: scheduled__2023-05-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:24.325735+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:28.067+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-06 00:00:00+00:00, run_id=scheduled__2023-05-06T00:00:00+00:00, run_start_date=2023-09-11 06:49:24.344001+00:00, run_end_date=2023-09-11 06:49:28.067697+00:00, run_duration=3.723696, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-06 00:00:00+00:00, data_interval_end=2023-05-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:28.071+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-07T00:00:00+00:00, run_after=2023-05-08T00:00:00+00:00[0m
[[34m2023-09-11T06:49:29.312+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-08T00:00:00+00:00, run_after=2023-05-09T00:00:00+00:00[0m
[[34m2023-09-11T06:49:29.357+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:29.357+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:29.357+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:29.359+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:29.360+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:29.360+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:29.363+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:31.190+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:31.321+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:31.502+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:31.535+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:31.994+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:31.995+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:32.064+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:32.124+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:32.821+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:32.832+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:32.209655+00:00, run_end_date=2023-09-11 06:49:32.435671+00:00, run_duration=0.226016, state=success, executor_state=success, try_number=1, max_tries=0, job_id=131, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:29.358437+00:00, queued_by_job_id=2, pid=42024[0m
[[34m2023-09-11T06:49:33.086+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-09T00:00:00+00:00, run_after=2023-05-10T00:00:00+00:00[0m
[[34m2023-09-11T06:49:33.123+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-07 00:00:00+00:00: scheduled__2023-05-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:29.308002+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:33.123+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-07 00:00:00+00:00, run_id=scheduled__2023-05-07T00:00:00+00:00, run_start_date=2023-09-11 06:49:29.325733+00:00, run_end_date=2023-09-11 06:49:33.123671+00:00, run_duration=3.797938, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-07 00:00:00+00:00, data_interval_end=2023-05-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:33.127+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-08T00:00:00+00:00, run_after=2023-05-09T00:00:00+00:00[0m
[[34m2023-09-11T06:49:33.142+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:33.142+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:33.142+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:33.145+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:33.145+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:33.145+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:33.148+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:34.985+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:35.110+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:35.286+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:35.322+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:35.794+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:35.795+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:35.865+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:35.934+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:36.631+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:36.642+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:36.019951+00:00, run_end_date=2023-09-11 06:49:36.240417+00:00, run_duration=0.220466, state=success, executor_state=success, try_number=1, max_tries=0, job_id=132, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:33.143529+00:00, queued_by_job_id=2, pid=42033[0m
[[34m2023-09-11T06:49:36.892+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-09T00:00:00+00:00, run_after=2023-05-10T00:00:00+00:00[0m
[[34m2023-09-11T06:49:36.915+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-08 00:00:00+00:00: scheduled__2023-05-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:33.082240+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:36.915+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-08 00:00:00+00:00, run_id=scheduled__2023-05-08T00:00:00+00:00, run_start_date=2023-09-11 06:49:33.099053+00:00, run_end_date=2023-09-11 06:49:36.915501+00:00, run_duration=3.816448, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-08 00:00:00+00:00, data_interval_end=2023-05-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:36.919+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-09T00:00:00+00:00, run_after=2023-05-10T00:00:00+00:00[0m
[[34m2023-09-11T06:49:38.267+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-10T00:00:00+00:00, run_after=2023-05-11T00:00:00+00:00[0m
[[34m2023-09-11T06:49:38.311+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:38.312+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:38.312+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:38.314+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:38.314+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:38.315+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:38.317+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:40.146+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:40.276+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:40.447+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:40.480+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:40.947+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:40.947+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:41.017+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:41.075+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:41.772+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:41.783+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:41.160381+00:00, run_end_date=2023-09-11 06:49:41.384497+00:00, run_duration=0.224116, state=success, executor_state=success, try_number=1, max_tries=0, job_id=133, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:38.312985+00:00, queued_by_job_id=2, pid=42043[0m
[[34m2023-09-11T06:49:42.497+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-11T00:00:00+00:00, run_after=2023-05-12T00:00:00+00:00[0m
[[34m2023-09-11T06:49:42.531+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-09 00:00:00+00:00: scheduled__2023-05-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:38.261638+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:42.531+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-09 00:00:00+00:00, run_id=scheduled__2023-05-09T00:00:00+00:00, run_start_date=2023-09-11 06:49:38.281082+00:00, run_end_date=2023-09-11 06:49:42.531686+00:00, run_duration=4.250604, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-09 00:00:00+00:00, data_interval_end=2023-05-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:42.535+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-10T00:00:00+00:00, run_after=2023-05-11T00:00:00+00:00[0m
[[34m2023-09-11T06:49:42.554+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:42.555+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:42.555+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:42.557+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:42.558+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:42.558+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:42.561+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:44.371+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:44.504+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:44.674+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:44.705+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:45.175+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:45.176+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:45.247+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:45.306+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:46.091+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:46.102+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:45.392761+00:00, run_end_date=2023-09-11 06:49:45.612979+00:00, run_duration=0.220218, state=success, executor_state=success, try_number=1, max_tries=0, job_id=134, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:42.556205+00:00, queued_by_job_id=2, pid=42052[0m
[[34m2023-09-11T06:49:46.370+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-11T00:00:00+00:00, run_after=2023-05-12T00:00:00+00:00[0m
[[34m2023-09-11T06:49:46.397+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-10 00:00:00+00:00: scheduled__2023-05-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:42.491998+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:46.398+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-10 00:00:00+00:00, run_id=scheduled__2023-05-10T00:00:00+00:00, run_start_date=2023-09-11 06:49:42.509583+00:00, run_end_date=2023-09-11 06:49:46.397941+00:00, run_duration=3.888358, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-10 00:00:00+00:00, data_interval_end=2023-05-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:46.401+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-11T00:00:00+00:00, run_after=2023-05-12T00:00:00+00:00[0m
[[34m2023-09-11T06:49:47.065+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-12T00:00:00+00:00, run_after=2023-05-13T00:00:00+00:00[0m
[[34m2023-09-11T06:49:47.108+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:47.109+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:47.109+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:47.111+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:47.112+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:47.112+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:47.115+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:48.967+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:49.091+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:49.262+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:49.296+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:49.760+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:49.761+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:49.830+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:49.885+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:50.585+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:50.595+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:49.968152+00:00, run_end_date=2023-09-11 06:49:50.181723+00:00, run_duration=0.213571, state=success, executor_state=success, try_number=1, max_tries=0, job_id=135, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:47.110019+00:00, queued_by_job_id=2, pid=42062[0m
[[34m2023-09-11T06:49:51.693+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-13T00:00:00+00:00, run_after=2023-05-14T00:00:00+00:00[0m
[[34m2023-09-11T06:49:51.726+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-11 00:00:00+00:00: scheduled__2023-05-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:47.060137+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:51.726+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-11 00:00:00+00:00, run_id=scheduled__2023-05-11T00:00:00+00:00, run_start_date=2023-09-11 06:49:47.076889+00:00, run_end_date=2023-09-11 06:49:51.726771+00:00, run_duration=4.649882, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-11 00:00:00+00:00, data_interval_end=2023-05-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:51.730+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-12T00:00:00+00:00, run_after=2023-05-13T00:00:00+00:00[0m
[[34m2023-09-11T06:49:51.744+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:51.745+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:51.745+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:51.748+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:51.749+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:51.749+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:51.752+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:53.594+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:53.732+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:53.910+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:53.942+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:54.409+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:54.410+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:54.480+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:54.544+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:55.239+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:55.251+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:54.630023+00:00, run_end_date=2023-09-11 06:49:54.849055+00:00, run_duration=0.219032, state=success, executor_state=success, try_number=1, max_tries=0, job_id=136, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:51.746373+00:00, queued_by_job_id=2, pid=42071[0m
[[34m2023-09-11T06:49:55.796+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-13T00:00:00+00:00, run_after=2023-05-14T00:00:00+00:00[0m
[[34m2023-09-11T06:49:55.819+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-12 00:00:00+00:00: scheduled__2023-05-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:51.688021+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:55.820+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-12 00:00:00+00:00, run_id=scheduled__2023-05-12T00:00:00+00:00, run_start_date=2023-09-11 06:49:51.705110+00:00, run_end_date=2023-09-11 06:49:55.819841+00:00, run_duration=4.114731, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-12 00:00:00+00:00, data_interval_end=2023-05-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:55.823+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-13T00:00:00+00:00, run_after=2023-05-14T00:00:00+00:00[0m
[[34m2023-09-11T06:49:56.691+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-14T00:00:00+00:00, run_after=2023-05-15T00:00:00+00:00[0m
[[34m2023-09-11T06:49:56.735+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:56.735+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:56.735+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:56.737+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:56.738+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:56.738+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:56.741+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:58.910+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:59.058+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:59.267+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:59.303+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:59.770+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:59.771+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:59.839+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:59.908+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:00.661+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:00.672+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:00.000840+00:00, run_end_date=2023-09-11 06:50:00.221774+00:00, run_duration=0.220934, state=success, executor_state=success, try_number=1, max_tries=0, job_id=137, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:56.736431+00:00, queued_by_job_id=2, pid=42082[0m
[[34m2023-09-11T06:50:00.701+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:50:01.147+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-15T00:00:00+00:00, run_after=2023-05-16T00:00:00+00:00[0m
[[34m2023-09-11T06:50:01.191+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-13 00:00:00+00:00: scheduled__2023-05-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:56.686710+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:01.193+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-13 00:00:00+00:00, run_id=scheduled__2023-05-13T00:00:00+00:00, run_start_date=2023-09-11 06:49:56.703420+00:00, run_end_date=2023-09-11 06:50:01.193295+00:00, run_duration=4.489875, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-13 00:00:00+00:00, data_interval_end=2023-05-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:01.199+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-14T00:00:00+00:00, run_after=2023-05-15T00:00:00+00:00[0m
[[34m2023-09-11T06:50:01.220+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:01.221+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:01.221+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:01.225+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:01.226+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:01.226+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:01.229+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:03.137+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:03.272+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:03.449+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:03.486+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:03.984+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:03.985+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:04.054+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:04.113+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:04.873+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:04.884+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:04.211560+00:00, run_end_date=2023-09-11 06:50:04.461743+00:00, run_duration=0.250183, state=success, executor_state=success, try_number=1, max_tries=0, job_id=138, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:01.222656+00:00, queued_by_job_id=2, pid=42091[0m
[[34m2023-09-11T06:50:05.288+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-15T00:00:00+00:00, run_after=2023-05-16T00:00:00+00:00[0m
[[34m2023-09-11T06:50:05.311+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-14 00:00:00+00:00: scheduled__2023-05-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:01.142939+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:05.312+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-14 00:00:00+00:00, run_id=scheduled__2023-05-14T00:00:00+00:00, run_start_date=2023-09-11 06:50:01.161715+00:00, run_end_date=2023-09-11 06:50:05.311890+00:00, run_duration=4.150175, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-14 00:00:00+00:00, data_interval_end=2023-05-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:05.315+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-15T00:00:00+00:00, run_after=2023-05-16T00:00:00+00:00[0m
[[34m2023-09-11T06:50:06.102+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-16T00:00:00+00:00, run_after=2023-05-17T00:00:00+00:00[0m
[[34m2023-09-11T06:50:06.146+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:06.147+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:06.147+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:06.149+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:06.150+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:06.150+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:06.153+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:07.999+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:08.126+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:08.300+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:08.333+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:08.803+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:08.803+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:08.873+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:08.930+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:09.667+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:09.678+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:09.015407+00:00, run_end_date=2023-09-11 06:50:09.250457+00:00, run_duration=0.23505, state=success, executor_state=success, try_number=1, max_tries=0, job_id=139, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:06.148204+00:00, queued_by_job_id=2, pid=42101[0m
[[34m2023-09-11T06:50:09.948+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-17T00:00:00+00:00, run_after=2023-05-18T00:00:00+00:00[0m
[[34m2023-09-11T06:50:09.984+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-15 00:00:00+00:00: scheduled__2023-05-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:06.097029+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:09.984+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-15 00:00:00+00:00, run_id=scheduled__2023-05-15T00:00:00+00:00, run_start_date=2023-09-11 06:50:06.114953+00:00, run_end_date=2023-09-11 06:50:09.984468+00:00, run_duration=3.869515, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-15 00:00:00+00:00, data_interval_end=2023-05-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:09.987+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-16T00:00:00+00:00, run_after=2023-05-17T00:00:00+00:00[0m
[[34m2023-09-11T06:50:10.003+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:10.003+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:10.003+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:10.005+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:10.006+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:10.006+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:10.009+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:11.832+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:11.970+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:12.139+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:12.170+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:12.636+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:12.637+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:12.707+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:12.767+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:13.483+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:13.495+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:12.849267+00:00, run_end_date=2023-09-11 06:50:13.065356+00:00, run_duration=0.216089, state=success, executor_state=success, try_number=1, max_tries=0, job_id=140, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:10.004201+00:00, queued_by_job_id=2, pid=42108[0m
[[34m2023-09-11T06:50:13.894+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-17T00:00:00+00:00, run_after=2023-05-18T00:00:00+00:00[0m
[[34m2023-09-11T06:50:13.918+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-16 00:00:00+00:00: scheduled__2023-05-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:09.943684+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:13.919+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-16 00:00:00+00:00, run_id=scheduled__2023-05-16T00:00:00+00:00, run_start_date=2023-09-11 06:50:09.961505+00:00, run_end_date=2023-09-11 06:50:13.919036+00:00, run_duration=3.957531, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-16 00:00:00+00:00, data_interval_end=2023-05-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:13.923+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-17T00:00:00+00:00, run_after=2023-05-18T00:00:00+00:00[0m
[[34m2023-09-11T06:50:15.137+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-18T00:00:00+00:00, run_after=2023-05-19T00:00:00+00:00[0m
[[34m2023-09-11T06:50:15.181+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:15.182+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:15.182+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:15.184+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:15.185+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:15.185+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:15.187+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:17.011+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:17.141+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:17.317+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:17.350+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:17.861+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:17.862+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:17.933+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:17.989+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:18.713+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:18.723+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:18.082606+00:00, run_end_date=2023-09-11 06:50:18.307204+00:00, run_duration=0.224598, state=success, executor_state=success, try_number=1, max_tries=0, job_id=141, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:15.182944+00:00, queued_by_job_id=2, pid=42118[0m
[[34m2023-09-11T06:50:19.253+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-19T00:00:00+00:00, run_after=2023-05-20T00:00:00+00:00[0m
[[34m2023-09-11T06:50:19.288+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-17 00:00:00+00:00: scheduled__2023-05-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:15.133011+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:19.288+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-17 00:00:00+00:00, run_id=scheduled__2023-05-17T00:00:00+00:00, run_start_date=2023-09-11 06:50:15.149518+00:00, run_end_date=2023-09-11 06:50:19.288709+00:00, run_duration=4.139191, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-17 00:00:00+00:00, data_interval_end=2023-05-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:19.293+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-18T00:00:00+00:00, run_after=2023-05-19T00:00:00+00:00[0m
[[34m2023-09-11T06:50:19.308+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:19.309+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:19.309+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:19.311+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:19.312+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:19.312+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:19.315+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:21.139+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:21.268+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:21.437+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:21.469+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:21.929+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:21.929+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:22.000+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:22.057+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:22.757+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:22.767+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:22.143861+00:00, run_end_date=2023-09-11 06:50:22.363800+00:00, run_duration=0.219939, state=success, executor_state=success, try_number=1, max_tries=0, job_id=142, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:19.310022+00:00, queued_by_job_id=2, pid=42125[0m
[[34m2023-09-11T06:50:23.110+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-19T00:00:00+00:00, run_after=2023-05-20T00:00:00+00:00[0m
[[34m2023-09-11T06:50:23.136+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-18 00:00:00+00:00: scheduled__2023-05-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:19.248200+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:23.137+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-18 00:00:00+00:00, run_id=scheduled__2023-05-18T00:00:00+00:00, run_start_date=2023-09-11 06:50:19.265971+00:00, run_end_date=2023-09-11 06:50:23.137316+00:00, run_duration=3.871345, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-18 00:00:00+00:00, data_interval_end=2023-05-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:23.141+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-19T00:00:00+00:00, run_after=2023-05-20T00:00:00+00:00[0m
[[34m2023-09-11T06:50:24.247+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-20T00:00:00+00:00, run_after=2023-05-21T00:00:00+00:00[0m
[[34m2023-09-11T06:50:24.292+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:24.292+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:24.293+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:24.295+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:24.296+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:24.296+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:24.299+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:26.136+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:26.263+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:26.436+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:26.467+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:26.932+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:26.933+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:27.001+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:27.057+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:27.756+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:27.766+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:27.141939+00:00, run_end_date=2023-09-11 06:50:27.362327+00:00, run_duration=0.220388, state=success, executor_state=success, try_number=1, max_tries=0, job_id=143, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:24.293802+00:00, queued_by_job_id=2, pid=42135[0m
[[34m2023-09-11T06:50:27.937+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-21T00:00:00+00:00, run_after=2023-05-22T00:00:00+00:00[0m
[[34m2023-09-11T06:50:27.974+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-19 00:00:00+00:00: scheduled__2023-05-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:24.242204+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:27.975+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-19 00:00:00+00:00, run_id=scheduled__2023-05-19T00:00:00+00:00, run_start_date=2023-09-11 06:50:24.260589+00:00, run_end_date=2023-09-11 06:50:27.975354+00:00, run_duration=3.714765, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-19 00:00:00+00:00, data_interval_end=2023-05-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:27.978+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-20T00:00:00+00:00, run_after=2023-05-21T00:00:00+00:00[0m
[[34m2023-09-11T06:50:27.993+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:27.994+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:27.994+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:27.996+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:27.996+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:27.997+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:27.999+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:29.837+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:29.968+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:30.141+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:30.173+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:30.643+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:30.643+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:30.713+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:30.771+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:31.486+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:31.497+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:30.854434+00:00, run_end_date=2023-09-11 06:50:31.088527+00:00, run_duration=0.234093, state=success, executor_state=success, try_number=1, max_tries=0, job_id=144, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:27.995016+00:00, queued_by_job_id=2, pid=42144[0m
[[34m2023-09-11T06:50:31.897+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-21T00:00:00+00:00, run_after=2023-05-22T00:00:00+00:00[0m
[[34m2023-09-11T06:50:31.920+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-20 00:00:00+00:00: scheduled__2023-05-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:27.933153+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:31.920+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-20 00:00:00+00:00, run_id=scheduled__2023-05-20T00:00:00+00:00, run_start_date=2023-09-11 06:50:27.951924+00:00, run_end_date=2023-09-11 06:50:31.920451+00:00, run_duration=3.968527, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-20 00:00:00+00:00, data_interval_end=2023-05-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:31.924+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-21T00:00:00+00:00, run_after=2023-05-22T00:00:00+00:00[0m
[[34m2023-09-11T06:50:33.147+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-22T00:00:00+00:00, run_after=2023-05-23T00:00:00+00:00[0m
[[34m2023-09-11T06:50:33.191+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:33.192+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:33.192+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:33.194+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:33.195+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:33.195+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:33.197+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:35.049+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:35.175+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:35.348+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:35.381+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:35.843+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:35.844+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:35.915+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:35.971+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:36.678+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:36.689+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:36.054469+00:00, run_end_date=2023-09-11 06:50:36.269632+00:00, run_duration=0.215163, state=success, executor_state=success, try_number=1, max_tries=0, job_id=145, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:33.193133+00:00, queued_by_job_id=2, pid=42154[0m
[[34m2023-09-11T06:50:36.955+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-23T00:00:00+00:00, run_after=2023-05-24T00:00:00+00:00[0m
[[34m2023-09-11T06:50:36.991+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-21 00:00:00+00:00: scheduled__2023-05-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:33.143451+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:36.992+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-21 00:00:00+00:00, run_id=scheduled__2023-05-21T00:00:00+00:00, run_start_date=2023-09-11 06:50:33.160352+00:00, run_end_date=2023-09-11 06:50:36.992309+00:00, run_duration=3.831957, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-21 00:00:00+00:00, data_interval_end=2023-05-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:36.995+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-22T00:00:00+00:00, run_after=2023-05-23T00:00:00+00:00[0m
[[34m2023-09-11T06:50:37.027+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:37.027+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:37.027+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:37.033+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:37.034+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:37.035+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:37.037+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:38.920+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:39.050+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:39.230+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:39.264+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:39.726+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:39.726+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:39.799+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:39.855+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:40.571+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:40.582+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:39.938382+00:00, run_end_date=2023-09-11 06:50:40.159545+00:00, run_duration=0.221163, state=success, executor_state=success, try_number=1, max_tries=0, job_id=146, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:37.031738+00:00, queued_by_job_id=2, pid=42164[0m
[[34m2023-09-11T06:50:40.942+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-23T00:00:00+00:00, run_after=2023-05-24T00:00:00+00:00[0m
[[34m2023-09-11T06:50:40.965+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-22 00:00:00+00:00: scheduled__2023-05-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:36.950453+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:40.965+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-22 00:00:00+00:00, run_id=scheduled__2023-05-22T00:00:00+00:00, run_start_date=2023-09-11 06:50:36.968861+00:00, run_end_date=2023-09-11 06:50:40.965655+00:00, run_duration=3.996794, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-22 00:00:00+00:00, data_interval_end=2023-05-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:40.969+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-23T00:00:00+00:00, run_after=2023-05-24T00:00:00+00:00[0m
[[34m2023-09-11T06:50:41.771+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-24T00:00:00+00:00, run_after=2023-05-25T00:00:00+00:00[0m
[[34m2023-09-11T06:50:41.816+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:41.816+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:41.816+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:41.818+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:41.819+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:41.819+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:41.822+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:43.650+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:43.780+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:43.967+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:44.000+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:44.463+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:44.464+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:44.545+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:44.625+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:45.358+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:45.369+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:44.734095+00:00, run_end_date=2023-09-11 06:50:44.957772+00:00, run_duration=0.223677, state=success, executor_state=success, try_number=1, max_tries=0, job_id=147, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:41.817322+00:00, queued_by_job_id=2, pid=42174[0m
[[34m2023-09-11T06:50:45.538+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-25T00:00:00+00:00, run_after=2023-05-26T00:00:00+00:00[0m
[[34m2023-09-11T06:50:45.575+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-23 00:00:00+00:00: scheduled__2023-05-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:41.766780+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:45.575+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-23 00:00:00+00:00, run_id=scheduled__2023-05-23T00:00:00+00:00, run_start_date=2023-09-11 06:50:41.784385+00:00, run_end_date=2023-09-11 06:50:45.575634+00:00, run_duration=3.791249, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-23 00:00:00+00:00, data_interval_end=2023-05-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:45.578+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-24T00:00:00+00:00, run_after=2023-05-25T00:00:00+00:00[0m
[[34m2023-09-11T06:50:45.594+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:45.594+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:45.594+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:45.596+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:45.597+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:45.597+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:45.600+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:47.451+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:47.601+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:47.804+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:47.847+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:48.364+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:48.365+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:48.436+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:48.495+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:49.338+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:49.349+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:48.584356+00:00, run_end_date=2023-09-11 06:50:48.829067+00:00, run_duration=0.244711, state=success, executor_state=success, try_number=1, max_tries=0, job_id=148, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:45.595219+00:00, queued_by_job_id=2, pid=42181[0m
[[34m2023-09-11T06:50:49.587+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-25T00:00:00+00:00, run_after=2023-05-26T00:00:00+00:00[0m
[[34m2023-09-11T06:50:49.611+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-24 00:00:00+00:00: scheduled__2023-05-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:45.533773+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:49.612+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-24 00:00:00+00:00, run_id=scheduled__2023-05-24T00:00:00+00:00, run_start_date=2023-09-11 06:50:45.552308+00:00, run_end_date=2023-09-11 06:50:49.612402+00:00, run_duration=4.060094, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-24 00:00:00+00:00, data_interval_end=2023-05-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:49.615+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-25T00:00:00+00:00, run_after=2023-05-26T00:00:00+00:00[0m
[[34m2023-09-11T06:50:50.590+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-26T00:00:00+00:00, run_after=2023-05-27T00:00:00+00:00[0m
[[34m2023-09-11T06:50:50.634+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:50.635+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:50.635+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:50.637+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:50.638+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:50.638+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:50.641+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:52.866+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:53.067+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:53.259+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:53.293+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:53.828+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:53.829+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:53.920+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:53.979+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:54.795+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:54.808+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:54.064896+00:00, run_end_date=2023-09-11 06:50:54.299782+00:00, run_duration=0.234886, state=success, executor_state=success, try_number=1, max_tries=0, job_id=149, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:50.635975+00:00, queued_by_job_id=2, pid=42193[0m
[[34m2023-09-11T06:50:54.973+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-27T00:00:00+00:00, run_after=2023-05-28T00:00:00+00:00[0m
[[34m2023-09-11T06:50:55.025+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-25 00:00:00+00:00: scheduled__2023-05-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:50.585536+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:55.026+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-25 00:00:00+00:00, run_id=scheduled__2023-05-25T00:00:00+00:00, run_start_date=2023-09-11 06:50:50.603199+00:00, run_end_date=2023-09-11 06:50:55.026263+00:00, run_duration=4.423064, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-25 00:00:00+00:00, data_interval_end=2023-05-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:55.029+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-26T00:00:00+00:00, run_after=2023-05-27T00:00:00+00:00[0m
[[34m2023-09-11T06:50:55.045+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:55.045+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:55.045+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:55.048+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:55.048+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:55.048+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:55.051+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:56.881+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:57.007+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:57.207+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:57.239+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:57.713+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:57.714+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:57.783+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:57.842+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:58.557+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:58.568+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:57.926431+00:00, run_end_date=2023-09-11 06:50:58.147432+00:00, run_duration=0.221001, state=success, executor_state=success, try_number=1, max_tries=0, job_id=150, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:55.046624+00:00, queued_by_job_id=2, pid=42200[0m
[[34m2023-09-11T06:50:58.711+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-27T00:00:00+00:00, run_after=2023-05-28T00:00:00+00:00[0m
[[34m2023-09-11T06:50:58.734+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-26 00:00:00+00:00: scheduled__2023-05-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:54.968349+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:58.734+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-26 00:00:00+00:00, run_id=scheduled__2023-05-26T00:00:00+00:00, run_start_date=2023-09-11 06:50:55.003272+00:00, run_end_date=2023-09-11 06:50:58.734685+00:00, run_duration=3.731413, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-26 00:00:00+00:00, data_interval_end=2023-05-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:58.738+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-27T00:00:00+00:00, run_after=2023-05-28T00:00:00+00:00[0m
[[34m2023-09-11T06:50:59.913+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-28T00:00:00+00:00, run_after=2023-05-29T00:00:00+00:00[0m
[[34m2023-09-11T06:50:59.967+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:59.967+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:59.967+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:59.969+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:59.970+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:59.970+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:59.973+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:01.829+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:02.016+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:02.218+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:02.259+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:02.744+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:02.745+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:02.819+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:02.881+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:03.621+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:03.636+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:02.970861+00:00, run_end_date=2023-09-11 06:51:03.204881+00:00, run_duration=0.23402, state=success, executor_state=success, try_number=1, max_tries=0, job_id=151, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:59.968231+00:00, queued_by_job_id=2, pid=42210[0m
[[34m2023-09-11T06:51:03.922+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-29T00:00:00+00:00, run_after=2023-05-30T00:00:00+00:00[0m
[[34m2023-09-11T06:51:03.958+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-27 00:00:00+00:00: scheduled__2023-05-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:59.909266+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:03.958+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-27 00:00:00+00:00, run_id=scheduled__2023-05-27T00:00:00+00:00, run_start_date=2023-09-11 06:50:59.935286+00:00, run_end_date=2023-09-11 06:51:03.958585+00:00, run_duration=4.023299, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-27 00:00:00+00:00, data_interval_end=2023-05-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:03.962+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-28T00:00:00+00:00, run_after=2023-05-29T00:00:00+00:00[0m
[[34m2023-09-11T06:51:03.979+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:03.980+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:03.980+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:03.982+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:03.983+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:03.983+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:03.986+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:06.286+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:06.448+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:06.681+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:06.743+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:07.531+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:07.532+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:07.661+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:07.777+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:08.788+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:08.800+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:07.881721+00:00, run_end_date=2023-09-11 06:51:08.215722+00:00, run_duration=0.334001, state=success, executor_state=success, try_number=1, max_tries=0, job_id=152, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:03.981072+00:00, queued_by_job_id=2, pid=42219[0m
[[34m2023-09-11T06:51:09.069+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-29T00:00:00+00:00, run_after=2023-05-30T00:00:00+00:00[0m
[[34m2023-09-11T06:51:09.096+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-28 00:00:00+00:00: scheduled__2023-05-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:03.917450+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:09.096+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-28 00:00:00+00:00, run_id=scheduled__2023-05-28T00:00:00+00:00, run_start_date=2023-09-11 06:51:03.935166+00:00, run_end_date=2023-09-11 06:51:09.096502+00:00, run_duration=5.161336, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-28 00:00:00+00:00, data_interval_end=2023-05-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:09.099+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-29T00:00:00+00:00, run_after=2023-05-30T00:00:00+00:00[0m
[[34m2023-09-11T06:51:10.089+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-30T00:00:00+00:00, run_after=2023-05-31T00:00:00+00:00[0m
[[34m2023-09-11T06:51:10.169+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:10.169+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:10.169+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:10.175+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:10.175+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:10.176+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:10.179+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:12.291+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:12.442+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:12.634+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:12.671+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:13.190+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:13.191+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:13.268+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:13.335+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:14.162+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:14.174+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:13.432458+00:00, run_end_date=2023-09-11 06:51:13.701137+00:00, run_duration=0.268679, state=success, executor_state=success, try_number=1, max_tries=0, job_id=153, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:10.172958+00:00, queued_by_job_id=2, pid=42229[0m
[[34m2023-09-11T06:51:14.448+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-31T00:00:00+00:00, run_after=2023-06-01T00:00:00+00:00[0m
[[34m2023-09-11T06:51:14.490+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-29 00:00:00+00:00: scheduled__2023-05-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:10.083529+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:14.490+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-29 00:00:00+00:00, run_id=scheduled__2023-05-29T00:00:00+00:00, run_start_date=2023-09-11 06:51:10.131469+00:00, run_end_date=2023-09-11 06:51:14.490784+00:00, run_duration=4.359315, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-29 00:00:00+00:00, data_interval_end=2023-05-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:14.494+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-30T00:00:00+00:00, run_after=2023-05-31T00:00:00+00:00[0m
[[34m2023-09-11T06:51:14.511+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:14.511+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:14.512+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:14.514+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:14.514+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:14.514+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:14.517+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:16.551+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:16.706+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:16.894+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:16.931+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:17.485+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:17.486+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:17.564+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:17.662+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:18.555+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:18.566+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:17.771515+00:00, run_end_date=2023-09-11 06:51:18.076870+00:00, run_duration=0.305355, state=success, executor_state=success, try_number=1, max_tries=0, job_id=154, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:14.512687+00:00, queued_by_job_id=2, pid=42238[0m
[[34m2023-09-11T06:51:18.818+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-31T00:00:00+00:00, run_after=2023-06-01T00:00:00+00:00[0m
[[34m2023-09-11T06:51:18.845+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-30 00:00:00+00:00: scheduled__2023-05-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:14.443468+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:18.845+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-30 00:00:00+00:00, run_id=scheduled__2023-05-30T00:00:00+00:00, run_start_date=2023-09-11 06:51:14.464065+00:00, run_end_date=2023-09-11 06:51:18.845635+00:00, run_duration=4.38157, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-30 00:00:00+00:00, data_interval_end=2023-05-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:18.849+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-31T00:00:00+00:00, run_after=2023-06-01T00:00:00+00:00[0m
[[34m2023-09-11T06:51:19.464+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-01T00:00:00+00:00, run_after=2023-06-02T00:00:00+00:00[0m
[[34m2023-09-11T06:51:19.516+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:19.517+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:19.517+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:19.520+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:19.522+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:19.522+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:19.525+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:21.605+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:21.750+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:21.942+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:21.978+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:22.496+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:22.496+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:22.580+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:22.647+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:23.479+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:23.491+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:22.743488+00:00, run_end_date=2023-09-11 06:51:22.996973+00:00, run_duration=0.253485, state=success, executor_state=success, try_number=1, max_tries=0, job_id=155, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:19.518043+00:00, queued_by_job_id=2, pid=42248[0m
[[34m2023-09-11T06:51:23.751+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-02T00:00:00+00:00, run_after=2023-06-03T00:00:00+00:00[0m
[[34m2023-09-11T06:51:23.793+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-31 00:00:00+00:00: scheduled__2023-05-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:19.459098+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:23.793+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-31 00:00:00+00:00, run_id=scheduled__2023-05-31T00:00:00+00:00, run_start_date=2023-09-11 06:51:19.479622+00:00, run_end_date=2023-09-11 06:51:23.793730+00:00, run_duration=4.314108, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-31 00:00:00+00:00, data_interval_end=2023-06-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:23.797+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-01T00:00:00+00:00, run_after=2023-06-02T00:00:00+00:00[0m
[[34m2023-09-11T06:51:23.815+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:23.815+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:23.815+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:23.819+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:23.821+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:23.821+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:23.824+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:25.945+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:26.091+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:26.281+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:26.316+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:26.868+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:26.869+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:26.949+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:27.014+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:27.826+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:27.837+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:27.110113+00:00, run_end_date=2023-09-11 06:51:27.365927+00:00, run_duration=0.255814, state=success, executor_state=success, try_number=1, max_tries=0, job_id=156, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:23.816956+00:00, queued_by_job_id=2, pid=42255[0m
[[34m2023-09-11T06:51:28.095+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-02T00:00:00+00:00, run_after=2023-06-03T00:00:00+00:00[0m
[[34m2023-09-11T06:51:28.122+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-01 00:00:00+00:00: scheduled__2023-06-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:23.746549+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:28.123+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-01 00:00:00+00:00, run_id=scheduled__2023-06-01T00:00:00+00:00, run_start_date=2023-09-11 06:51:23.765519+00:00, run_end_date=2023-09-11 06:51:28.123097+00:00, run_duration=4.357578, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-01 00:00:00+00:00, data_interval_end=2023-06-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:28.126+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-02T00:00:00+00:00, run_after=2023-06-03T00:00:00+00:00[0m
[[34m2023-09-11T06:51:28.825+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-03T00:00:00+00:00, run_after=2023-06-04T00:00:00+00:00[0m
[[34m2023-09-11T06:51:28.874+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:28.874+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:28.875+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:28.877+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:28.878+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:28.878+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:28.881+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:30.945+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:31.092+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:31.281+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:31.327+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:31.908+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:31.909+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:31.985+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:32.051+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:32.867+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:32.879+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:32.149418+00:00, run_end_date=2023-09-11 06:51:32.394654+00:00, run_duration=0.245236, state=success, executor_state=success, try_number=1, max_tries=0, job_id=157, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:28.875904+00:00, queued_by_job_id=2, pid=42265[0m
[[34m2023-09-11T06:51:33.156+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-04T00:00:00+00:00, run_after=2023-06-05T00:00:00+00:00[0m
[[34m2023-09-11T06:51:33.195+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-02 00:00:00+00:00: scheduled__2023-06-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:28.818604+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:33.195+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-02 00:00:00+00:00, run_id=scheduled__2023-06-02T00:00:00+00:00, run_start_date=2023-09-11 06:51:28.839695+00:00, run_end_date=2023-09-11 06:51:33.195798+00:00, run_duration=4.356103, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-02 00:00:00+00:00, data_interval_end=2023-06-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:33.199+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-03T00:00:00+00:00, run_after=2023-06-04T00:00:00+00:00[0m
[[34m2023-09-11T06:51:33.216+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:33.216+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:33.216+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:33.218+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:33.219+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:33.220+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:33.223+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:35.326+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:35.473+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:35.667+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:35.704+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:36.224+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:36.225+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:36.302+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:36.368+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:37.200+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:37.212+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:36.462978+00:00, run_end_date=2023-09-11 06:51:36.711263+00:00, run_duration=0.248285, state=success, executor_state=success, try_number=1, max_tries=0, job_id=158, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:33.217330+00:00, queued_by_job_id=2, pid=42274[0m
[[34m2023-09-11T06:51:37.525+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-04T00:00:00+00:00, run_after=2023-06-05T00:00:00+00:00[0m
[[34m2023-09-11T06:51:37.549+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-03 00:00:00+00:00: scheduled__2023-06-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:33.149711+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:37.550+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-03 00:00:00+00:00, run_id=scheduled__2023-06-03T00:00:00+00:00, run_start_date=2023-09-11 06:51:33.169691+00:00, run_end_date=2023-09-11 06:51:37.550295+00:00, run_duration=4.380604, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-03 00:00:00+00:00, data_interval_end=2023-06-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:37.555+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-04T00:00:00+00:00, run_after=2023-06-05T00:00:00+00:00[0m
[[34m2023-09-11T06:51:38.477+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-05T00:00:00+00:00, run_after=2023-06-06T00:00:00+00:00[0m
[[34m2023-09-11T06:51:38.527+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:38.528+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:38.528+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:38.530+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:38.530+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:38.531+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:38.533+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:40.561+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:40.705+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:40.905+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:40.941+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:41.459+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:41.459+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:41.538+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:41.602+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:42.416+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:42.428+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:41.697259+00:00, run_end_date=2023-09-11 06:51:41.956023+00:00, run_duration=0.258764, state=success, executor_state=success, try_number=1, max_tries=0, job_id=159, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:38.528999+00:00, queued_by_job_id=2, pid=42284[0m
[[34m2023-09-11T06:51:42.687+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-06T00:00:00+00:00, run_after=2023-06-07T00:00:00+00:00[0m
[[34m2023-09-11T06:51:42.726+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-04 00:00:00+00:00: scheduled__2023-06-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:38.473062+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:42.726+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-04 00:00:00+00:00, run_id=scheduled__2023-06-04T00:00:00+00:00, run_start_date=2023-09-11 06:51:38.491221+00:00, run_end_date=2023-09-11 06:51:42.726817+00:00, run_duration=4.235596, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-04 00:00:00+00:00, data_interval_end=2023-06-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:42.730+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-05T00:00:00+00:00, run_after=2023-06-06T00:00:00+00:00[0m
[[34m2023-09-11T06:51:42.747+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:42.747+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:42.747+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:42.749+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:42.750+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:42.750+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:42.754+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:44.848+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:44.994+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:45.194+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:45.230+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:45.773+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:45.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:45.851+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:45.918+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:46.734+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:46.746+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:46.014619+00:00, run_end_date=2023-09-11 06:51:46.261548+00:00, run_duration=0.246929, state=success, executor_state=success, try_number=1, max_tries=0, job_id=160, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:42.748295+00:00, queued_by_job_id=2, pid=42293[0m
[[34m2023-09-11T06:51:47.000+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-06T00:00:00+00:00, run_after=2023-06-07T00:00:00+00:00[0m
[[34m2023-09-11T06:51:47.026+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-05 00:00:00+00:00: scheduled__2023-06-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:42.681660+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:47.026+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-05 00:00:00+00:00, run_id=scheduled__2023-06-05T00:00:00+00:00, run_start_date=2023-09-11 06:51:42.700857+00:00, run_end_date=2023-09-11 06:51:47.026827+00:00, run_duration=4.32597, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-05 00:00:00+00:00, data_interval_end=2023-06-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:47.030+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-06T00:00:00+00:00, run_after=2023-06-07T00:00:00+00:00[0m
[[34m2023-09-11T06:51:47.686+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-07T00:00:00+00:00, run_after=2023-06-08T00:00:00+00:00[0m
[[34m2023-09-11T06:51:47.733+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:47.734+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:47.734+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:47.738+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:47.738+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:47.739+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:47.741+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:49.866+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:50.020+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:50.213+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:50.249+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:50.764+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:50.764+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:50.841+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:50.914+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:51.737+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:51.748+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:51.011581+00:00, run_end_date=2023-09-11 06:51:51.262449+00:00, run_duration=0.250868, state=success, executor_state=success, try_number=1, max_tries=0, job_id=161, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:47.734940+00:00, queued_by_job_id=2, pid=42303[0m
[[34m2023-09-11T06:51:52.012+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-08T00:00:00+00:00, run_after=2023-06-09T00:00:00+00:00[0m
[[34m2023-09-11T06:51:52.052+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-06 00:00:00+00:00: scheduled__2023-06-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:47.681359+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:52.053+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-06 00:00:00+00:00, run_id=scheduled__2023-06-06T00:00:00+00:00, run_start_date=2023-09-11 06:51:47.700323+00:00, run_end_date=2023-09-11 06:51:52.053690+00:00, run_duration=4.353367, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-06 00:00:00+00:00, data_interval_end=2023-06-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:52.057+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-07T00:00:00+00:00, run_after=2023-06-08T00:00:00+00:00[0m
[[34m2023-09-11T06:51:52.075+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:52.075+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:52.076+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:52.078+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:52.078+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:52.079+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:52.081+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:54.123+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:54.265+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:54.457+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:54.493+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:55.014+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:55.015+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:55.094+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:55.158+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:55.966+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:55.978+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:55.250020+00:00, run_end_date=2023-09-11 06:51:55.523778+00:00, run_duration=0.273758, state=success, executor_state=success, try_number=1, max_tries=0, job_id=162, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:52.076789+00:00, queued_by_job_id=2, pid=42312[0m
[[34m2023-09-11T06:51:56.227+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-08T00:00:00+00:00, run_after=2023-06-09T00:00:00+00:00[0m
[[34m2023-09-11T06:51:56.252+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-07 00:00:00+00:00: scheduled__2023-06-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:52.006953+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:56.253+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-07 00:00:00+00:00, run_id=scheduled__2023-06-07T00:00:00+00:00, run_start_date=2023-09-11 06:51:52.027201+00:00, run_end_date=2023-09-11 06:51:56.253414+00:00, run_duration=4.226213, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-07 00:00:00+00:00, data_interval_end=2023-06-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:56.257+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-08T00:00:00+00:00, run_after=2023-06-09T00:00:00+00:00[0m
[[34m2023-09-11T06:51:57.309+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-09T00:00:00+00:00, run_after=2023-06-10T00:00:00+00:00[0m
[[34m2023-09-11T06:51:57.362+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:57.363+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:57.363+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:57.365+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:57.366+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:57.366+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:57.370+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:59.480+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:59.628+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:59.822+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:59.861+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:00.421+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:00.422+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:00.499+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:00.565+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:01.383+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:01.407+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:00.663573+00:00, run_end_date=2023-09-11 06:52:00.909032+00:00, run_duration=0.245459, state=success, executor_state=success, try_number=1, max_tries=0, job_id=163, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:57.363944+00:00, queued_by_job_id=2, pid=42322[0m
[[34m2023-09-11T06:52:01.581+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-10T00:00:00+00:00, run_after=2023-06-11T00:00:00+00:00[0m
[[34m2023-09-11T06:52:01.624+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-08 00:00:00+00:00: scheduled__2023-06-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:57.304325+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:01.624+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-08 00:00:00+00:00, run_id=scheduled__2023-06-08T00:00:00+00:00, run_start_date=2023-09-11 06:51:57.326485+00:00, run_end_date=2023-09-11 06:52:01.624800+00:00, run_duration=4.298315, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-08 00:00:00+00:00, data_interval_end=2023-06-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:01.628+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-09T00:00:00+00:00, run_after=2023-06-10T00:00:00+00:00[0m
[[34m2023-09-11T06:52:01.644+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:01.645+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:01.645+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:01.647+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:01.647+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:01.648+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:01.666+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:03.765+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:03.941+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:04.144+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:04.181+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:04.719+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:04.720+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:04.797+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:04.862+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:05.756+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:05.767+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:04.963304+00:00, run_end_date=2023-09-11 06:52:05.220397+00:00, run_duration=0.257093, state=success, executor_state=success, try_number=1, max_tries=0, job_id=164, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:01.646119+00:00, queued_by_job_id=2, pid=42331[0m
[[34m2023-09-11T06:52:06.013+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-10T00:00:00+00:00, run_after=2023-06-11T00:00:00+00:00[0m
[[34m2023-09-11T06:52:06.040+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-09 00:00:00+00:00: scheduled__2023-06-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:01.576821+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:06.041+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-09 00:00:00+00:00, run_id=scheduled__2023-06-09T00:00:00+00:00, run_start_date=2023-09-11 06:52:01.596176+00:00, run_end_date=2023-09-11 06:52:06.041237+00:00, run_duration=4.445061, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-09 00:00:00+00:00, data_interval_end=2023-06-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:06.044+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-10T00:00:00+00:00, run_after=2023-06-11T00:00:00+00:00[0m
[[34m2023-09-11T06:52:06.684+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-11T00:00:00+00:00, run_after=2023-06-12T00:00:00+00:00[0m
[[34m2023-09-11T06:52:06.732+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:06.732+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:06.732+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:06.735+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:06.736+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:06.737+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:06.740+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:08.782+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:08.939+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:09.129+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:09.166+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:09.713+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:09.714+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:09.794+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:09.859+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:10.676+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:10.689+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:09.956297+00:00, run_end_date=2023-09-11 06:52:10.205970+00:00, run_duration=0.249673, state=success, executor_state=success, try_number=1, max_tries=0, job_id=165, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:06.733487+00:00, queued_by_job_id=2, pid=42341[0m
[[34m2023-09-11T06:52:10.962+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-12T00:00:00+00:00, run_after=2023-06-13T00:00:00+00:00[0m
[[34m2023-09-11T06:52:11.003+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-10 00:00:00+00:00: scheduled__2023-06-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:06.679749+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:11.004+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-10 00:00:00+00:00, run_id=scheduled__2023-06-10T00:00:00+00:00, run_start_date=2023-09-11 06:52:06.698445+00:00, run_end_date=2023-09-11 06:52:11.003892+00:00, run_duration=4.305447, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-10 00:00:00+00:00, data_interval_end=2023-06-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:11.007+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-11T00:00:00+00:00, run_after=2023-06-12T00:00:00+00:00[0m
[[34m2023-09-11T06:52:11.024+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:11.024+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:11.025+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:11.027+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:11.027+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:11.027+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:11.030+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:13.101+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:13.249+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:13.458+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:13.495+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:14.020+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:14.021+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:14.100+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:14.166+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:14.991+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:15.004+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:14.262481+00:00, run_end_date=2023-09-11 06:52:14.509114+00:00, run_duration=0.246633, state=success, executor_state=success, try_number=1, max_tries=0, job_id=166, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:11.025767+00:00, queued_by_job_id=2, pid=42350[0m
[[34m2023-09-11T06:52:15.256+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-12T00:00:00+00:00, run_after=2023-06-13T00:00:00+00:00[0m
[[34m2023-09-11T06:52:15.281+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-11 00:00:00+00:00: scheduled__2023-06-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:10.957756+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:15.282+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-11 00:00:00+00:00, run_id=scheduled__2023-06-11T00:00:00+00:00, run_start_date=2023-09-11 06:52:10.978118+00:00, run_end_date=2023-09-11 06:52:15.282320+00:00, run_duration=4.304202, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-11 00:00:00+00:00, data_interval_end=2023-06-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:15.287+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-12T00:00:00+00:00, run_after=2023-06-13T00:00:00+00:00[0m
[[34m2023-09-11T06:52:15.965+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-13T00:00:00+00:00, run_after=2023-06-14T00:00:00+00:00[0m
[[34m2023-09-11T06:52:16.014+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:16.015+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:16.015+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:16.017+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:16.019+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:16.019+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:16.023+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:18.173+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:18.345+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:18.557+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:18.597+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:19.194+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:19.195+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:19.272+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:19.332+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:20.151+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:20.162+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:19.438906+00:00, run_end_date=2023-09-11 06:52:19.707990+00:00, run_duration=0.269084, state=success, executor_state=success, try_number=1, max_tries=0, job_id=167, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:16.016028+00:00, queued_by_job_id=2, pid=42360[0m
[[34m2023-09-11T06:52:20.444+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-14T00:00:00+00:00, run_after=2023-06-15T00:00:00+00:00[0m
[[34m2023-09-11T06:52:20.488+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-12 00:00:00+00:00: scheduled__2023-06-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:15.960287+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:20.489+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-12 00:00:00+00:00, run_id=scheduled__2023-06-12T00:00:00+00:00, run_start_date=2023-09-11 06:52:15.979834+00:00, run_end_date=2023-09-11 06:52:20.489033+00:00, run_duration=4.509199, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-12 00:00:00+00:00, data_interval_end=2023-06-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:20.492+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-13T00:00:00+00:00, run_after=2023-06-14T00:00:00+00:00[0m
[[34m2023-09-11T06:52:20.513+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:20.513+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:20.514+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:20.516+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:20.517+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:20.518+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:20.522+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:22.837+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:22.989+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:23.182+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:23.220+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:23.743+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:23.744+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:23.821+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:23.884+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:24.792+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:24.804+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:23.989612+00:00, run_end_date=2023-09-11 06:52:24.270669+00:00, run_duration=0.281057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=168, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:20.514898+00:00, queued_by_job_id=2, pid=42369[0m
[[34m2023-09-11T06:52:25.047+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-14T00:00:00+00:00, run_after=2023-06-15T00:00:00+00:00[0m
[[34m2023-09-11T06:52:25.073+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-13 00:00:00+00:00: scheduled__2023-06-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:20.436166+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:25.073+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-13 00:00:00+00:00, run_id=scheduled__2023-06-13T00:00:00+00:00, run_start_date=2023-09-11 06:52:20.460747+00:00, run_end_date=2023-09-11 06:52:25.073710+00:00, run_duration=4.612963, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-13 00:00:00+00:00, data_interval_end=2023-06-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:25.077+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-14T00:00:00+00:00, run_after=2023-06-15T00:00:00+00:00[0m
[[34m2023-09-11T06:52:25.421+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-15T00:00:00+00:00, run_after=2023-06-16T00:00:00+00:00[0m
[[34m2023-09-11T06:52:25.496+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:25.496+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:25.497+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:25.499+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:25.500+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:25.500+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:25.504+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:27.627+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:27.790+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:27.982+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:28.020+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:28.615+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:28.615+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:28.694+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:28.774+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:29.624+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:29.636+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:28.866094+00:00, run_end_date=2023-09-11 06:52:29.113662+00:00, run_duration=0.247568, state=success, executor_state=success, try_number=1, max_tries=0, job_id=169, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:25.497916+00:00, queued_by_job_id=2, pid=42377[0m
[[34m2023-09-11T06:52:29.923+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-16T00:00:00+00:00, run_after=2023-06-17T00:00:00+00:00[0m
[[34m2023-09-11T06:52:29.963+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-14 00:00:00+00:00: scheduled__2023-06-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:25.414785+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:29.964+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-14 00:00:00+00:00, run_id=scheduled__2023-06-14T00:00:00+00:00, run_start_date=2023-09-11 06:52:25.446692+00:00, run_end_date=2023-09-11 06:52:29.964008+00:00, run_duration=4.517316, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-14 00:00:00+00:00, data_interval_end=2023-06-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:29.968+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-15T00:00:00+00:00, run_after=2023-06-16T00:00:00+00:00[0m
[[34m2023-09-11T06:52:29.985+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:29.986+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:29.986+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:29.989+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:29.989+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:29.989+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:29.993+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:32.055+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:32.208+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:32.406+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:32.443+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:32.979+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:32.979+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:33.059+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:33.125+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:33.944+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:33.956+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:33.223363+00:00, run_end_date=2023-09-11 06:52:33.482911+00:00, run_duration=0.259548, state=success, executor_state=success, try_number=1, max_tries=0, job_id=170, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:29.987373+00:00, queued_by_job_id=2, pid=42386[0m
[[34m2023-09-11T06:52:34.214+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-16T00:00:00+00:00, run_after=2023-06-17T00:00:00+00:00[0m
[[34m2023-09-11T06:52:34.239+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-15 00:00:00+00:00: scheduled__2023-06-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:29.916295+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:34.240+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-15 00:00:00+00:00, run_id=scheduled__2023-06-15T00:00:00+00:00, run_start_date=2023-09-11 06:52:29.938234+00:00, run_end_date=2023-09-11 06:52:34.240069+00:00, run_duration=4.301835, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-15 00:00:00+00:00, data_interval_end=2023-06-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:34.243+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-16T00:00:00+00:00, run_after=2023-06-17T00:00:00+00:00[0m
[[34m2023-09-11T06:52:34.948+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-17T00:00:00+00:00, run_after=2023-06-18T00:00:00+00:00[0m
[[34m2023-09-11T06:52:34.997+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:34.997+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:34.998+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:35.001+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:35.002+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:35.003+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:35.006+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:37.026+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:37.169+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:37.364+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:37.404+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:37.956+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:37.957+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:38.042+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:38.109+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:39.064+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:39.076+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:38.207264+00:00, run_end_date=2023-09-11 06:52:38.585413+00:00, run_duration=0.378149, state=success, executor_state=success, try_number=1, max_tries=0, job_id=171, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:34.998854+00:00, queued_by_job_id=2, pid=42396[0m
[[34m2023-09-11T06:52:39.353+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-18T00:00:00+00:00, run_after=2023-06-19T00:00:00+00:00[0m
[[34m2023-09-11T06:52:39.393+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-16 00:00:00+00:00: scheduled__2023-06-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:34.944236+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:39.394+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-16 00:00:00+00:00, run_id=scheduled__2023-06-16T00:00:00+00:00, run_start_date=2023-09-11 06:52:34.962929+00:00, run_end_date=2023-09-11 06:52:39.394101+00:00, run_duration=4.431172, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-16 00:00:00+00:00, data_interval_end=2023-06-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:39.397+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-17T00:00:00+00:00, run_after=2023-06-18T00:00:00+00:00[0m
[[34m2023-09-11T06:52:39.413+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:39.414+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:39.414+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:39.417+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:39.418+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:39.418+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:39.422+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:41.496+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:41.640+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:41.833+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:41.870+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:42.409+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:42.409+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:42.490+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:42.557+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:43.408+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:43.421+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:42.657290+00:00, run_end_date=2023-09-11 06:52:42.931297+00:00, run_duration=0.274007, state=success, executor_state=success, try_number=1, max_tries=0, job_id=172, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:39.415163+00:00, queued_by_job_id=2, pid=42403[0m
[[34m2023-09-11T06:52:43.563+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-18T00:00:00+00:00, run_after=2023-06-19T00:00:00+00:00[0m
[[34m2023-09-11T06:52:43.589+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-17 00:00:00+00:00: scheduled__2023-06-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:39.346989+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:43.590+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-17 00:00:00+00:00, run_id=scheduled__2023-06-17T00:00:00+00:00, run_start_date=2023-09-11 06:52:39.368567+00:00, run_end_date=2023-09-11 06:52:43.590004+00:00, run_duration=4.221437, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-17 00:00:00+00:00, data_interval_end=2023-06-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:43.593+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-18T00:00:00+00:00, run_after=2023-06-19T00:00:00+00:00[0m
[[34m2023-09-11T06:52:44.999+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-19T00:00:00+00:00, run_after=2023-06-20T00:00:00+00:00[0m
[[34m2023-09-11T06:52:45.047+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:45.047+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:45.048+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:45.051+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:45.052+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:45.052+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:45.055+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:47.143+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:47.288+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:47.488+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:47.525+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:48.062+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:48.062+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:48.145+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:48.237+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:49.063+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:49.075+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:48.346812+00:00, run_end_date=2023-09-11 06:52:48.623153+00:00, run_duration=0.276341, state=success, executor_state=success, try_number=1, max_tries=0, job_id=173, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:45.048881+00:00, queued_by_job_id=2, pid=42415[0m
[[34m2023-09-11T06:52:49.417+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-20T00:00:00+00:00, run_after=2023-06-21T00:00:00+00:00[0m
[[34m2023-09-11T06:52:49.458+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-18 00:00:00+00:00: scheduled__2023-06-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:44.994362+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:49.459+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-18 00:00:00+00:00, run_id=scheduled__2023-06-18T00:00:00+00:00, run_start_date=2023-09-11 06:52:45.013028+00:00, run_end_date=2023-09-11 06:52:49.458921+00:00, run_duration=4.445893, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-18 00:00:00+00:00, data_interval_end=2023-06-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:49.462+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-19T00:00:00+00:00, run_after=2023-06-20T00:00:00+00:00[0m
[[34m2023-09-11T06:52:49.479+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:49.479+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:49.480+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:49.482+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:49.484+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:49.485+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:49.488+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:51.548+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:51.696+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:51.889+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:51.925+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:52.445+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:52.446+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:52.525+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:52.589+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:53.404+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:53.414+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:52.683861+00:00, run_end_date=2023-09-11 06:52:52.941726+00:00, run_duration=0.257865, state=success, executor_state=success, try_number=1, max_tries=0, job_id=174, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:49.480975+00:00, queued_by_job_id=2, pid=42422[0m
[[34m2023-09-11T06:52:53.696+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-20T00:00:00+00:00, run_after=2023-06-21T00:00:00+00:00[0m
[[34m2023-09-11T06:52:53.722+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-19 00:00:00+00:00: scheduled__2023-06-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:49.411184+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:53.722+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-19 00:00:00+00:00, run_id=scheduled__2023-06-19T00:00:00+00:00, run_start_date=2023-09-11 06:52:49.431759+00:00, run_end_date=2023-09-11 06:52:53.722655+00:00, run_duration=4.290896, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-19 00:00:00+00:00, data_interval_end=2023-06-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:53.726+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-20T00:00:00+00:00, run_after=2023-06-21T00:00:00+00:00[0m
[[34m2023-09-11T06:52:54.411+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-21T00:00:00+00:00, run_after=2023-06-22T00:00:00+00:00[0m
[[34m2023-09-11T06:52:54.463+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:54.463+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:54.464+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:54.467+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:54.468+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:54.468+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:54.471+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:56.514+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:56.662+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:56.854+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:56.890+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:57.448+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:57.450+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:57.533+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:57.596+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:58.451+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:58.462+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:57.707380+00:00, run_end_date=2023-09-11 06:52:57.963868+00:00, run_duration=0.256488, state=success, executor_state=success, try_number=1, max_tries=0, job_id=175, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:54.464787+00:00, queued_by_job_id=2, pid=42432[0m
[[34m2023-09-11T06:52:58.679+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-22T00:00:00+00:00, run_after=2023-06-23T00:00:00+00:00[0m
[[34m2023-09-11T06:52:58.720+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-20 00:00:00+00:00: scheduled__2023-06-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:54.406564+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:58.720+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-20 00:00:00+00:00, run_id=scheduled__2023-06-20T00:00:00+00:00, run_start_date=2023-09-11 06:52:54.426732+00:00, run_end_date=2023-09-11 06:52:58.720561+00:00, run_duration=4.293829, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-20 00:00:00+00:00, data_interval_end=2023-06-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:58.724+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-21T00:00:00+00:00, run_after=2023-06-22T00:00:00+00:00[0m
[[34m2023-09-11T06:52:58.741+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:58.741+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:58.741+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:58.743+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:58.744+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:58.744+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:58.747+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:00.818+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:00.963+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:01.156+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:01.191+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:01.718+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:01.718+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:01.806+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:01.872+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:02.714+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:02.726+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:01.979415+00:00, run_end_date=2023-09-11 06:53:02.234854+00:00, run_duration=0.255439, state=success, executor_state=success, try_number=1, max_tries=0, job_id=176, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:58.742177+00:00, queued_by_job_id=2, pid=42441[0m
[[34m2023-09-11T06:53:03.063+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-22T00:00:00+00:00, run_after=2023-06-23T00:00:00+00:00[0m
[[34m2023-09-11T06:53:03.090+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-21 00:00:00+00:00: scheduled__2023-06-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:58.674380+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:03.090+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-21 00:00:00+00:00, run_id=scheduled__2023-06-21T00:00:00+00:00, run_start_date=2023-09-11 06:52:58.693443+00:00, run_end_date=2023-09-11 06:53:03.090781+00:00, run_duration=4.397338, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-21 00:00:00+00:00, data_interval_end=2023-06-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:03.094+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-22T00:00:00+00:00, run_after=2023-06-23T00:00:00+00:00[0m
[[34m2023-09-11T06:53:03.767+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-23T00:00:00+00:00, run_after=2023-06-24T00:00:00+00:00[0m
[[34m2023-09-11T06:53:03.813+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:03.814+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:03.814+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:03.818+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:03.818+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:03.819+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:03.822+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:05.928+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:06.090+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:06.289+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:06.326+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:06.858+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:06.858+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:06.938+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:07.003+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:07.810+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:07.823+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:07.100482+00:00, run_end_date=2023-09-11 06:53:07.352590+00:00, run_duration=0.252108, state=success, executor_state=success, try_number=1, max_tries=0, job_id=177, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:03.815195+00:00, queued_by_job_id=2, pid=42451[0m
[[34m2023-09-11T06:53:08.095+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-24T00:00:00+00:00, run_after=2023-06-25T00:00:00+00:00[0m
[[34m2023-09-11T06:53:08.137+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-22 00:00:00+00:00: scheduled__2023-06-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:03.761094+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:08.138+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-22 00:00:00+00:00, run_id=scheduled__2023-06-22T00:00:00+00:00, run_start_date=2023-09-11 06:53:03.780863+00:00, run_end_date=2023-09-11 06:53:08.138182+00:00, run_duration=4.357319, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-22 00:00:00+00:00, data_interval_end=2023-06-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:08.141+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-23T00:00:00+00:00, run_after=2023-06-24T00:00:00+00:00[0m
[[34m2023-09-11T06:53:08.158+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:08.159+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:08.159+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:08.161+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:08.162+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:08.162+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:08.166+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:10.231+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:10.375+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:10.576+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:10.615+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:11.137+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:11.138+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:11.216+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:11.282+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:12.092+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:12.104+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:11.374525+00:00, run_end_date=2023-09-11 06:53:11.622657+00:00, run_duration=0.248132, state=success, executor_state=success, try_number=1, max_tries=0, job_id=178, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:08.159973+00:00, queued_by_job_id=2, pid=42460[0m
[[34m2023-09-11T06:53:12.344+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-24T00:00:00+00:00, run_after=2023-06-25T00:00:00+00:00[0m
[[34m2023-09-11T06:53:12.371+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-23 00:00:00+00:00: scheduled__2023-06-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:08.090052+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:12.371+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-23 00:00:00+00:00, run_id=scheduled__2023-06-23T00:00:00+00:00, run_start_date=2023-09-11 06:53:08.111110+00:00, run_end_date=2023-09-11 06:53:12.371614+00:00, run_duration=4.260504, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-23 00:00:00+00:00, data_interval_end=2023-06-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:12.375+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-24T00:00:00+00:00, run_after=2023-06-25T00:00:00+00:00[0m
[[34m2023-09-11T06:53:12.979+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-25T00:00:00+00:00, run_after=2023-06-26T00:00:00+00:00[0m
[[34m2023-09-11T06:53:13.028+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:13.028+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:13.028+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:13.030+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:13.031+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:13.032+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:13.035+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:15.107+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:15.258+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:15.458+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:15.493+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:16.020+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:16.021+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:16.102+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:16.170+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:16.994+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:17.007+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:16.262601+00:00, run_end_date=2023-09-11 06:53:16.503452+00:00, run_duration=0.240851, state=success, executor_state=success, try_number=1, max_tries=0, job_id=179, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:13.029238+00:00, queued_by_job_id=2, pid=42470[0m
[[34m2023-09-11T06:53:17.273+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-26T00:00:00+00:00, run_after=2023-06-27T00:00:00+00:00[0m
[[34m2023-09-11T06:53:17.311+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-24 00:00:00+00:00: scheduled__2023-06-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:12.974926+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:17.311+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-24 00:00:00+00:00, run_id=scheduled__2023-06-24T00:00:00+00:00, run_start_date=2023-09-11 06:53:12.993280+00:00, run_end_date=2023-09-11 06:53:17.311593+00:00, run_duration=4.318313, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-24 00:00:00+00:00, data_interval_end=2023-06-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:17.316+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-25T00:00:00+00:00, run_after=2023-06-26T00:00:00+00:00[0m
[[34m2023-09-11T06:53:17.335+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:17.335+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:17.335+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:17.338+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:17.338+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:17.339+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:17.342+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:19.439+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:19.601+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:19.811+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:19.851+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:20.390+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:20.391+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:20.470+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:20.538+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:21.346+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:21.358+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:20.633732+00:00, run_end_date=2023-09-11 06:53:20.878816+00:00, run_duration=0.245084, state=success, executor_state=success, try_number=1, max_tries=0, job_id=180, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:17.336535+00:00, queued_by_job_id=2, pid=42479[0m
[[34m2023-09-11T06:53:21.630+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-26T00:00:00+00:00, run_after=2023-06-27T00:00:00+00:00[0m
[[34m2023-09-11T06:53:21.658+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-25 00:00:00+00:00: scheduled__2023-06-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:17.267458+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:21.658+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-25 00:00:00+00:00, run_id=scheduled__2023-06-25T00:00:00+00:00, run_start_date=2023-09-11 06:53:17.287165+00:00, run_end_date=2023-09-11 06:53:21.658765+00:00, run_duration=4.3716, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-25 00:00:00+00:00, data_interval_end=2023-06-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:21.662+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-26T00:00:00+00:00, run_after=2023-06-27T00:00:00+00:00[0m
[[34m2023-09-11T06:53:22.274+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-27T00:00:00+00:00, run_after=2023-06-28T00:00:00+00:00[0m
[[34m2023-09-11T06:53:22.324+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:22.324+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:22.324+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:22.327+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:22.327+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:22.327+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:22.330+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:24.464+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:24.612+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:24.803+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:24.840+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:25.383+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:25.384+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:25.468+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:25.530+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:26.345+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:26.358+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:25.626531+00:00, run_end_date=2023-09-11 06:53:25.877401+00:00, run_duration=0.25087, state=success, executor_state=success, try_number=1, max_tries=0, job_id=181, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:22.325549+00:00, queued_by_job_id=2, pid=42489[0m
[[34m2023-09-11T06:53:26.640+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-28T00:00:00+00:00, run_after=2023-06-29T00:00:00+00:00[0m
[[34m2023-09-11T06:53:26.679+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-26 00:00:00+00:00: scheduled__2023-06-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:22.270094+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:26.679+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-26 00:00:00+00:00, run_id=scheduled__2023-06-26T00:00:00+00:00, run_start_date=2023-09-11 06:53:22.288850+00:00, run_end_date=2023-09-11 06:53:26.679687+00:00, run_duration=4.390837, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-26 00:00:00+00:00, data_interval_end=2023-06-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:26.685+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-27T00:00:00+00:00, run_after=2023-06-28T00:00:00+00:00[0m
[[34m2023-09-11T06:53:26.702+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:26.703+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:26.703+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:26.705+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:26.706+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:26.706+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:26.709+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:28.870+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:29.027+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:29.221+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:29.257+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:29.794+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:29.795+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:29.885+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:29.955+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:30.774+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:30.786+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:30.047378+00:00, run_end_date=2023-09-11 06:53:30.303707+00:00, run_duration=0.256329, state=success, executor_state=success, try_number=1, max_tries=0, job_id=182, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:26.704021+00:00, queued_by_job_id=2, pid=42498[0m
[[34m2023-09-11T06:53:31.045+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-28T00:00:00+00:00, run_after=2023-06-29T00:00:00+00:00[0m
[[34m2023-09-11T06:53:31.071+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-27 00:00:00+00:00: scheduled__2023-06-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:26.635222+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:31.071+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-27 00:00:00+00:00, run_id=scheduled__2023-06-27T00:00:00+00:00, run_start_date=2023-09-11 06:53:26.654258+00:00, run_end_date=2023-09-11 06:53:31.071662+00:00, run_duration=4.417404, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-27 00:00:00+00:00, data_interval_end=2023-06-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:31.075+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-28T00:00:00+00:00, run_after=2023-06-29T00:00:00+00:00[0m
[[34m2023-09-11T06:53:31.698+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-29T00:00:00+00:00, run_after=2023-06-30T00:00:00+00:00[0m
[[34m2023-09-11T06:53:31.749+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:31.749+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:31.750+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:31.753+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:31.754+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:31.754+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:31.757+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:33.860+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:34.026+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:34.222+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:34.258+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:34.779+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:34.781+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:34.858+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:34.928+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:35.765+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:35.776+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:35.024519+00:00, run_end_date=2023-09-11 06:53:35.274413+00:00, run_duration=0.249894, state=success, executor_state=success, try_number=1, max_tries=0, job_id=183, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:31.751082+00:00, queued_by_job_id=2, pid=42508[0m
[[34m2023-09-11T06:53:36.241+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-30T00:00:00+00:00, run_after=2023-07-01T00:00:00+00:00[0m
[[34m2023-09-11T06:53:36.285+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-28 00:00:00+00:00: scheduled__2023-06-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:31.692134+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:36.285+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-28 00:00:00+00:00, run_id=scheduled__2023-06-28T00:00:00+00:00, run_start_date=2023-09-11 06:53:31.712501+00:00, run_end_date=2023-09-11 06:53:36.285660+00:00, run_duration=4.573159, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-28 00:00:00+00:00, data_interval_end=2023-06-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:36.289+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-29T00:00:00+00:00, run_after=2023-06-30T00:00:00+00:00[0m
[[34m2023-09-11T06:53:36.310+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:36.310+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:36.310+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:36.312+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:36.313+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:36.314+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:36.317+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:38.423+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:38.583+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:38.777+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:38.815+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:39.362+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:39.362+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:39.451+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:39.516+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:40.335+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:40.346+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:39.615865+00:00, run_end_date=2023-09-11 06:53:39.867130+00:00, run_duration=0.251265, state=success, executor_state=success, try_number=1, max_tries=0, job_id=184, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:36.311347+00:00, queued_by_job_id=2, pid=42517[0m
[[34m2023-09-11T06:53:40.896+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-30T00:00:00+00:00, run_after=2023-07-01T00:00:00+00:00[0m
[[34m2023-09-11T06:53:40.927+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-29 00:00:00+00:00: scheduled__2023-06-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:36.236421+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:40.927+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-29 00:00:00+00:00, run_id=scheduled__2023-06-29T00:00:00+00:00, run_start_date=2023-09-11 06:53:36.257633+00:00, run_end_date=2023-09-11 06:53:40.927454+00:00, run_duration=4.669821, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-29 00:00:00+00:00, data_interval_end=2023-06-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:40.932+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-30T00:00:00+00:00, run_after=2023-07-01T00:00:00+00:00[0m
[[34m2023-09-11T06:53:41.724+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-01T00:00:00+00:00, run_after=2023-07-02T00:00:00+00:00[0m
[[34m2023-09-11T06:53:41.779+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:41.779+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:41.780+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:41.784+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:41.785+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:41.785+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:41.788+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:43.920+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:44.066+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:44.258+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:44.295+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:44.835+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:44.836+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:44.919+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:44.987+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:45.865+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:45.880+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:45.082346+00:00, run_end_date=2023-09-11 06:53:45.365642+00:00, run_duration=0.283296, state=success, executor_state=success, try_number=1, max_tries=0, job_id=185, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:41.782889+00:00, queued_by_job_id=2, pid=42527[0m
[[34m2023-09-11T06:53:46.366+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-02T00:00:00+00:00, run_after=2023-07-03T00:00:00+00:00[0m
[[34m2023-09-11T06:53:46.406+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-30 00:00:00+00:00: scheduled__2023-06-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:41.719318+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:46.406+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-30 00:00:00+00:00, run_id=scheduled__2023-06-30T00:00:00+00:00, run_start_date=2023-09-11 06:53:41.741378+00:00, run_end_date=2023-09-11 06:53:46.406694+00:00, run_duration=4.665316, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-30 00:00:00+00:00, data_interval_end=2023-07-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:46.410+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-01T00:00:00+00:00, run_after=2023-07-02T00:00:00+00:00[0m
[[34m2023-09-11T06:53:46.427+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:46.427+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:46.427+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:46.429+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:46.431+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:46.431+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:46.454+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:48.545+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:48.691+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:48.903+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:48.941+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:49.498+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:49.499+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:49.587+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:49.655+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:50.478+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:50.490+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:49.756028+00:00, run_end_date=2023-09-11 06:53:50.016009+00:00, run_duration=0.259981, state=success, executor_state=success, try_number=1, max_tries=0, job_id=186, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:46.428155+00:00, queued_by_job_id=2, pid=42536[0m
[[34m2023-09-11T06:53:50.967+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-02T00:00:00+00:00, run_after=2023-07-03T00:00:00+00:00[0m
[[34m2023-09-11T06:53:50.992+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-01 00:00:00+00:00: scheduled__2023-07-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:46.359183+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:50.993+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-01 00:00:00+00:00, run_id=scheduled__2023-07-01T00:00:00+00:00, run_start_date=2023-09-11 06:53:46.378935+00:00, run_end_date=2023-09-11 06:53:50.992977+00:00, run_duration=4.614042, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-01 00:00:00+00:00, data_interval_end=2023-07-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:50.997+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-02T00:00:00+00:00, run_after=2023-07-03T00:00:00+00:00[0m
[[34m2023-09-11T06:53:51.558+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-03T00:00:00+00:00, run_after=2023-07-04T00:00:00+00:00[0m
[[34m2023-09-11T06:53:51.610+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:51.610+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:51.610+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:51.613+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:51.614+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:51.614+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:51.635+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:53.772+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:53.925+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:54.119+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:54.156+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:54.685+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:54.685+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:54.760+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:54.828+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:55.651+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:55.661+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:54.926140+00:00, run_end_date=2023-09-11 06:53:55.172862+00:00, run_duration=0.246722, state=success, executor_state=success, try_number=1, max_tries=0, job_id=187, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:51.611437+00:00, queued_by_job_id=2, pid=42546[0m
[[34m2023-09-11T06:53:56.241+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-04T00:00:00+00:00, run_after=2023-07-05T00:00:00+00:00[0m
[[34m2023-09-11T06:53:56.285+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-02 00:00:00+00:00: scheduled__2023-07-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:51.554107+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:56.285+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-02 00:00:00+00:00, run_id=scheduled__2023-07-02T00:00:00+00:00, run_start_date=2023-09-11 06:53:51.573062+00:00, run_end_date=2023-09-11 06:53:56.285875+00:00, run_duration=4.712813, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-02 00:00:00+00:00, data_interval_end=2023-07-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:56.289+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-03T00:00:00+00:00, run_after=2023-07-04T00:00:00+00:00[0m
[[34m2023-09-11T06:53:56.306+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:56.307+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:56.307+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:56.309+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:56.310+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:56.310+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:56.314+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:58.433+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:58.582+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:58.782+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:58.818+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:59.359+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:59.359+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:59.449+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:59.515+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:00.413+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:00.425+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:59.632552+00:00, run_end_date=2023-09-11 06:53:59.883452+00:00, run_duration=0.2509, state=success, executor_state=success, try_number=1, max_tries=0, job_id=188, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:56.308148+00:00, queued_by_job_id=2, pid=42555[0m
[[34m2023-09-11T06:54:00.875+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-04T00:00:00+00:00, run_after=2023-07-05T00:00:00+00:00[0m
[[34m2023-09-11T06:54:00.901+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-03 00:00:00+00:00: scheduled__2023-07-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:56.236325+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:00.902+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-03 00:00:00+00:00, run_id=scheduled__2023-07-03T00:00:00+00:00, run_start_date=2023-09-11 06:53:56.257869+00:00, run_end_date=2023-09-11 06:54:00.902006+00:00, run_duration=4.644137, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-03 00:00:00+00:00, data_interval_end=2023-07-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:00.905+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-04T00:00:00+00:00, run_after=2023-07-05T00:00:00+00:00[0m
[[34m2023-09-11T06:54:02.294+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-05T00:00:00+00:00, run_after=2023-07-06T00:00:00+00:00[0m
[[34m2023-09-11T06:54:02.343+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:02.344+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:02.344+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:02.348+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:02.349+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:02.350+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:02.352+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:04.733+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:04.958+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:05.194+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:05.230+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:05.750+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:05.751+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:05.839+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:05.909+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:06.823+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:06.835+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:06.019628+00:00, run_end_date=2023-09-11 06:54:06.294999+00:00, run_duration=0.275371, state=success, executor_state=success, try_number=1, max_tries=0, job_id=189, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:02.345368+00:00, queued_by_job_id=2, pid=42565[0m
[[34m2023-09-11T06:54:07.299+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-06T00:00:00+00:00, run_after=2023-07-07T00:00:00+00:00[0m
[[34m2023-09-11T06:54:07.338+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-04 00:00:00+00:00: scheduled__2023-07-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:02.288842+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:07.338+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-04 00:00:00+00:00, run_id=scheduled__2023-07-04T00:00:00+00:00, run_start_date=2023-09-11 06:54:02.306828+00:00, run_end_date=2023-09-11 06:54:07.338338+00:00, run_duration=5.03151, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-04 00:00:00+00:00, data_interval_end=2023-07-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:07.342+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-05T00:00:00+00:00, run_after=2023-07-06T00:00:00+00:00[0m
[[34m2023-09-11T06:54:07.359+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:07.359+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:07.359+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:07.361+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:07.363+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:07.363+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:07.367+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:09.478+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:09.625+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:09.835+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:09.870+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:10.407+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:10.408+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:10.486+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:10.551+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:11.354+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:11.367+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:10.648024+00:00, run_end_date=2023-09-11 06:54:10.901319+00:00, run_duration=0.253295, state=success, executor_state=success, try_number=1, max_tries=0, job_id=190, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:07.360357+00:00, queued_by_job_id=2, pid=42574[0m
[[34m2023-09-11T06:54:11.880+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-06T00:00:00+00:00, run_after=2023-07-07T00:00:00+00:00[0m
[[34m2023-09-11T06:54:11.911+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-05 00:00:00+00:00: scheduled__2023-07-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:07.293528+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:11.913+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-05 00:00:00+00:00, run_id=scheduled__2023-07-05T00:00:00+00:00, run_start_date=2023-09-11 06:54:07.314011+00:00, run_end_date=2023-09-11 06:54:11.912537+00:00, run_duration=4.598526, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-05 00:00:00+00:00, data_interval_end=2023-07-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:11.917+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-06T00:00:00+00:00, run_after=2023-07-07T00:00:00+00:00[0m
[[34m2023-09-11T06:54:13.497+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-07T00:00:00+00:00, run_after=2023-07-08T00:00:00+00:00[0m
[[34m2023-09-11T06:54:13.545+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:13.546+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:13.547+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:13.549+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:13.550+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:13.550+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:13.554+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:15.660+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:15.807+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:16.001+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:16.036+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:16.587+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:16.588+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:16.671+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:16.741+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:17.559+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:17.571+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:16.839026+00:00, run_end_date=2023-09-11 06:54:17.098050+00:00, run_duration=0.259024, state=success, executor_state=success, try_number=1, max_tries=0, job_id=191, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:13.547939+00:00, queued_by_job_id=2, pid=42585[0m
[[34m2023-09-11T06:54:18.085+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-08T00:00:00+00:00, run_after=2023-07-09T00:00:00+00:00[0m
[[34m2023-09-11T06:54:18.131+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-06 00:00:00+00:00: scheduled__2023-07-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:13.491010+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:18.132+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-06 00:00:00+00:00, run_id=scheduled__2023-07-06T00:00:00+00:00, run_start_date=2023-09-11 06:54:13.510710+00:00, run_end_date=2023-09-11 06:54:18.132029+00:00, run_duration=4.621319, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-06 00:00:00+00:00, data_interval_end=2023-07-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:18.135+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-07T00:00:00+00:00, run_after=2023-07-08T00:00:00+00:00[0m
[[34m2023-09-11T06:54:18.154+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:18.154+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:18.155+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:18.158+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:18.159+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:18.160+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:18.167+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:20.350+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:20.494+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:20.692+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:20.735+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:21.280+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:21.281+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:21.357+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:21.422+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:22.226+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:22.238+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:21.515069+00:00, run_end_date=2023-09-11 06:54:21.769449+00:00, run_duration=0.25438, state=success, executor_state=success, try_number=1, max_tries=0, job_id=192, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:18.156628+00:00, queued_by_job_id=2, pid=42594[0m
[[34m2023-09-11T06:54:22.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-08T00:00:00+00:00, run_after=2023-07-09T00:00:00+00:00[0m
[[34m2023-09-11T06:54:22.701+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-07 00:00:00+00:00: scheduled__2023-07-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:18.080520+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:22.702+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-07 00:00:00+00:00, run_id=scheduled__2023-07-07T00:00:00+00:00, run_start_date=2023-09-11 06:54:18.100551+00:00, run_end_date=2023-09-11 06:54:22.702209+00:00, run_duration=4.601658, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-07 00:00:00+00:00, data_interval_end=2023-07-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:22.705+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-08T00:00:00+00:00, run_after=2023-07-09T00:00:00+00:00[0m
[[34m2023-09-11T06:54:24.248+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-09T00:00:00+00:00, run_after=2023-07-10T00:00:00+00:00[0m
[[34m2023-09-11T06:54:24.296+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:24.297+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:24.297+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:24.301+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:24.302+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:24.302+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:24.306+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:26.439+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:26.584+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:26.775+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:26.811+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:27.333+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:27.334+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:27.415+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:27.482+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:28.466+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:28.478+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:27.613790+00:00, run_end_date=2023-09-11 06:54:27.971178+00:00, run_duration=0.357388, state=success, executor_state=success, try_number=1, max_tries=0, job_id=193, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:24.298815+00:00, queued_by_job_id=2, pid=42604[0m
[[34m2023-09-11T06:54:28.648+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-10T00:00:00+00:00, run_after=2023-07-11T00:00:00+00:00[0m
[[34m2023-09-11T06:54:28.689+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-08 00:00:00+00:00: scheduled__2023-07-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:24.242705+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:28.689+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-08 00:00:00+00:00, run_id=scheduled__2023-07-08T00:00:00+00:00, run_start_date=2023-09-11 06:54:24.262263+00:00, run_end_date=2023-09-11 06:54:28.689636+00:00, run_duration=4.427373, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-08 00:00:00+00:00, data_interval_end=2023-07-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:28.693+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-09T00:00:00+00:00, run_after=2023-07-10T00:00:00+00:00[0m
[[34m2023-09-11T06:54:28.709+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:28.709+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:28.709+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:28.713+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:28.714+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:28.714+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:28.717+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:31.022+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:31.186+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:31.402+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:31.458+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:32.154+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:32.154+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:32.239+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:32.304+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:33.555+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:33.567+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:32.401298+00:00, run_end_date=2023-09-11 06:54:32.649178+00:00, run_duration=0.24788, state=success, executor_state=success, try_number=1, max_tries=0, job_id=194, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:28.710663+00:00, queued_by_job_id=2, pid=42613[0m
[[34m2023-09-11T06:54:34.618+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-10T00:00:00+00:00, run_after=2023-07-11T00:00:00+00:00[0m
[[34m2023-09-11T06:54:34.665+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-09 00:00:00+00:00: scheduled__2023-07-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:28.641240+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:34.667+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-09 00:00:00+00:00, run_id=scheduled__2023-07-09T00:00:00+00:00, run_start_date=2023-09-11 06:54:28.664843+00:00, run_end_date=2023-09-11 06:54:34.666824+00:00, run_duration=6.001981, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-09 00:00:00+00:00, data_interval_end=2023-07-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:34.672+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-10T00:00:00+00:00, run_after=2023-07-11T00:00:00+00:00[0m
[[34m2023-09-11T06:54:36.036+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-11T00:00:00+00:00, run_after=2023-07-12T00:00:00+00:00[0m
[[34m2023-09-11T06:54:36.102+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:36.103+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:36.103+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:36.105+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:36.106+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:36.106+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:36.109+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:38.205+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:38.385+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:38.689+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:38.732+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:39.489+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:39.490+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:39.607+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:39.738+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:40.815+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:40.832+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:39.871413+00:00, run_end_date=2023-09-11 06:54:40.248140+00:00, run_duration=0.376727, state=success, executor_state=success, try_number=1, max_tries=0, job_id=195, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:36.104169+00:00, queued_by_job_id=2, pid=42627[0m
[[34m2023-09-11T06:54:41.253+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-12T00:00:00+00:00, run_after=2023-07-13T00:00:00+00:00[0m
[[34m2023-09-11T06:54:41.292+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-10 00:00:00+00:00: scheduled__2023-07-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:36.031482+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:41.292+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-10 00:00:00+00:00, run_id=scheduled__2023-07-10T00:00:00+00:00, run_start_date=2023-09-11 06:54:36.050323+00:00, run_end_date=2023-09-11 06:54:41.292819+00:00, run_duration=5.242496, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-10 00:00:00+00:00, data_interval_end=2023-07-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:41.298+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-11T00:00:00+00:00, run_after=2023-07-12T00:00:00+00:00[0m
[[34m2023-09-11T06:54:41.315+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:41.317+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:41.317+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:41.319+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:41.320+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:41.320+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:41.323+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:43.689+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:43.843+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:44.072+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:44.109+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:44.691+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:44.692+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:44.772+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:44.836+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:45.750+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:45.761+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:44.949330+00:00, run_end_date=2023-09-11 06:54:45.238007+00:00, run_duration=0.288677, state=success, executor_state=success, try_number=1, max_tries=0, job_id=196, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:41.318364+00:00, queued_by_job_id=2, pid=42636[0m
[[34m2023-09-11T06:54:46.031+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-12T00:00:00+00:00, run_after=2023-07-13T00:00:00+00:00[0m
[[34m2023-09-11T06:54:46.057+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-11 00:00:00+00:00: scheduled__2023-07-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:41.245852+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:46.058+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-11 00:00:00+00:00, run_id=scheduled__2023-07-11T00:00:00+00:00, run_start_date=2023-09-11 06:54:41.267260+00:00, run_end_date=2023-09-11 06:54:46.058030+00:00, run_duration=4.79077, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-11 00:00:00+00:00, data_interval_end=2023-07-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:46.063+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-12T00:00:00+00:00, run_after=2023-07-13T00:00:00+00:00[0m
[[34m2023-09-11T06:54:47.436+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-13T00:00:00+00:00, run_after=2023-07-14T00:00:00+00:00[0m
[[34m2023-09-11T06:54:47.488+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:47.489+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:47.489+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:47.491+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:47.492+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:47.492+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:47.496+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:49.803+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:49.984+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:50.190+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:50.226+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:50.783+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:50.784+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:50.905+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:51.001+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:51.819+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:51.831+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:51.092809+00:00, run_end_date=2023-09-11 06:54:51.371963+00:00, run_duration=0.279154, state=success, executor_state=success, try_number=1, max_tries=0, job_id=197, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:47.490254+00:00, queued_by_job_id=2, pid=42646[0m
[[34m2023-09-11T06:54:52.000+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-14T00:00:00+00:00, run_after=2023-07-15T00:00:00+00:00[0m
[[34m2023-09-11T06:54:52.038+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-12 00:00:00+00:00: scheduled__2023-07-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:47.431722+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:52.038+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-12 00:00:00+00:00, run_id=scheduled__2023-07-12T00:00:00+00:00, run_start_date=2023-09-11 06:54:47.450896+00:00, run_end_date=2023-09-11 06:54:52.038704+00:00, run_duration=4.587808, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-12 00:00:00+00:00, data_interval_end=2023-07-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:52.042+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-13T00:00:00+00:00, run_after=2023-07-14T00:00:00+00:00[0m
[[34m2023-09-11T06:54:52.058+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:52.058+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:52.058+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:52.062+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:52.063+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:52.063+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:52.066+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:54.152+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:54.277+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:54.453+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:54.487+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:54.970+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:54.971+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:55.039+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:55.097+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:55.858+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:55.868+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:55.192531+00:00, run_end_date=2023-09-11 06:54:55.427636+00:00, run_duration=0.235105, state=success, executor_state=success, try_number=1, max_tries=0, job_id=198, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:52.059527+00:00, queued_by_job_id=2, pid=42655[0m
[[34m2023-09-11T06:54:56.156+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-14T00:00:00+00:00, run_after=2023-07-15T00:00:00+00:00[0m
[[34m2023-09-11T06:54:56.191+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-13 00:00:00+00:00: scheduled__2023-07-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:51.994203+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:56.191+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-13 00:00:00+00:00, run_id=scheduled__2023-07-13T00:00:00+00:00, run_start_date=2023-09-11 06:54:52.014384+00:00, run_end_date=2023-09-11 06:54:56.191606+00:00, run_duration=4.177222, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-13 00:00:00+00:00, data_interval_end=2023-07-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:56.198+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-14T00:00:00+00:00, run_after=2023-07-15T00:00:00+00:00[0m
[[34m2023-09-11T06:54:57.053+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-15T00:00:00+00:00, run_after=2023-07-16T00:00:00+00:00[0m
[[34m2023-09-11T06:54:57.101+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:57.102+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:57.102+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:57.104+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:57.105+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:57.105+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:57.107+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:59.205+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:59.347+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:59.539+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:59.574+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:00.098+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:00.099+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:00.182+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:00.245+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:01.069+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:01.081+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:00.343275+00:00, run_end_date=2023-09-11 06:55:00.586382+00:00, run_duration=0.243107, state=success, executor_state=success, try_number=1, max_tries=0, job_id=199, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:57.103006+00:00, queued_by_job_id=2, pid=42665[0m
[[34m2023-09-11T06:55:01.106+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:55:01.259+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-16T00:00:00+00:00, run_after=2023-07-17T00:00:00+00:00[0m
[[34m2023-09-11T06:55:01.306+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-14 00:00:00+00:00: scheduled__2023-07-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:57.045636+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:01.306+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-14 00:00:00+00:00, run_id=scheduled__2023-07-14T00:00:00+00:00, run_start_date=2023-09-11 06:54:57.067179+00:00, run_end_date=2023-09-11 06:55:01.306496+00:00, run_duration=4.239317, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-14 00:00:00+00:00, data_interval_end=2023-07-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:01.310+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-15T00:00:00+00:00, run_after=2023-07-16T00:00:00+00:00[0m
[[34m2023-09-11T06:55:01.326+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:01.327+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:01.327+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:01.330+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:01.331+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:01.331+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:01.334+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:03.435+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:03.633+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:03.906+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:03.944+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:04.545+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:04.546+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:04.636+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:04.704+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:05.667+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:05.681+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:04.804175+00:00, run_end_date=2023-09-11 06:55:05.078817+00:00, run_duration=0.274642, state=success, executor_state=success, try_number=1, max_tries=0, job_id=200, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:01.329273+00:00, queued_by_job_id=2, pid=42674[0m
[[34m2023-09-11T06:55:05.847+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-16T00:00:00+00:00, run_after=2023-07-17T00:00:00+00:00[0m
[[34m2023-09-11T06:55:05.880+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-15 00:00:00+00:00: scheduled__2023-07-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:01.254254+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:05.881+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-15 00:00:00+00:00, run_id=scheduled__2023-07-15T00:00:00+00:00, run_start_date=2023-09-11 06:55:01.281975+00:00, run_end_date=2023-09-11 06:55:05.881119+00:00, run_duration=4.599144, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-15 00:00:00+00:00, data_interval_end=2023-07-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:05.884+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-16T00:00:00+00:00, run_after=2023-07-17T00:00:00+00:00[0m
[[34m2023-09-11T06:55:07.010+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-17T00:00:00+00:00, run_after=2023-07-18T00:00:00+00:00[0m
[[34m2023-09-11T06:55:07.053+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:07.054+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:07.054+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:07.056+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:07.057+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:07.057+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:07.060+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:09.742+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:09.899+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:10.231+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:10.283+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:10.863+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:10.864+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:10.964+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:11.040+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:11.921+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:11.932+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:11.147530+00:00, run_end_date=2023-09-11 06:55:11.454943+00:00, run_duration=0.307413, state=success, executor_state=success, try_number=1, max_tries=0, job_id=201, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:07.055183+00:00, queued_by_job_id=2, pid=42684[0m
[[34m2023-09-11T06:55:12.201+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-18T00:00:00+00:00, run_after=2023-07-19T00:00:00+00:00[0m
[[34m2023-09-11T06:55:12.241+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-16 00:00:00+00:00: scheduled__2023-07-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:07.005075+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:12.241+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-16 00:00:00+00:00, run_id=scheduled__2023-07-16T00:00:00+00:00, run_start_date=2023-09-11 06:55:07.022617+00:00, run_end_date=2023-09-11 06:55:12.241411+00:00, run_duration=5.218794, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-16 00:00:00+00:00, data_interval_end=2023-07-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:12.246+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-17T00:00:00+00:00, run_after=2023-07-18T00:00:00+00:00[0m
[[34m2023-09-11T06:55:12.262+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:12.263+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:12.263+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:12.265+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:12.266+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:12.266+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:12.269+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:14.338+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:14.479+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:14.657+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:14.690+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:15.206+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:15.206+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:15.283+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:15.346+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:16.189+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:16.201+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:15.440375+00:00, run_end_date=2023-09-11 06:55:15.689680+00:00, run_duration=0.249305, state=success, executor_state=success, try_number=1, max_tries=0, job_id=202, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:12.264074+00:00, queued_by_job_id=2, pid=42693[0m
[[34m2023-09-11T06:55:16.449+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-18T00:00:00+00:00, run_after=2023-07-19T00:00:00+00:00[0m
[[34m2023-09-11T06:55:16.475+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-17 00:00:00+00:00: scheduled__2023-07-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:12.196003+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:16.476+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-17 00:00:00+00:00, run_id=scheduled__2023-07-17T00:00:00+00:00, run_start_date=2023-09-11 06:55:12.216049+00:00, run_end_date=2023-09-11 06:55:16.476678+00:00, run_duration=4.260629, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-17 00:00:00+00:00, data_interval_end=2023-07-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:16.482+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-18T00:00:00+00:00, run_after=2023-07-19T00:00:00+00:00[0m
[[34m2023-09-11T06:55:17.218+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-19T00:00:00+00:00, run_after=2023-07-20T00:00:00+00:00[0m
[[34m2023-09-11T06:55:17.266+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:17.266+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:17.267+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:17.269+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:17.269+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:17.270+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:17.273+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:19.329+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:19.464+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:19.649+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:19.683+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:20.178+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:20.178+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:20.255+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:20.318+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:21.057+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:21.068+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:20.407619+00:00, run_end_date=2023-09-11 06:55:20.642279+00:00, run_duration=0.23466, state=success, executor_state=success, try_number=1, max_tries=0, job_id=203, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:17.267791+00:00, queued_by_job_id=2, pid=42703[0m
[[34m2023-09-11T06:55:21.370+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-20T00:00:00+00:00, run_after=2023-07-21T00:00:00+00:00[0m
[[34m2023-09-11T06:55:21.408+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-18 00:00:00+00:00: scheduled__2023-07-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:17.213662+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:21.409+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-18 00:00:00+00:00, run_id=scheduled__2023-07-18T00:00:00+00:00, run_start_date=2023-09-11 06:55:17.232115+00:00, run_end_date=2023-09-11 06:55:21.409374+00:00, run_duration=4.177259, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-18 00:00:00+00:00, data_interval_end=2023-07-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:21.413+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-19T00:00:00+00:00, run_after=2023-07-20T00:00:00+00:00[0m
[[34m2023-09-11T06:55:21.430+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:21.430+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:21.431+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:21.433+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:21.434+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:21.434+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:21.436+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:23.558+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:23.699+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:23.879+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:23.915+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:24.410+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:24.410+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:24.483+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:24.545+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:25.351+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:25.362+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:24.637263+00:00, run_end_date=2023-09-11 06:55:24.882040+00:00, run_duration=0.244777, state=success, executor_state=success, try_number=1, max_tries=0, job_id=204, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:21.431948+00:00, queued_by_job_id=2, pid=42712[0m
[[34m2023-09-11T06:55:25.633+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-20T00:00:00+00:00, run_after=2023-07-21T00:00:00+00:00[0m
[[34m2023-09-11T06:55:25.657+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-19 00:00:00+00:00: scheduled__2023-07-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:21.365018+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:25.657+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-19 00:00:00+00:00, run_id=scheduled__2023-07-19T00:00:00+00:00, run_start_date=2023-09-11 06:55:21.384868+00:00, run_end_date=2023-09-11 06:55:25.657474+00:00, run_duration=4.272606, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-19 00:00:00+00:00, data_interval_end=2023-07-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:25.662+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-20T00:00:00+00:00, run_after=2023-07-21T00:00:00+00:00[0m
[[34m2023-09-11T06:55:26.339+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-21T00:00:00+00:00, run_after=2023-07-22T00:00:00+00:00[0m
[[34m2023-09-11T06:55:26.387+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:26.387+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:26.388+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:26.390+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:26.391+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:26.391+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:26.394+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:28.366+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:28.502+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:28.680+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:28.714+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:29.238+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:29.238+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:29.313+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:29.375+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:30.136+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:30.147+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:29.466422+00:00, run_end_date=2023-09-11 06:55:29.700224+00:00, run_duration=0.233802, state=success, executor_state=success, try_number=1, max_tries=0, job_id=205, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:26.388950+00:00, queued_by_job_id=2, pid=42720[0m
[[34m2023-09-11T06:55:30.418+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-22T00:00:00+00:00, run_after=2023-07-23T00:00:00+00:00[0m
[[34m2023-09-11T06:55:30.456+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-20 00:00:00+00:00: scheduled__2023-07-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:26.334644+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:30.456+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-20 00:00:00+00:00, run_id=scheduled__2023-07-20T00:00:00+00:00, run_start_date=2023-09-11 06:55:26.352150+00:00, run_end_date=2023-09-11 06:55:30.456412+00:00, run_duration=4.104262, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-20 00:00:00+00:00, data_interval_end=2023-07-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:30.460+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-21T00:00:00+00:00, run_after=2023-07-22T00:00:00+00:00[0m
[[34m2023-09-11T06:55:30.477+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:30.478+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:30.478+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:30.480+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:30.481+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:30.481+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:30.484+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:32.490+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:32.622+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:32.795+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:32.830+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:33.313+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:33.314+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:33.387+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:33.451+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:34.218+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:34.229+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:33.539189+00:00, run_end_date=2023-09-11 06:55:33.815394+00:00, run_duration=0.276205, state=success, executor_state=success, try_number=1, max_tries=0, job_id=206, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:30.479015+00:00, queued_by_job_id=2, pid=42729[0m
[[34m2023-09-11T06:55:34.482+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-22T00:00:00+00:00, run_after=2023-07-23T00:00:00+00:00[0m
[[34m2023-09-11T06:55:34.505+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-21 00:00:00+00:00: scheduled__2023-07-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:30.413049+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:34.506+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-21 00:00:00+00:00, run_id=scheduled__2023-07-21T00:00:00+00:00, run_start_date=2023-09-11 06:55:30.431894+00:00, run_end_date=2023-09-11 06:55:34.506187+00:00, run_duration=4.074293, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-21 00:00:00+00:00, data_interval_end=2023-07-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:34.510+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-22T00:00:00+00:00, run_after=2023-07-23T00:00:00+00:00[0m
[[34m2023-09-11T06:55:35.405+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-23T00:00:00+00:00, run_after=2023-07-24T00:00:00+00:00[0m
[[34m2023-09-11T06:55:35.451+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:35.451+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:35.451+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:35.453+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:35.454+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:35.454+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:35.458+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:37.418+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:37.556+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:37.742+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:37.777+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:38.277+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:38.278+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:38.350+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:38.412+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:39.211+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:39.222+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:38.502151+00:00, run_end_date=2023-09-11 06:55:38.764470+00:00, run_duration=0.262319, state=success, executor_state=success, try_number=1, max_tries=0, job_id=207, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:35.452384+00:00, queued_by_job_id=2, pid=42737[0m
[[34m2023-09-11T06:55:39.491+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-24T00:00:00+00:00, run_after=2023-07-25T00:00:00+00:00[0m
[[34m2023-09-11T06:55:39.529+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-22 00:00:00+00:00: scheduled__2023-07-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:35.400529+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:39.529+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-22 00:00:00+00:00, run_id=scheduled__2023-07-22T00:00:00+00:00, run_start_date=2023-09-11 06:55:35.417858+00:00, run_end_date=2023-09-11 06:55:39.529524+00:00, run_duration=4.111666, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-22 00:00:00+00:00, data_interval_end=2023-07-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:39.533+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-23T00:00:00+00:00, run_after=2023-07-24T00:00:00+00:00[0m
[[34m2023-09-11T06:55:39.550+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:39.550+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:39.550+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:39.553+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:39.554+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:39.554+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:39.557+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:41.614+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:41.765+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:41.948+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:41.983+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:42.474+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:42.475+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:42.549+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:42.609+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:43.351+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:43.363+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:42.704420+00:00, run_end_date=2023-09-11 06:55:42.936300+00:00, run_duration=0.23188, state=success, executor_state=success, try_number=1, max_tries=0, job_id=208, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:39.551507+00:00, queued_by_job_id=2, pid=42746[0m
[[34m2023-09-11T06:55:43.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-24T00:00:00+00:00, run_after=2023-07-25T00:00:00+00:00[0m
[[34m2023-09-11T06:55:43.698+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-23 00:00:00+00:00: scheduled__2023-07-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:39.485936+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:43.699+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-23 00:00:00+00:00, run_id=scheduled__2023-07-23T00:00:00+00:00, run_start_date=2023-09-11 06:55:39.504182+00:00, run_end_date=2023-09-11 06:55:43.699310+00:00, run_duration=4.195128, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-23 00:00:00+00:00, data_interval_end=2023-07-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:43.703+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-24T00:00:00+00:00, run_after=2023-07-25T00:00:00+00:00[0m
[[34m2023-09-11T06:55:44.486+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-25T00:00:00+00:00, run_after=2023-07-26T00:00:00+00:00[0m
[[34m2023-09-11T06:55:44.535+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:44.535+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:44.536+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:44.538+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:44.538+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:44.538+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:44.542+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:46.470+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:46.601+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:46.778+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:46.814+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:47.310+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:47.310+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:47.382+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:47.443+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:48.267+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:48.278+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:47.532567+00:00, run_end_date=2023-09-11 06:55:47.839668+00:00, run_duration=0.307101, state=success, executor_state=success, try_number=1, max_tries=0, job_id=209, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:44.536897+00:00, queued_by_job_id=2, pid=42756[0m
[[34m2023-09-11T06:55:48.538+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-26T00:00:00+00:00, run_after=2023-07-27T00:00:00+00:00[0m
[[34m2023-09-11T06:55:48.576+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-24 00:00:00+00:00: scheduled__2023-07-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:44.481968+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:48.577+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-24 00:00:00+00:00, run_id=scheduled__2023-07-24T00:00:00+00:00, run_start_date=2023-09-11 06:55:44.501676+00:00, run_end_date=2023-09-11 06:55:48.577145+00:00, run_duration=4.075469, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-24 00:00:00+00:00, data_interval_end=2023-07-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:48.580+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-25T00:00:00+00:00, run_after=2023-07-26T00:00:00+00:00[0m
[[34m2023-09-11T06:55:48.596+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:48.597+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:48.597+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:48.599+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:48.600+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:48.600+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:48.603+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:50.616+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:50.754+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:50.932+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:50.968+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:51.467+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:51.468+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:51.546+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:51.611+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:52.394+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:52.405+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:51.703033+00:00, run_end_date=2023-09-11 06:55:51.940769+00:00, run_duration=0.237736, state=success, executor_state=success, try_number=1, max_tries=0, job_id=210, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:48.598049+00:00, queued_by_job_id=2, pid=42765[0m
[[34m2023-09-11T06:55:52.647+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-26T00:00:00+00:00, run_after=2023-07-27T00:00:00+00:00[0m
[[34m2023-09-11T06:55:52.671+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-25 00:00:00+00:00: scheduled__2023-07-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:48.533156+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:52.671+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-25 00:00:00+00:00, run_id=scheduled__2023-07-25T00:00:00+00:00, run_start_date=2023-09-11 06:55:48.550761+00:00, run_end_date=2023-09-11 06:55:52.671657+00:00, run_duration=4.120896, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-25 00:00:00+00:00, data_interval_end=2023-07-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:52.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-26T00:00:00+00:00, run_after=2023-07-27T00:00:00+00:00[0m
[[34m2023-09-11T06:55:54.308+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-27T00:00:00+00:00, run_after=2023-07-28T00:00:00+00:00[0m
[[34m2023-09-11T06:55:54.355+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:54.355+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:54.355+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:54.358+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:54.359+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:54.359+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:54.362+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:56.320+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:56.458+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:56.644+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:56.678+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:57.214+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:57.214+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:57.291+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:57.358+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:58.305+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:58.317+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:57.449518+00:00, run_end_date=2023-09-11 06:55:57.805579+00:00, run_duration=0.356061, state=success, executor_state=success, try_number=1, max_tries=0, job_id=211, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:54.356554+00:00, queued_by_job_id=2, pid=42775[0m
[[34m2023-09-11T06:55:58.596+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-28T00:00:00+00:00, run_after=2023-07-29T00:00:00+00:00[0m
[[34m2023-09-11T06:55:58.649+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-26 00:00:00+00:00: scheduled__2023-07-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:54.303787+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:58.650+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-26 00:00:00+00:00, run_id=scheduled__2023-07-26T00:00:00+00:00, run_start_date=2023-09-11 06:55:54.321707+00:00, run_end_date=2023-09-11 06:55:58.649870+00:00, run_duration=4.328163, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-26 00:00:00+00:00, data_interval_end=2023-07-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:58.657+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-27T00:00:00+00:00, run_after=2023-07-28T00:00:00+00:00[0m
[[34m2023-09-11T06:55:58.686+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:58.686+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:58.687+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:58.690+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:58.692+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:58.694+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:58.700+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:01.013+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:01.149+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:01.332+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:01.366+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:01.885+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:01.886+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:01.966+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:02.022+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:02.787+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:02.798+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:02.110797+00:00, run_end_date=2023-09-11 06:56:02.338804+00:00, run_duration=0.228007, state=success, executor_state=success, try_number=1, max_tries=0, job_id=212, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:58.688572+00:00, queued_by_job_id=2, pid=42784[0m
[[34m2023-09-11T06:56:03.052+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-28T00:00:00+00:00, run_after=2023-07-29T00:00:00+00:00[0m
[[34m2023-09-11T06:56:03.077+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-27 00:00:00+00:00: scheduled__2023-07-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:58.588156+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:03.077+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-27 00:00:00+00:00, run_id=scheduled__2023-07-27T00:00:00+00:00, run_start_date=2023-09-11 06:55:58.614558+00:00, run_end_date=2023-09-11 06:56:03.077572+00:00, run_duration=4.463014, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-27 00:00:00+00:00, data_interval_end=2023-07-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:03.081+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-28T00:00:00+00:00, run_after=2023-07-29T00:00:00+00:00[0m
[[34m2023-09-11T06:56:03.700+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-29T00:00:00+00:00, run_after=2023-07-30T00:00:00+00:00[0m
[[34m2023-09-11T06:56:03.777+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:03.778+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:03.778+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:03.781+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:03.782+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:03.783+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:03.786+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:05.836+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:05.980+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:06.181+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:06.214+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:06.725+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:06.726+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:06.799+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:06.861+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:07.628+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:07.638+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:06.949700+00:00, run_end_date=2023-09-11 06:56:07.181916+00:00, run_duration=0.232216, state=success, executor_state=success, try_number=1, max_tries=0, job_id=213, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:03.779837+00:00, queued_by_job_id=2, pid=42794[0m
[[34m2023-09-11T06:56:07.911+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-30T00:00:00+00:00, run_after=2023-07-31T00:00:00+00:00[0m
[[34m2023-09-11T06:56:07.947+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-28 00:00:00+00:00: scheduled__2023-07-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:03.695261+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:07.948+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-28 00:00:00+00:00, run_id=scheduled__2023-07-28T00:00:00+00:00, run_start_date=2023-09-11 06:56:03.714862+00:00, run_end_date=2023-09-11 06:56:07.948099+00:00, run_duration=4.233237, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-28 00:00:00+00:00, data_interval_end=2023-07-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:07.951+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-29T00:00:00+00:00, run_after=2023-07-30T00:00:00+00:00[0m
[[34m2023-09-11T06:56:07.967+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:07.967+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:07.968+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:07.970+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:07.970+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:07.971+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:07.974+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:09.928+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:10.078+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:10.286+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:10.326+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:10.796+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:10.797+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:10.874+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:10.941+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:12.212+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:12.224+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:11.028068+00:00, run_end_date=2023-09-11 06:56:11.764967+00:00, run_duration=0.736899, state=success, executor_state=success, try_number=1, max_tries=0, job_id=214, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:07.968788+00:00, queued_by_job_id=2, pid=42803[0m
[[34m2023-09-11T06:56:12.383+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-30T00:00:00+00:00, run_after=2023-07-31T00:00:00+00:00[0m
[[34m2023-09-11T06:56:12.409+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-29 00:00:00+00:00: scheduled__2023-07-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:07.905530+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:12.409+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-29 00:00:00+00:00, run_id=scheduled__2023-07-29T00:00:00+00:00, run_start_date=2023-09-11 06:56:07.924346+00:00, run_end_date=2023-09-11 06:56:12.409696+00:00, run_duration=4.48535, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-29 00:00:00+00:00, data_interval_end=2023-07-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:12.413+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-30T00:00:00+00:00, run_after=2023-07-31T00:00:00+00:00[0m
[[34m2023-09-11T06:56:12.955+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-31T00:00:00+00:00, run_after=2023-08-01T00:00:00+00:00[0m
[[34m2023-09-11T06:56:13.006+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:13.006+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:13.006+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:13.009+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:13.009+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:13.010+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:13.012+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:14.881+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:15.009+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:15.182+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:15.216+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:15.698+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:15.699+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:15.771+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:15.833+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:16.586+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:16.596+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:15.945284+00:00, run_end_date=2023-09-11 06:56:16.174434+00:00, run_duration=0.22915, state=success, executor_state=success, try_number=1, max_tries=0, job_id=215, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:13.007757+00:00, queued_by_job_id=2, pid=42813[0m
[[34m2023-09-11T06:56:16.875+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-01T00:00:00+00:00, run_after=2023-08-02T00:00:00+00:00[0m
[[34m2023-09-11T06:56:16.929+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-30 00:00:00+00:00: scheduled__2023-07-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:12.950316+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:16.930+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-30 00:00:00+00:00, run_id=scheduled__2023-07-30T00:00:00+00:00, run_start_date=2023-09-11 06:56:12.970854+00:00, run_end_date=2023-09-11 06:56:16.929962+00:00, run_duration=3.959108, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-30 00:00:00+00:00, data_interval_end=2023-07-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:16.933+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-31T00:00:00+00:00, run_after=2023-08-01T00:00:00+00:00[0m
[[34m2023-09-11T06:56:16.948+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:16.948+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:16.948+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:16.951+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:16.951+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:16.951+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:16.954+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:19.031+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:19.189+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:19.538+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:19.578+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:20.233+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:20.233+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:20.337+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:20.422+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:21.396+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:21.408+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:20.514137+00:00, run_end_date=2023-09-11 06:56:20.848814+00:00, run_duration=0.334677, state=success, executor_state=success, try_number=1, max_tries=0, job_id=216, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:16.949565+00:00, queued_by_job_id=2, pid=42822[0m
[[34m2023-09-11T06:56:21.748+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-01T00:00:00+00:00, run_after=2023-08-02T00:00:00+00:00[0m
[[34m2023-09-11T06:56:21.771+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-31 00:00:00+00:00: scheduled__2023-07-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:16.870609+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:21.772+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-31 00:00:00+00:00, run_id=scheduled__2023-07-31T00:00:00+00:00, run_start_date=2023-09-11 06:56:16.889503+00:00, run_end_date=2023-09-11 06:56:21.772450+00:00, run_duration=4.882947, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-31 00:00:00+00:00, data_interval_end=2023-08-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:21.779+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-01T00:00:00+00:00, run_after=2023-08-02T00:00:00+00:00[0m
[[34m2023-09-11T06:56:22.758+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-02T00:00:00+00:00, run_after=2023-08-03T00:00:00+00:00[0m
[[34m2023-09-11T06:56:23.078+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:23.078+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:23.078+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:23.082+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:23.083+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:23.083+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:23.089+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:26.032+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:26.199+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:26.843+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:26.979+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:27.970+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:27.970+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:28.070+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:28.163+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:29.105+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:29.120+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:28.288214+00:00, run_end_date=2023-09-11 06:56:28.610638+00:00, run_duration=0.322424, state=success, executor_state=success, try_number=1, max_tries=0, job_id=217, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:23.080116+00:00, queued_by_job_id=2, pid=42832[0m
[[34m2023-09-11T06:56:29.403+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-03T00:00:00+00:00, run_after=2023-08-04T00:00:00+00:00[0m
[[34m2023-09-11T06:56:29.439+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-01 00:00:00+00:00: scheduled__2023-08-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:22.749624+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:29.440+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-01 00:00:00+00:00, run_id=scheduled__2023-08-01T00:00:00+00:00, run_start_date=2023-09-11 06:56:22.995080+00:00, run_end_date=2023-09-11 06:56:29.440045+00:00, run_duration=6.444965, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-01 00:00:00+00:00, data_interval_end=2023-08-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:29.444+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-02T00:00:00+00:00, run_after=2023-08-03T00:00:00+00:00[0m
[[34m2023-09-11T06:56:29.460+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:29.461+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:29.461+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:29.463+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:29.463+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:29.464+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:29.466+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:31.343+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:31.478+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:31.658+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:31.702+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:32.181+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:32.182+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:32.256+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:32.322+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:33.065+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:33.076+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:32.408941+00:00, run_end_date=2023-09-11 06:56:32.676321+00:00, run_duration=0.26738, state=success, executor_state=success, try_number=1, max_tries=0, job_id=218, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:29.462035+00:00, queued_by_job_id=2, pid=42841[0m
[[34m2023-09-11T06:56:33.319+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-03T00:00:00+00:00, run_after=2023-08-04T00:00:00+00:00[0m
[[34m2023-09-11T06:56:33.343+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-02 00:00:00+00:00: scheduled__2023-08-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:29.397859+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:33.343+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-02 00:00:00+00:00, run_id=scheduled__2023-08-02T00:00:00+00:00, run_start_date=2023-09-11 06:56:29.415989+00:00, run_end_date=2023-09-11 06:56:33.343514+00:00, run_duration=3.927525, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-02 00:00:00+00:00, data_interval_end=2023-08-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:33.347+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-03T00:00:00+00:00, run_after=2023-08-04T00:00:00+00:00[0m
[[34m2023-09-11T06:56:34.482+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-04T00:00:00+00:00, run_after=2023-08-05T00:00:00+00:00[0m
[[34m2023-09-11T06:56:34.526+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:34.527+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:34.527+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:34.529+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:34.530+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:34.530+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:34.533+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:36.703+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:36.861+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:37.060+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:37.107+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:37.742+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:37.743+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:37.947+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:38.053+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:38.931+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:38.947+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:38.156239+00:00, run_end_date=2023-09-11 06:56:38.425193+00:00, run_duration=0.268954, state=success, executor_state=success, try_number=1, max_tries=0, job_id=219, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:34.528069+00:00, queued_by_job_id=2, pid=42850[0m
[[34m2023-09-11T06:56:39.270+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-05T00:00:00+00:00, run_after=2023-08-06T00:00:00+00:00[0m
[[34m2023-09-11T06:56:39.316+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-03 00:00:00+00:00: scheduled__2023-08-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:34.478241+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:39.317+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-03 00:00:00+00:00, run_id=scheduled__2023-08-03T00:00:00+00:00, run_start_date=2023-09-11 06:56:34.495331+00:00, run_end_date=2023-09-11 06:56:39.317398+00:00, run_duration=4.822067, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-03 00:00:00+00:00, data_interval_end=2023-08-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:39.321+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-04T00:00:00+00:00, run_after=2023-08-05T00:00:00+00:00[0m
[[34m2023-09-11T06:56:39.338+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:39.338+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:39.339+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:39.342+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:39.343+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:39.343+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:39.346+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:41.896+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:42.132+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:42.370+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:42.438+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:43.060+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:43.061+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:43.192+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:43.257+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:44.193+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:44.205+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:43.355426+00:00, run_end_date=2023-09-11 06:56:43.660484+00:00, run_duration=0.305058, state=success, executor_state=success, try_number=1, max_tries=0, job_id=220, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:39.341103+00:00, queued_by_job_id=2, pid=42859[0m
[[34m2023-09-11T06:56:44.631+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-05T00:00:00+00:00, run_after=2023-08-06T00:00:00+00:00[0m
[[34m2023-09-11T06:56:44.667+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-04 00:00:00+00:00: scheduled__2023-08-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:39.263765+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:44.668+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-04 00:00:00+00:00, run_id=scheduled__2023-08-04T00:00:00+00:00, run_start_date=2023-09-11 06:56:39.287544+00:00, run_end_date=2023-09-11 06:56:44.668093+00:00, run_duration=5.380549, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-04 00:00:00+00:00, data_interval_end=2023-08-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:44.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-05T00:00:00+00:00, run_after=2023-08-06T00:00:00+00:00[0m
[[34m2023-09-11T06:56:45.955+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-06T00:00:00+00:00, run_after=2023-08-07T00:00:00+00:00[0m
[[34m2023-09-11T06:56:46.003+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:46.004+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:46.004+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:46.008+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:46.009+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:46.010+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:46.012+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:48.419+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:48.553+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:48.744+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:48.780+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:49.310+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:49.310+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:49.414+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:49.472+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:50.199+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:50.212+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:49.568042+00:00, run_end_date=2023-09-11 06:56:49.800128+00:00, run_duration=0.232086, state=success, executor_state=success, try_number=1, max_tries=0, job_id=221, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:46.006173+00:00, queued_by_job_id=2, pid=42871[0m
[[34m2023-09-11T06:56:50.556+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-07T00:00:00+00:00, run_after=2023-08-08T00:00:00+00:00[0m
[[34m2023-09-11T06:56:50.602+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-05 00:00:00+00:00: scheduled__2023-08-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:45.950544+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:50.602+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-05 00:00:00+00:00, run_id=scheduled__2023-08-05T00:00:00+00:00, run_start_date=2023-09-11 06:56:45.968014+00:00, run_end_date=2023-09-11 06:56:50.602743+00:00, run_duration=4.634729, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-05 00:00:00+00:00, data_interval_end=2023-08-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:50.608+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-06T00:00:00+00:00, run_after=2023-08-07T00:00:00+00:00[0m
[[34m2023-09-11T06:56:50.626+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:50.626+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:50.627+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:50.629+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:50.630+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:50.630+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:50.633+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:52.975+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:53.144+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:53.332+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:53.369+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:53.980+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:53.981+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:54.061+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:54.145+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:54.985+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:54.998+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:54.238033+00:00, run_end_date=2023-09-11 06:56:54.477784+00:00, run_duration=0.239751, state=success, executor_state=success, try_number=1, max_tries=0, job_id=222, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:50.627840+00:00, queued_by_job_id=2, pid=42878[0m
[[34m2023-09-11T06:56:55.246+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-07T00:00:00+00:00, run_after=2023-08-08T00:00:00+00:00[0m
[[34m2023-09-11T06:56:55.268+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-06 00:00:00+00:00: scheduled__2023-08-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:50.549295+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:55.269+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-06 00:00:00+00:00, run_id=scheduled__2023-08-06T00:00:00+00:00, run_start_date=2023-09-11 06:56:50.573532+00:00, run_end_date=2023-09-11 06:56:55.269055+00:00, run_duration=4.695523, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-06 00:00:00+00:00, data_interval_end=2023-08-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:55.272+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-07T00:00:00+00:00, run_after=2023-08-08T00:00:00+00:00[0m
[[34m2023-09-11T06:56:56.600+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-08T00:00:00+00:00, run_after=2023-08-09T00:00:00+00:00[0m
[[34m2023-09-11T06:56:56.653+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:56.653+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:56.653+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:56.657+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:56.658+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:56.658+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:56.661+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:58.910+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:59.070+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:59.314+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:59.356+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:59.852+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:59.852+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:59.932+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:59.998+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:00.781+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:00.792+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:00.088958+00:00, run_end_date=2023-09-11 06:57:00.346640+00:00, run_duration=0.257682, state=success, executor_state=success, try_number=1, max_tries=0, job_id=223, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:56.655526+00:00, queued_by_job_id=2, pid=42890[0m
[[34m2023-09-11T06:57:00.955+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-09T00:00:00+00:00, run_after=2023-08-10T00:00:00+00:00[0m
[[34m2023-09-11T06:57:00.993+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-07 00:00:00+00:00: scheduled__2023-08-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:56.595392+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:00.993+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-07 00:00:00+00:00, run_id=scheduled__2023-08-07T00:00:00+00:00, run_start_date=2023-09-11 06:56:56.616813+00:00, run_end_date=2023-09-11 06:57:00.993658+00:00, run_duration=4.376845, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-07 00:00:00+00:00, data_interval_end=2023-08-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:00.997+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-08T00:00:00+00:00, run_after=2023-08-09T00:00:00+00:00[0m
[[34m2023-09-11T06:57:01.012+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:01.013+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:01.013+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:01.015+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:01.015+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:01.016+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:01.018+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:02.881+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:03.008+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:03.178+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:03.210+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:03.773+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:03.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:03.869+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:03.950+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:04.882+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:04.897+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:04.053849+00:00, run_end_date=2023-09-11 06:57:04.335093+00:00, run_duration=0.281244, state=success, executor_state=success, try_number=1, max_tries=0, job_id=224, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:01.013984+00:00, queued_by_job_id=2, pid=42897[0m
[[34m2023-09-11T06:57:05.158+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-09T00:00:00+00:00, run_after=2023-08-10T00:00:00+00:00[0m
[[34m2023-09-11T06:57:05.181+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-08 00:00:00+00:00: scheduled__2023-08-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:00.949620+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:05.182+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-08 00:00:00+00:00, run_id=scheduled__2023-08-08T00:00:00+00:00, run_start_date=2023-09-11 06:57:00.968846+00:00, run_end_date=2023-09-11 06:57:05.182230+00:00, run_duration=4.213384, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-08 00:00:00+00:00, data_interval_end=2023-08-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:05.185+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-09T00:00:00+00:00, run_after=2023-08-10T00:00:00+00:00[0m
[[34m2023-09-11T06:57:05.959+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-10T00:00:00+00:00, run_after=2023-08-11T00:00:00+00:00[0m
[[34m2023-09-11T06:57:06.016+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:06.016+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:06.017+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:06.019+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:06.020+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:06.020+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:06.044+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:08.261+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:08.386+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:08.567+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:08.604+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:09.058+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:09.059+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:09.130+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:09.191+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:09.898+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:09.910+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:09.276833+00:00, run_end_date=2023-09-11 06:57:09.497629+00:00, run_duration=0.220796, state=success, executor_state=success, try_number=1, max_tries=0, job_id=225, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:06.017776+00:00, queued_by_job_id=2, pid=42907[0m
[[34m2023-09-11T06:57:10.074+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-11T00:00:00+00:00, run_after=2023-08-12T00:00:00+00:00[0m
[[34m2023-09-11T06:57:10.108+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-09 00:00:00+00:00: scheduled__2023-08-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:05.952105+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:10.108+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-09 00:00:00+00:00, run_id=scheduled__2023-08-09T00:00:00+00:00, run_start_date=2023-09-11 06:57:05.975550+00:00, run_end_date=2023-09-11 06:57:10.108732+00:00, run_duration=4.133182, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-09 00:00:00+00:00, data_interval_end=2023-08-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:10.112+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-10T00:00:00+00:00, run_after=2023-08-11T00:00:00+00:00[0m
[[34m2023-09-11T06:57:10.128+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:10.128+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:10.128+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:10.131+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:10.131+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:10.132+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:10.134+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:12.046+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:12.187+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:12.365+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:12.396+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:12.867+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:12.868+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:12.939+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:12.994+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:13.724+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:13.735+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:13.078541+00:00, run_end_date=2023-09-11 06:57:13.296303+00:00, run_duration=0.217762, state=success, executor_state=success, try_number=1, max_tries=0, job_id=226, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:10.129676+00:00, queued_by_job_id=2, pid=42916[0m
[[34m2023-09-11T06:57:13.910+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-11T00:00:00+00:00, run_after=2023-08-12T00:00:00+00:00[0m
[[34m2023-09-11T06:57:13.933+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-10 00:00:00+00:00: scheduled__2023-08-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:10.068148+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:13.933+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-10 00:00:00+00:00, run_id=scheduled__2023-08-10T00:00:00+00:00, run_start_date=2023-09-11 06:57:10.086144+00:00, run_end_date=2023-09-11 06:57:13.933870+00:00, run_duration=3.847726, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-10 00:00:00+00:00, data_interval_end=2023-08-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:13.937+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-11T00:00:00+00:00, run_after=2023-08-12T00:00:00+00:00[0m
[[34m2023-09-11T06:57:15.207+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-12T00:00:00+00:00, run_after=2023-08-13T00:00:00+00:00[0m
[[34m2023-09-11T06:57:15.251+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:15.251+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:15.251+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:15.254+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:15.254+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:15.254+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:15.257+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:17.267+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:17.427+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:17.632+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:17.670+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:18.349+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:18.350+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:18.427+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:18.491+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:19.482+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:19.498+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:18.594472+00:00, run_end_date=2023-09-11 06:57:18.912499+00:00, run_duration=0.318027, state=success, executor_state=success, try_number=1, max_tries=0, job_id=227, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:15.252471+00:00, queued_by_job_id=2, pid=42926[0m
[[34m2023-09-11T06:57:19.832+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-13T00:00:00+00:00, run_after=2023-08-14T00:00:00+00:00[0m
[[34m2023-09-11T06:57:19.876+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-11 00:00:00+00:00: scheduled__2023-08-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:15.202788+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:19.877+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-11 00:00:00+00:00, run_id=scheduled__2023-08-11T00:00:00+00:00, run_start_date=2023-09-11 06:57:15.220449+00:00, run_end_date=2023-09-11 06:57:19.877181+00:00, run_duration=4.656732, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-11 00:00:00+00:00, data_interval_end=2023-08-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:19.880+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-12T00:00:00+00:00, run_after=2023-08-13T00:00:00+00:00[0m
[[34m2023-09-11T06:57:19.897+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:19.898+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:19.899+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:19.905+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:19.906+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:19.906+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:19.908+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:22.048+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:22.238+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:22.440+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:22.475+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:22.965+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:22.965+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:23.037+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:23.096+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:23.858+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:23.869+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:23.190204+00:00, run_end_date=2023-09-11 06:57:23.450985+00:00, run_duration=0.260781, state=success, executor_state=success, try_number=1, max_tries=0, job_id=228, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:19.901288+00:00, queued_by_job_id=2, pid=42935[0m
[[34m2023-09-11T06:57:24.314+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-13T00:00:00+00:00, run_after=2023-08-14T00:00:00+00:00[0m
[[34m2023-09-11T06:57:24.339+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-12 00:00:00+00:00: scheduled__2023-08-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:19.825343+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:24.340+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-12 00:00:00+00:00, run_id=scheduled__2023-08-12T00:00:00+00:00, run_start_date=2023-09-11 06:57:19.848267+00:00, run_end_date=2023-09-11 06:57:24.339935+00:00, run_duration=4.491668, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-12 00:00:00+00:00, data_interval_end=2023-08-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:24.343+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-13T00:00:00+00:00, run_after=2023-08-14T00:00:00+00:00[0m
[[34m2023-09-11T06:57:25.187+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-14T00:00:00+00:00, run_after=2023-08-15T00:00:00+00:00[0m
[[34m2023-09-11T06:57:25.244+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:25.245+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:25.245+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:25.247+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:25.248+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:25.248+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:25.251+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:27.379+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:27.527+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:27.782+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:27.825+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:28.650+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:28.651+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:28.727+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:28.788+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:29.796+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:29.807+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:28.924348+00:00, run_end_date=2023-09-11 06:57:29.257682+00:00, run_duration=0.333334, state=success, executor_state=success, try_number=1, max_tries=0, job_id=229, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:25.246244+00:00, queued_by_job_id=2, pid=42945[0m
[[34m2023-09-11T06:57:30.285+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-15T00:00:00+00:00, run_after=2023-08-16T00:00:00+00:00[0m
[[34m2023-09-11T06:57:30.345+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-13 00:00:00+00:00: scheduled__2023-08-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:25.182345+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:30.346+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-13 00:00:00+00:00, run_id=scheduled__2023-08-13T00:00:00+00:00, run_start_date=2023-09-11 06:57:25.211806+00:00, run_end_date=2023-09-11 06:57:30.346234+00:00, run_duration=5.134428, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-13 00:00:00+00:00, data_interval_end=2023-08-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:30.352+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-14T00:00:00+00:00, run_after=2023-08-15T00:00:00+00:00[0m
[[34m2023-09-11T06:57:30.373+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:30.373+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:30.374+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:30.377+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:30.378+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:30.378+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:30.381+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:32.500+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:32.632+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:32.865+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:32.899+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:33.440+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:33.441+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:33.514+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:33.573+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:34.314+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:34.329+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:33.662626+00:00, run_end_date=2023-09-11 06:57:33.882916+00:00, run_duration=0.22029, state=success, executor_state=success, try_number=1, max_tries=0, job_id=230, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:30.375236+00:00, queued_by_job_id=2, pid=42954[0m
[[34m2023-09-11T06:57:34.582+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-15T00:00:00+00:00, run_after=2023-08-16T00:00:00+00:00[0m
[[34m2023-09-11T06:57:34.609+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-14 00:00:00+00:00: scheduled__2023-08-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:30.277097+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:34.609+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-14 00:00:00+00:00, run_id=scheduled__2023-08-14T00:00:00+00:00, run_start_date=2023-09-11 06:57:30.313677+00:00, run_end_date=2023-09-11 06:57:34.609787+00:00, run_duration=4.29611, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-14 00:00:00+00:00, data_interval_end=2023-08-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:34.614+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-15T00:00:00+00:00, run_after=2023-08-16T00:00:00+00:00[0m
[[34m2023-09-11T06:57:35.073+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-16T00:00:00+00:00, run_after=2023-08-17T00:00:00+00:00[0m
[[34m2023-09-11T06:57:35.121+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:35.122+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:35.122+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:35.124+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:35.125+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:35.125+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:35.128+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:37.185+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:37.327+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:37.539+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:37.572+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:38.088+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:38.089+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:38.162+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:38.231+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:39.036+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:39.047+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:38.332293+00:00, run_end_date=2023-09-11 06:57:38.586419+00:00, run_duration=0.254126, state=success, executor_state=success, try_number=1, max_tries=0, job_id=231, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:35.123135+00:00, queued_by_job_id=2, pid=42964[0m
[[34m2023-09-11T06:57:39.326+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-17T00:00:00+00:00, run_after=2023-08-18T00:00:00+00:00[0m
[[34m2023-09-11T06:57:39.361+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-15 00:00:00+00:00: scheduled__2023-08-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:35.067298+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:39.362+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-15 00:00:00+00:00, run_id=scheduled__2023-08-15T00:00:00+00:00, run_start_date=2023-09-11 06:57:35.085451+00:00, run_end_date=2023-09-11 06:57:39.361999+00:00, run_duration=4.276548, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-15 00:00:00+00:00, data_interval_end=2023-08-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:39.367+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-16T00:00:00+00:00, run_after=2023-08-17T00:00:00+00:00[0m
[[34m2023-09-11T06:57:39.385+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:39.386+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:39.387+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:39.396+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:39.397+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:39.397+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:39.400+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:41.630+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:41.782+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:41.981+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:42.017+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:42.672+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:42.674+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:42.760+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:42.825+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:43.860+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:43.871+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:42.985217+00:00, run_end_date=2023-09-11 06:57:43.289337+00:00, run_duration=0.30412, state=success, executor_state=success, try_number=1, max_tries=0, job_id=232, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:39.394458+00:00, queued_by_job_id=2, pid=42973[0m
[[34m2023-09-11T06:57:44.563+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-17T00:00:00+00:00, run_after=2023-08-18T00:00:00+00:00[0m
[[34m2023-09-11T06:57:44.614+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-16 00:00:00+00:00: scheduled__2023-08-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:39.321634+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:44.614+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-16 00:00:00+00:00, run_id=scheduled__2023-08-16T00:00:00+00:00, run_start_date=2023-09-11 06:57:39.338677+00:00, run_end_date=2023-09-11 06:57:44.614845+00:00, run_duration=5.276168, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-16 00:00:00+00:00, data_interval_end=2023-08-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:44.620+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-17T00:00:00+00:00, run_after=2023-08-18T00:00:00+00:00[0m
[[34m2023-09-11T06:57:46.144+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-18T00:00:00+00:00, run_after=2023-08-19T00:00:00+00:00[0m
[[34m2023-09-11T06:57:46.227+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:46.227+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:46.227+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:46.229+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:46.230+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:46.230+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:46.233+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:48.282+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:48.456+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:48.641+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:48.677+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:49.187+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:49.187+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:49.260+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:49.319+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:50.215+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:50.229+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:49.428399+00:00, run_end_date=2023-09-11 06:57:49.715193+00:00, run_duration=0.286794, state=success, executor_state=success, try_number=1, max_tries=0, job_id=233, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:46.228411+00:00, queued_by_job_id=2, pid=42983[0m
[[34m2023-09-11T06:57:50.590+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-19T00:00:00+00:00, run_after=2023-08-20T00:00:00+00:00[0m
[[34m2023-09-11T06:57:50.625+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-17 00:00:00+00:00: scheduled__2023-08-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:46.139979+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:50.625+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-17 00:00:00+00:00, run_id=scheduled__2023-08-17T00:00:00+00:00, run_start_date=2023-09-11 06:57:46.192554+00:00, run_end_date=2023-09-11 06:57:50.625441+00:00, run_duration=4.432887, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-17 00:00:00+00:00, data_interval_end=2023-08-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:50.628+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-18T00:00:00+00:00, run_after=2023-08-19T00:00:00+00:00[0m
[[34m2023-09-11T06:57:50.643+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:50.643+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:50.643+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:50.645+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:50.646+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:50.646+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:50.649+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:52.930+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:53.059+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:53.266+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:53.315+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:53.821+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:53.822+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:53.894+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:53.950+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:55.015+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:55.028+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:54.112341+00:00, run_end_date=2023-09-11 06:57:54.522926+00:00, run_duration=0.410585, state=success, executor_state=success, try_number=1, max_tries=0, job_id=234, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:50.644318+00:00, queued_by_job_id=2, pid=42992[0m
[[34m2023-09-11T06:57:55.477+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-19T00:00:00+00:00, run_after=2023-08-20T00:00:00+00:00[0m
[[34m2023-09-11T06:57:55.504+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-18 00:00:00+00:00: scheduled__2023-08-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:50.585076+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:55.505+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-18 00:00:00+00:00, run_id=scheduled__2023-08-18T00:00:00+00:00, run_start_date=2023-09-11 06:57:50.602626+00:00, run_end_date=2023-09-11 06:57:55.505020+00:00, run_duration=4.902394, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-18 00:00:00+00:00, data_interval_end=2023-08-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:55.510+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-19T00:00:00+00:00, run_after=2023-08-20T00:00:00+00:00[0m
[[34m2023-09-11T06:57:56.989+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-20T00:00:00+00:00, run_after=2023-08-21T00:00:00+00:00[0m
[[34m2023-09-11T06:57:57.051+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:57.052+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:57.053+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:57.056+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:57.057+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:57.057+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:57.060+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:59.142+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:59.270+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:59.435+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:59.469+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:00.076+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:00.078+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:00.187+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:00.257+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:00.985+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:00.998+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:00.344518+00:00, run_end_date=2023-09-11 06:58:00.578565+00:00, run_duration=0.234047, state=success, executor_state=success, try_number=1, max_tries=0, job_id=235, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:57.054503+00:00, queued_by_job_id=2, pid=43004[0m
[[34m2023-09-11T06:58:01.519+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-21T00:00:00+00:00, run_after=2023-08-22T00:00:00+00:00[0m
[[34m2023-09-11T06:58:01.571+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-19 00:00:00+00:00: scheduled__2023-08-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:56.981565+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:01.572+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-19 00:00:00+00:00, run_id=scheduled__2023-08-19T00:00:00+00:00, run_start_date=2023-09-11 06:57:57.007159+00:00, run_end_date=2023-09-11 06:58:01.572277+00:00, run_duration=4.565118, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-19 00:00:00+00:00, data_interval_end=2023-08-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:01.578+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-20T00:00:00+00:00, run_after=2023-08-21T00:00:00+00:00[0m
[[34m2023-09-11T06:58:01.599+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:01.600+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:01.600+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:01.604+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:01.605+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:01.605+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:01.608+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:03.885+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:04.015+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:04.194+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:04.234+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:04.792+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:04.792+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:04.866+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:04.930+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:05.820+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:05.833+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:05.046235+00:00, run_end_date=2023-09-11 06:58:05.362739+00:00, run_duration=0.316504, state=success, executor_state=success, try_number=1, max_tries=0, job_id=236, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:01.602056+00:00, queued_by_job_id=2, pid=43013[0m
[[34m2023-09-11T06:58:06.194+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-21T00:00:00+00:00, run_after=2023-08-22T00:00:00+00:00[0m
[[34m2023-09-11T06:58:06.219+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-20 00:00:00+00:00: scheduled__2023-08-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:01.511915+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:06.220+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-20 00:00:00+00:00, run_id=scheduled__2023-08-20T00:00:00+00:00, run_start_date=2023-09-11 06:58:01.537388+00:00, run_end_date=2023-09-11 06:58:06.220493+00:00, run_duration=4.683105, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-20 00:00:00+00:00, data_interval_end=2023-08-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:06.224+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-21T00:00:00+00:00, run_after=2023-08-22T00:00:00+00:00[0m
[[34m2023-09-11T06:58:07.713+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-22T00:00:00+00:00, run_after=2023-08-23T00:00:00+00:00[0m
[[34m2023-09-11T06:58:07.756+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:07.756+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:07.756+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:07.758+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:07.759+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:07.759+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:07.762+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:09.953+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:10.172+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:10.380+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:10.424+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:10.973+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:10.973+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:11.057+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:11.119+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:12.037+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:12.053+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:11.214721+00:00, run_end_date=2023-09-11 06:58:11.527253+00:00, run_duration=0.312532, state=success, executor_state=success, try_number=1, max_tries=0, job_id=237, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:07.757270+00:00, queued_by_job_id=2, pid=43025[0m
[[34m2023-09-11T06:58:12.537+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-23T00:00:00+00:00, run_after=2023-08-24T00:00:00+00:00[0m
[[34m2023-09-11T06:58:12.577+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-21 00:00:00+00:00: scheduled__2023-08-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:07.708990+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:12.578+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-21 00:00:00+00:00, run_id=scheduled__2023-08-21T00:00:00+00:00, run_start_date=2023-09-11 06:58:07.725960+00:00, run_end_date=2023-09-11 06:58:12.577975+00:00, run_duration=4.852015, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-21 00:00:00+00:00, data_interval_end=2023-08-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:12.582+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-22T00:00:00+00:00, run_after=2023-08-23T00:00:00+00:00[0m
[[34m2023-09-11T06:58:12.598+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:12.599+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:12.599+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:12.602+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:12.603+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:12.603+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:12.606+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:14.559+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:14.688+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:14.856+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:14.888+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:15.361+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:15.362+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:15.441+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:15.500+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:16.206+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:16.217+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:15.587469+00:00, run_end_date=2023-09-11 06:58:15.813048+00:00, run_duration=0.225579, state=success, executor_state=success, try_number=1, max_tries=0, job_id=238, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:12.600377+00:00, queued_by_job_id=2, pid=43033[0m
[[34m2023-09-11T06:58:16.573+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-23T00:00:00+00:00, run_after=2023-08-24T00:00:00+00:00[0m
[[34m2023-09-11T06:58:16.596+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-22 00:00:00+00:00: scheduled__2023-08-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:12.531209+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:16.596+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-22 00:00:00+00:00, run_id=scheduled__2023-08-22T00:00:00+00:00, run_start_date=2023-09-11 06:58:12.550785+00:00, run_end_date=2023-09-11 06:58:16.596489+00:00, run_duration=4.045704, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-22 00:00:00+00:00, data_interval_end=2023-08-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:16.600+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-23T00:00:00+00:00, run_after=2023-08-24T00:00:00+00:00[0m
[[34m2023-09-11T06:58:17.652+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-24T00:00:00+00:00, run_after=2023-08-25T00:00:00+00:00[0m
[[34m2023-09-11T06:58:17.707+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:17.707+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:17.707+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:17.710+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:17.711+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:17.711+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:17.714+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:19.748+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:19.875+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:20.055+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:20.087+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:20.561+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:20.562+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:20.645+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:20.713+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:21.451+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:21.463+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:20.812383+00:00, run_end_date=2023-09-11 06:58:21.063065+00:00, run_duration=0.250682, state=success, executor_state=success, try_number=1, max_tries=0, job_id=239, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:17.708560+00:00, queued_by_job_id=2, pid=43043[0m
[[34m2023-09-11T06:58:21.821+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-25T00:00:00+00:00, run_after=2023-08-26T00:00:00+00:00[0m
[[34m2023-09-11T06:58:21.857+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-23 00:00:00+00:00: scheduled__2023-08-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:17.647047+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:21.857+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-23 00:00:00+00:00, run_id=scheduled__2023-08-23T00:00:00+00:00, run_start_date=2023-09-11 06:58:17.668516+00:00, run_end_date=2023-09-11 06:58:21.857683+00:00, run_duration=4.189167, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-23 00:00:00+00:00, data_interval_end=2023-08-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:21.861+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-24T00:00:00+00:00, run_after=2023-08-25T00:00:00+00:00[0m
[[34m2023-09-11T06:58:21.876+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:21.876+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:21.876+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:21.878+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:21.879+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:21.879+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:21.882+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:23.966+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:24.127+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:24.346+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:24.396+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:24.956+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:24.957+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:25.025+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:25.084+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:25.806+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:25.817+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:25.176775+00:00, run_end_date=2023-09-11 06:58:25.408029+00:00, run_duration=0.231254, state=success, executor_state=success, try_number=1, max_tries=0, job_id=240, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:21.877409+00:00, queued_by_job_id=2, pid=43052[0m
[[34m2023-09-11T06:58:26.027+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-25T00:00:00+00:00, run_after=2023-08-26T00:00:00+00:00[0m
[[34m2023-09-11T06:58:26.049+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-24 00:00:00+00:00: scheduled__2023-08-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:21.816035+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:26.050+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-24 00:00:00+00:00, run_id=scheduled__2023-08-24T00:00:00+00:00, run_start_date=2023-09-11 06:58:21.834676+00:00, run_end_date=2023-09-11 06:58:26.050413+00:00, run_duration=4.215737, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-24 00:00:00+00:00, data_interval_end=2023-08-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:26.053+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-25T00:00:00+00:00, run_after=2023-08-26T00:00:00+00:00[0m
[[34m2023-09-11T06:58:26.680+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-26T00:00:00+00:00, run_after=2023-08-27T00:00:00+00:00[0m
[[34m2023-09-11T06:58:26.741+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:26.741+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:26.741+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:26.743+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:26.744+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:26.745+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:26.747+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:28.584+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:28.717+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:28.880+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:28.920+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:29.386+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:29.386+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:29.455+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:29.514+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:30.233+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:30.243+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:29.603366+00:00, run_end_date=2023-09-11 06:58:29.816618+00:00, run_duration=0.213252, state=success, executor_state=success, try_number=1, max_tries=0, job_id=241, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:26.742341+00:00, queued_by_job_id=2, pid=43060[0m
[[34m2023-09-11T06:58:30.603+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-27T00:00:00+00:00, run_after=2023-08-28T00:00:00+00:00[0m
[[34m2023-09-11T06:58:30.643+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-25 00:00:00+00:00: scheduled__2023-08-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:26.675410+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:30.644+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-25 00:00:00+00:00, run_id=scheduled__2023-08-25T00:00:00+00:00, run_start_date=2023-09-11 06:58:26.708844+00:00, run_end_date=2023-09-11 06:58:30.644001+00:00, run_duration=3.935157, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-25 00:00:00+00:00, data_interval_end=2023-08-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:30.649+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-26T00:00:00+00:00, run_after=2023-08-27T00:00:00+00:00[0m
[[34m2023-09-11T06:58:30.669+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:30.670+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:30.670+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:30.673+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:30.673+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:30.673+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:30.676+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:32.547+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:32.674+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:32.870+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:32.901+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:33.361+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:33.361+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:33.434+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:33.490+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:34.206+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:34.217+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:33.574419+00:00, run_end_date=2023-09-11 06:58:33.789032+00:00, run_duration=0.214613, state=success, executor_state=success, try_number=1, max_tries=0, job_id=242, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:30.671589+00:00, queued_by_job_id=2, pid=43069[0m
[[34m2023-09-11T06:58:34.942+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-27T00:00:00+00:00, run_after=2023-08-28T00:00:00+00:00[0m
[[34m2023-09-11T06:58:34.970+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-26 00:00:00+00:00: scheduled__2023-08-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:30.598161+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:34.971+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-26 00:00:00+00:00, run_id=scheduled__2023-08-26T00:00:00+00:00, run_start_date=2023-09-11 06:58:30.615691+00:00, run_end_date=2023-09-11 06:58:34.971064+00:00, run_duration=4.355373, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-26 00:00:00+00:00, data_interval_end=2023-08-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:34.975+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-27T00:00:00+00:00, run_after=2023-08-28T00:00:00+00:00[0m
[[34m2023-09-11T06:58:35.907+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-28T00:00:00+00:00, run_after=2023-08-29T00:00:00+00:00[0m
[[34m2023-09-11T06:58:35.954+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:35.954+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:35.954+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:35.957+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:35.958+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:35.958+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:35.960+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:37.786+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:37.918+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:38.091+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:38.125+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:38.587+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:38.587+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:38.657+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:38.715+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:39.434+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:39.445+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:38.799612+00:00, run_end_date=2023-09-11 06:58:39.029149+00:00, run_duration=0.229537, state=success, executor_state=success, try_number=1, max_tries=0, job_id=243, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:35.955659+00:00, queued_by_job_id=2, pid=43079[0m
[[34m2023-09-11T06:58:39.928+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-29T00:00:00+00:00, run_after=2023-08-30T00:00:00+00:00[0m
[[34m2023-09-11T06:58:39.968+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-27 00:00:00+00:00: scheduled__2023-08-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:35.902733+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:39.968+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-27 00:00:00+00:00, run_id=scheduled__2023-08-27T00:00:00+00:00, run_start_date=2023-09-11 06:58:35.920118+00:00, run_end_date=2023-09-11 06:58:39.968495+00:00, run_duration=4.048377, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-27 00:00:00+00:00, data_interval_end=2023-08-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:39.973+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-28T00:00:00+00:00, run_after=2023-08-29T00:00:00+00:00[0m
[[34m2023-09-11T06:58:39.991+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:39.991+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:39.991+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:39.994+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:39.994+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:39.994+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:39.997+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:42.339+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:42.518+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:42.730+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:42.768+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:43.397+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:43.397+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:43.522+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:43.590+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:44.317+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:44.329+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:43.696714+00:00, run_end_date=2023-09-11 06:58:43.921859+00:00, run_duration=0.225145, state=success, executor_state=success, try_number=1, max_tries=0, job_id=244, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:39.992522+00:00, queued_by_job_id=2, pid=43088[0m
[[34m2023-09-11T06:58:45.044+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-29T00:00:00+00:00, run_after=2023-08-30T00:00:00+00:00[0m
[[34m2023-09-11T06:58:45.077+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-28 00:00:00+00:00: scheduled__2023-08-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:39.923468+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:45.078+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-28 00:00:00+00:00, run_id=scheduled__2023-08-28T00:00:00+00:00, run_start_date=2023-09-11 06:58:39.941999+00:00, run_end_date=2023-09-11 06:58:45.078517+00:00, run_duration=5.136518, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-28 00:00:00+00:00, data_interval_end=2023-08-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:45.084+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-29T00:00:00+00:00, run_after=2023-08-30T00:00:00+00:00[0m
[[34m2023-09-11T06:58:46.571+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-30T00:00:00+00:00, run_after=2023-08-31T00:00:00+00:00[0m
[[34m2023-09-11T06:58:46.618+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:46.618+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:46.618+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:46.620+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:46.621+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:46.621+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:46.624+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:48.571+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:48.704+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:48.869+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:48.901+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:49.423+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:49.423+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:49.496+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:49.556+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:50.339+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:50.350+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:49.642181+00:00, run_end_date=2023-09-11 06:58:49.896194+00:00, run_duration=0.254013, state=success, executor_state=success, try_number=1, max_tries=0, job_id=245, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:46.619383+00:00, queued_by_job_id=2, pid=43098[0m
[[34m2023-09-11T06:58:50.934+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-31T00:00:00+00:00, run_after=2023-09-01T00:00:00+00:00[0m
[[34m2023-09-11T06:58:50.968+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-29 00:00:00+00:00: scheduled__2023-08-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:46.566820+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:50.969+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-29 00:00:00+00:00, run_id=scheduled__2023-08-29T00:00:00+00:00, run_start_date=2023-09-11 06:58:46.584238+00:00, run_end_date=2023-09-11 06:58:50.969226+00:00, run_duration=4.384988, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-29 00:00:00+00:00, data_interval_end=2023-08-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:50.972+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-30T00:00:00+00:00, run_after=2023-08-31T00:00:00+00:00[0m
[[34m2023-09-11T06:58:50.989+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:50.989+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:50.990+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:50.992+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:50.992+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:50.993+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:50.996+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:52.914+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:53.055+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:53.262+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:53.303+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:53.813+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:53.813+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:53.884+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:53.954+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:55.294+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:55.305+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:54.042620+00:00, run_end_date=2023-09-11 06:58:54.292338+00:00, run_duration=0.249718, state=success, executor_state=success, try_number=1, max_tries=0, job_id=246, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:50.990721+00:00, queued_by_job_id=2, pid=43107[0m
[[34m2023-09-11T06:58:55.878+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-31T00:00:00+00:00, run_after=2023-09-01T00:00:00+00:00[0m
[[34m2023-09-11T06:58:55.905+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-30 00:00:00+00:00: scheduled__2023-08-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:50.928837+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:55.905+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-30 00:00:00+00:00, run_id=scheduled__2023-08-30T00:00:00+00:00, run_start_date=2023-09-11 06:58:50.946563+00:00, run_end_date=2023-09-11 06:58:55.905502+00:00, run_duration=4.958939, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-30 00:00:00+00:00, data_interval_end=2023-08-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:55.908+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-31T00:00:00+00:00, run_after=2023-09-01T00:00:00+00:00[0m
[[34m2023-09-11T06:58:57.268+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-01T00:00:00+00:00, run_after=2023-09-02T00:00:00+00:00[0m
[[34m2023-09-11T06:58:57.316+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:57.317+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:57.317+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:57.319+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:57.320+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:57.320+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:57.323+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:59.233+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:59.361+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:59.533+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:59.564+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:00.041+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:00.042+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:00.114+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:00.172+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:01.048+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:01.058+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:00.256163+00:00, run_end_date=2023-09-11 06:59:00.480089+00:00, run_duration=0.223926, state=success, executor_state=success, try_number=1, max_tries=0, job_id=247, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:57.318155+00:00, queued_by_job_id=2, pid=43119[0m
[[34m2023-09-11T06:59:01.329+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00[0m
[[34m2023-09-11T06:59:01.364+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-31 00:00:00+00:00: scheduled__2023-08-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:57.262809+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:01.364+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-31 00:00:00+00:00, run_id=scheduled__2023-08-31T00:00:00+00:00, run_start_date=2023-09-11 06:58:57.282677+00:00, run_end_date=2023-09-11 06:59:01.364459+00:00, run_duration=4.081782, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-31 00:00:00+00:00, data_interval_end=2023-09-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:01.368+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-01T00:00:00+00:00, run_after=2023-09-02T00:00:00+00:00[0m
[[34m2023-09-11T06:59:01.383+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:01.383+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:01.384+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:01.386+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:01.386+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:01.387+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:01.400+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:03.353+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:03.492+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:03.670+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:03.706+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:04.482+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:04.484+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:04.612+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:04.681+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:05.698+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:05.711+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:04.809214+00:00, run_end_date=2023-09-11 06:59:05.051655+00:00, run_duration=0.242441, state=success, executor_state=success, try_number=1, max_tries=0, job_id=248, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:01.384904+00:00, queued_by_job_id=2, pid=43126[0m
[[34m2023-09-11T06:59:05.976+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00[0m
[[34m2023-09-11T06:59:06.002+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-01 00:00:00+00:00: scheduled__2023-09-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:01.323766+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:06.003+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-01 00:00:00+00:00, run_id=scheduled__2023-09-01T00:00:00+00:00, run_start_date=2023-09-11 06:59:01.341818+00:00, run_end_date=2023-09-11 06:59:06.003371+00:00, run_duration=4.661553, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-01 00:00:00+00:00, data_interval_end=2023-09-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:06.007+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00[0m
[[34m2023-09-11T06:59:06.425+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-03T00:00:00+00:00, run_after=2023-09-04T00:00:00+00:00[0m
[[34m2023-09-11T06:59:06.470+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:06.470+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:06.470+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:06.473+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:06.474+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:06.474+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:06.479+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:08.672+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:08.871+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:09.075+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:09.119+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:09.630+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:09.631+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:09.708+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:09.774+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:10.533+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:10.544+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:09.869480+00:00, run_end_date=2023-09-11 06:59:10.104398+00:00, run_duration=0.234918, state=success, executor_state=success, try_number=1, max_tries=0, job_id=249, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:06.471419+00:00, queued_by_job_id=2, pid=43136[0m
[[34m2023-09-11T06:59:10.704+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-04T00:00:00+00:00, run_after=2023-09-05T00:00:00+00:00[0m
[[34m2023-09-11T06:59:10.743+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-02 00:00:00+00:00: scheduled__2023-09-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:06.420757+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:10.743+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-02 00:00:00+00:00, run_id=scheduled__2023-09-02T00:00:00+00:00, run_start_date=2023-09-11 06:59:06.438298+00:00, run_end_date=2023-09-11 06:59:10.743454+00:00, run_duration=4.305156, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-02 00:00:00+00:00, data_interval_end=2023-09-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:10.746+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-03T00:00:00+00:00, run_after=2023-09-04T00:00:00+00:00[0m
[[34m2023-09-11T06:59:10.762+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:10.762+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:10.763+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:10.765+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:10.766+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:10.767+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:10.770+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:12.792+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:12.934+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:13.130+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:13.166+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:13.651+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:13.652+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:13.728+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:13.796+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:14.544+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:14.555+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:13.886880+00:00, run_end_date=2023-09-11 06:59:14.118903+00:00, run_duration=0.232023, state=success, executor_state=success, try_number=1, max_tries=0, job_id=250, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:10.763773+00:00, queued_by_job_id=2, pid=43145[0m
[[34m2023-09-11T06:59:14.805+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-04T00:00:00+00:00, run_after=2023-09-05T00:00:00+00:00[0m
[[34m2023-09-11T06:59:14.828+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-03 00:00:00+00:00: scheduled__2023-09-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:10.699534+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:14.828+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-03 00:00:00+00:00, run_id=scheduled__2023-09-03T00:00:00+00:00, run_start_date=2023-09-11 06:59:10.719041+00:00, run_end_date=2023-09-11 06:59:14.828729+00:00, run_duration=4.109688, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-03 00:00:00+00:00, data_interval_end=2023-09-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:14.832+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-04T00:00:00+00:00, run_after=2023-09-05T00:00:00+00:00[0m
[[34m2023-09-11T06:59:15.701+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-05T00:00:00+00:00, run_after=2023-09-06T00:00:00+00:00[0m
[[34m2023-09-11T06:59:15.749+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:15.749+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:15.750+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:15.752+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:15.753+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:15.753+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:15.756+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:17.704+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:17.845+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:18.041+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:18.077+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:18.591+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:18.591+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:18.668+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:18.728+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:19.452+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:19.464+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:18.817067+00:00, run_end_date=2023-09-11 06:59:19.045019+00:00, run_duration=0.227952, state=success, executor_state=success, try_number=1, max_tries=0, job_id=251, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:15.750913+00:00, queued_by_job_id=2, pid=43155[0m
[[34m2023-09-11T06:59:19.632+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-06T00:00:00+00:00, run_after=2023-09-07T00:00:00+00:00[0m
[[34m2023-09-11T06:59:19.668+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-04 00:00:00+00:00: scheduled__2023-09-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:15.695198+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:19.669+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-04 00:00:00+00:00, run_id=scheduled__2023-09-04T00:00:00+00:00, run_start_date=2023-09-11 06:59:15.714745+00:00, run_end_date=2023-09-11 06:59:19.669232+00:00, run_duration=3.954487, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-04 00:00:00+00:00, data_interval_end=2023-09-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:19.673+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-05T00:00:00+00:00, run_after=2023-09-06T00:00:00+00:00[0m
[[34m2023-09-11T06:59:19.689+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:19.690+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:19.690+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:19.692+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:19.693+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:19.693+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:19.696+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:21.701+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:21.839+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:22.041+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:22.078+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:22.563+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:22.564+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:22.638+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:22.697+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:23.461+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:23.473+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:22.786616+00:00, run_end_date=2023-09-11 06:59:23.034985+00:00, run_duration=0.248369, state=success, executor_state=success, try_number=1, max_tries=0, job_id=252, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:19.691180+00:00, queued_by_job_id=2, pid=43164[0m
[[34m2023-09-11T06:59:23.646+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-06T00:00:00+00:00, run_after=2023-09-07T00:00:00+00:00[0m
[[34m2023-09-11T06:59:23.670+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-05 00:00:00+00:00: scheduled__2023-09-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:19.626682+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:23.671+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-05 00:00:00+00:00, run_id=scheduled__2023-09-05T00:00:00+00:00, run_start_date=2023-09-11 06:59:19.645012+00:00, run_end_date=2023-09-11 06:59:23.671179+00:00, run_duration=4.026167, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-05 00:00:00+00:00, data_interval_end=2023-09-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:23.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-06T00:00:00+00:00, run_after=2023-09-07T00:00:00+00:00[0m
[[34m2023-09-11T06:59:24.658+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00[0m
[[34m2023-09-11T06:59:24.708+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:24.708+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:24.708+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:24.711+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:24.711+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:24.711+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:24.715+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:26.722+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:26.873+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:27.079+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:27.120+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:27.674+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:27.674+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:27.754+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:27.814+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:28.625+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:28.637+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:27.905557+00:00, run_end_date=2023-09-11 06:59:28.153444+00:00, run_duration=0.247887, state=success, executor_state=success, try_number=1, max_tries=0, job_id=253, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:24.709369+00:00, queued_by_job_id=2, pid=43174[0m
[[34m2023-09-11T06:59:28.908+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00[0m
[[34m2023-09-11T06:59:28.963+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-06 00:00:00+00:00: scheduled__2023-09-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:24.652383+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:28.964+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-06 00:00:00+00:00, run_id=scheduled__2023-09-06T00:00:00+00:00, run_start_date=2023-09-11 06:59:24.672294+00:00, run_end_date=2023-09-11 06:59:28.963979+00:00, run_duration=4.291685, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-06 00:00:00+00:00, data_interval_end=2023-09-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:28.968+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00[0m
[[34m2023-09-11T06:59:28.983+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:28.984+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:28.984+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:28.986+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:28.987+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:28.987+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:28.990+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:31.008+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:31.147+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:31.328+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:31.364+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:31.882+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:31.882+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:31.959+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:32.021+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:32.816+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:32.827+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:32.112721+00:00, run_end_date=2023-09-11 06:59:32.350579+00:00, run_duration=0.237858, state=success, executor_state=success, try_number=1, max_tries=0, job_id=254, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:28.985015+00:00, queued_by_job_id=2, pid=43183[0m
[[34m2023-09-11T06:59:33.074+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00[0m
[[34m2023-09-11T06:59:33.098+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-07 00:00:00+00:00: scheduled__2023-09-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:28.903434+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:33.099+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-07 00:00:00+00:00, run_id=scheduled__2023-09-07T00:00:00+00:00, run_start_date=2023-09-11 06:59:28.922245+00:00, run_end_date=2023-09-11 06:59:33.099333+00:00, run_duration=4.177088, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-07 00:00:00+00:00, data_interval_end=2023-09-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:33.105+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00[0m
[[34m2023-09-11T06:59:33.992+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-09T00:00:00+00:00, run_after=2023-09-10T00:00:00+00:00[0m
[[34m2023-09-11T06:59:34.041+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:34.041+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:34.041+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:34.043+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:34.044+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:34.045+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:34.048+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:36.086+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:36.234+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:36.415+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:36.451+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:36.960+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:36.961+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:37.039+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:37.105+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:37.894+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:37.905+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:37.199637+00:00, run_end_date=2023-09-11 06:59:37.454572+00:00, run_duration=0.254935, state=success, executor_state=success, try_number=1, max_tries=0, job_id=255, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:34.042596+00:00, queued_by_job_id=2, pid=43193[0m
[[34m2023-09-11T06:59:38.164+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00[0m
[[34m2023-09-11T06:59:38.202+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-08 00:00:00+00:00: scheduled__2023-09-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:33.988047+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:38.203+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-08 00:00:00+00:00, run_id=scheduled__2023-09-08T00:00:00+00:00, run_start_date=2023-09-11 06:59:34.007064+00:00, run_end_date=2023-09-11 06:59:38.202993+00:00, run_duration=4.195929, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-08 00:00:00+00:00, data_interval_end=2023-09-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:38.206+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-09T00:00:00+00:00, run_after=2023-09-10T00:00:00+00:00[0m
[[34m2023-09-11T06:59:38.222+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:38.223+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:38.223+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:38.225+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:38.226+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:38.226+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:38.229+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:40.157+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:40.295+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:40.490+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:40.524+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:41.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:41.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:41.088+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:41.159+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:41.977+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:41.990+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:41.251785+00:00, run_end_date=2023-09-11 06:59:41.491433+00:00, run_duration=0.239648, state=success, executor_state=success, try_number=1, max_tries=0, job_id=256, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:38.224155+00:00, queued_by_job_id=2, pid=43200[0m
[[34m2023-09-11T06:59:42.239+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00[0m
[[34m2023-09-11T06:59:42.265+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-09 00:00:00+00:00: scheduled__2023-09-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:38.158972+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:42.265+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-09 00:00:00+00:00, run_id=scheduled__2023-09-09T00:00:00+00:00, run_start_date=2023-09-11 06:59:38.178530+00:00, run_end_date=2023-09-11 06:59:42.265608+00:00, run_duration=4.087078, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-09 00:00:00+00:00, data_interval_end=2023-09-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:42.270+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00[0m
[[34m2023-09-11T06:59:43.166+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-11T00:00:00+00:00, run_after=2023-09-12T00:00:00+00:00[0m
[[34m2023-09-11T06:59:43.211+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:43.212+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:43.212+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:43.214+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:43.215+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:43.215+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:43.218+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:45.129+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:45.291+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:45.507+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:45.542+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:46.023+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:46.024+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:46.103+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:46.177+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:46.969+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:46.982+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:46.272386+00:00, run_end_date=2023-09-11 06:59:46.528545+00:00, run_duration=0.256159, state=success, executor_state=success, try_number=1, max_tries=0, job_id=257, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:43.212994+00:00, queued_by_job_id=2, pid=43212[0m
[[34m2023-09-11T06:59:47.275+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 00:00:00+00:00: scheduled__2023-09-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:43.161610+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:47.275+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 00:00:00+00:00, run_id=scheduled__2023-09-10T00:00:00+00:00, run_start_date=2023-09-11 06:59:43.179052+00:00, run_end_date=2023-09-11 06:59:47.275521+00:00, run_duration=4.096469, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-10 00:00:00+00:00, data_interval_end=2023-09-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:47.279+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-11T00:00:00+00:00, run_after=2023-09-12T00:00:00+00:00[0m
[[34m2023-09-11T07:00:01.385+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:05:01.671+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:10:02.072+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:15:02.245+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:20:02.350+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:25:02.630+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:30:02.973+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:35:03.294+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:40:03.558+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:45:03.724+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:50:04.080+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:55:04.403+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:00:04.524+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-11T08:04:56.751+0000] {manager.py:543} INFO - DAG python_operatio is missing and will be deactivated.
[2023-09-11T08:04:56.754+0000] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2023-09-11T08:04:56.757+0000] {manager.py:557} INFO - Deleted DAG python_operatio in serialized_dag table
[[34m2023-09-11T08:05:04.547+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:05:35.908+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-02T00:00:00+00:00, run_after=2023-01-03T00:00:00+00:00[0m
[[34m2023-09-11T08:05:35.961+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 2 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: python_operation.print_python manual__2023-09-11T08:04:49+00:00 [scheduled]>[0m
[[34m2023-09-11T08:05:35.961+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:05:35.961+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 1/16 running and queued tasks[0m
[[34m2023-09-11T08:05:35.961+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: python_operation.print_python manual__2023-09-11T08:04:49+00:00 [scheduled]>[0m
[[34m2023-09-11T08:05:35.966+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:05:35.967+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:05:35.968+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:05:35.968+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:35.969+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='manual__2023-09-11T08:04:49+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:05:35.969+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'manual__2023-09-11T08:04:49+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:35.971+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:38.662+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:05:38.869+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:05:39.059+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:39.102+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:39.658+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:05:39.659+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:05:39.744+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-01T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:05:39.816+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:05:40.955+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'manual__2023-09-11T08:04:49+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:43.560+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:05:43.790+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:05:43.973+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:44.007+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:44.623+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:05:44.623+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:05:44.696+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=manual__2023-09-11T08:04:49+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:05:44.754+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python manual__2023-09-11T08:04:49+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:05:45.578+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:05:45.579+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='manual__2023-09-11T08:04:49+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:05:45.600+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=manual__2023-09-11T08:04:49+00:00, map_index=-1, run_start_date=2023-09-11 08:05:44.853789+00:00, run_end_date=2023-09-11 08:05:45.069453+00:00, run_duration=0.215664, state=success, executor_state=success, try_number=1, max_tries=0, job_id=259, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:05:35.962655+00:00, queued_by_job_id=2, pid=49814[0m
[[34m2023-09-11T08:05:45.601+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:05:39.938884+00:00, run_end_date=2023-09-11 08:05:40.274338+00:00, run_duration=0.335454, state=success, executor_state=success, try_number=1, max_tries=0, job_id=258, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:05:35.962655+00:00, queued_by_job_id=2, pid=49807[0m
[[34m2023-09-11T08:05:46.111+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00[0m
[[34m2023-09-11T08:05:46.169+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-01 00:00:00+00:00: scheduled__2023-01-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:05:35.903718+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:05:46.169+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-01 00:00:00+00:00, run_id=scheduled__2023-01-01T00:00:00+00:00, run_start_date=2023-09-11 08:05:35.921128+00:00, run_end_date=2023-09-11 08:05:46.169684+00:00, run_duration=10.248556, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-01 00:00:00+00:00, data_interval_end=2023-01-02 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:05:46.173+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-02T00:00:00+00:00, run_after=2023-01-03T00:00:00+00:00[0m
[[34m2023-09-11T08:05:46.178+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-11 08:04:49+00:00: manual__2023-09-11T08:04:49+00:00, state:running, queued_at: 2023-09-11 08:04:49.339298+00:00. externally triggered: True> successful[0m
[[34m2023-09-11T08:05:46.178+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-11 08:04:49+00:00, run_id=manual__2023-09-11T08:04:49+00:00, run_start_date=2023-09-11 08:05:35.921340+00:00, run_end_date=2023-09-11 08:05:46.178659+00:00, run_duration=10.257319, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-10 08:04:49+00:00, data_interval_end=2023-09-11 08:04:49+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:05:46.183+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-11T08:04:49+00:00, run_after=2023-09-12T08:04:49+00:00[0m
[[34m2023-09-11T08:05:46.200+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 2 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: python_operation.print_python manual__2023-09-11T08:05:35.369837+00:00 [scheduled]>[0m
[[34m2023-09-11T08:05:46.201+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:05:46.201+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 1/16 running and queued tasks[0m
[[34m2023-09-11T08:05:46.201+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: python_operation.print_python manual__2023-09-11T08:05:35.369837+00:00 [scheduled]>[0m
[[34m2023-09-11T08:05:46.207+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:05:46.207+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:05:46.208+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:05:46.208+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:46.209+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='manual__2023-09-11T08:05:35.369837+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:05:46.209+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'manual__2023-09-11T08:05:35.369837+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:46.212+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:48.653+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:05:48.799+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:05:48.986+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:49.035+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:49.588+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:05:49.589+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:05:49.665+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-02T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:05:49.748+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:05:50.691+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'manual__2023-09-11T08:05:35.369837+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:52.877+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:05:53.019+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:05:53.308+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:53.343+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:53.823+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:05:53.823+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:05:53.900+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=manual__2023-09-11T08:05:35.369837+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:05:53.960+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python manual__2023-09-11T08:05:35.369837+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:05:54.847+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:05:54.849+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='manual__2023-09-11T08:05:35.369837+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:05:54.860+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=manual__2023-09-11T08:05:35.369837+00:00, map_index=-1, run_start_date=2023-09-11 08:05:54.050675+00:00, run_end_date=2023-09-11 08:05:54.328232+00:00, run_duration=0.277557, state=success, executor_state=success, try_number=1, max_tries=0, job_id=261, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:05:46.202387+00:00, queued_by_job_id=2, pid=49828[0m
[[34m2023-09-11T08:05:54.861+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:05:49.870829+00:00, run_end_date=2023-09-11 08:05:50.193958+00:00, run_duration=0.323129, state=success, executor_state=success, try_number=1, max_tries=0, job_id=260, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:05:46.202387+00:00, queued_by_job_id=2, pid=49821[0m
[[34m2023-09-11T08:05:55.165+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-02 00:00:00+00:00: scheduled__2023-01-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:05:46.102631+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:05:55.166+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-02 00:00:00+00:00, run_id=scheduled__2023-01-02T00:00:00+00:00, run_start_date=2023-09-11 08:05:46.129219+00:00, run_end_date=2023-09-11 08:05:55.166343+00:00, run_duration=9.037124, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-02 00:00:00+00:00, data_interval_end=2023-01-03 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:05:55.170+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00[0m
[[34m2023-09-11T08:05:55.175+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-11 08:05:35.369837+00:00: manual__2023-09-11T08:05:35.369837+00:00, state:running, queued_at: 2023-09-11 08:05:35.409453+00:00. externally triggered: True> successful[0m
[[34m2023-09-11T08:05:55.176+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-11 08:05:35.369837+00:00, run_id=manual__2023-09-11T08:05:35.369837+00:00, run_start_date=2023-09-11 08:05:46.129560+00:00, run_end_date=2023-09-11 08:05:55.175916+00:00, run_duration=9.046356, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-10 08:05:35.369837+00:00, data_interval_end=2023-09-11 08:05:35.369837+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:05:55.180+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-11T08:05:35.369837+00:00, run_after=2023-09-12T08:05:35.369837+00:00[0m
[[34m2023-09-11T08:05:57.252+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00[0m
[[34m2023-09-11T08:05:57.375+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:05:57.375+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:05:57.376+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:05:57.378+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:05:57.378+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:05:57.379+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:57.381+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:05:59.186+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:05:59.312+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:05:59.477+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:59.508+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:05:59.971+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:05:59.971+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:00.044+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-03T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:00.102+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:00.780+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:00.791+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:00.188406+00:00, run_end_date=2023-09-11 08:06:00.379511+00:00, run_duration=0.191105, state=success, executor_state=success, try_number=1, max_tries=0, job_id=262, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:05:57.376967+00:00, queued_by_job_id=2, pid=49838[0m
[[34m2023-09-11T08:06:01.055+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-05T00:00:00+00:00, run_after=2023-01-06T00:00:00+00:00[0m
[[34m2023-09-11T08:06:01.112+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-03 00:00:00+00:00: scheduled__2023-01-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:05:57.247404+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:01.113+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-03 00:00:00+00:00, run_id=scheduled__2023-01-03T00:00:00+00:00, run_start_date=2023-09-11 08:05:57.343859+00:00, run_end_date=2023-09-11 08:06:01.113355+00:00, run_duration=3.769496, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-03 00:00:00+00:00, data_interval_end=2023-01-04 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:01.120+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00[0m
[[34m2023-09-11T08:06:01.146+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:01.147+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:01.148+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:01.151+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:01.152+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:01.152+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:01.156+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:03.169+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:03.305+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:03.482+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:03.515+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:03.997+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:03.998+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:04.070+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-04T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:04.137+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:04.976+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:04.987+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:04.237996+00:00, run_end_date=2023-09-11 08:06:04.447954+00:00, run_duration=0.209958, state=success, executor_state=success, try_number=1, max_tries=0, job_id=263, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:01.149339+00:00, queued_by_job_id=2, pid=49846[0m
[[34m2023-09-11T08:06:05.245+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-05T00:00:00+00:00, run_after=2023-01-06T00:00:00+00:00[0m
[[34m2023-09-11T08:06:05.268+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-04 00:00:00+00:00: scheduled__2023-01-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:01.049355+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:05.268+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-04 00:00:00+00:00, run_id=scheduled__2023-01-04T00:00:00+00:00, run_start_date=2023-09-11 08:06:01.075893+00:00, run_end_date=2023-09-11 08:06:05.268395+00:00, run_duration=4.192502, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-04 00:00:00+00:00, data_interval_end=2023-01-05 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:05.272+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-05T00:00:00+00:00, run_after=2023-01-06T00:00:00+00:00[0m
[[34m2023-09-11T08:06:06.162+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-06T00:00:00+00:00, run_after=2023-01-07T00:00:00+00:00[0m
[[34m2023-09-11T08:06:06.208+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:06.208+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:06.208+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:06.210+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:06.211+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:06.211+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:06.214+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:08.269+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:08.392+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:08.560+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:08.592+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:09.083+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:09.083+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:09.167+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-05T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:09.228+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:09.862+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:09.873+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:09.311880+00:00, run_end_date=2023-09-11 08:06:09.498376+00:00, run_duration=0.186496, state=success, executor_state=success, try_number=1, max_tries=0, job_id=264, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:06.209300+00:00, queued_by_job_id=2, pid=49855[0m
[[34m2023-09-11T08:06:10.149+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-07T00:00:00+00:00, run_after=2023-01-08T00:00:00+00:00[0m
[[34m2023-09-11T08:06:10.187+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-05 00:00:00+00:00: scheduled__2023-01-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:06.157381+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:10.187+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-05 00:00:00+00:00, run_id=scheduled__2023-01-05T00:00:00+00:00, run_start_date=2023-09-11 08:06:06.175576+00:00, run_end_date=2023-09-11 08:06:10.187414+00:00, run_duration=4.011838, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-05 00:00:00+00:00, data_interval_end=2023-01-06 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:10.191+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-06T00:00:00+00:00, run_after=2023-01-07T00:00:00+00:00[0m
[[34m2023-09-11T08:06:10.206+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:10.206+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:10.207+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:10.209+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:10.209+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:10.210+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:10.213+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:12.331+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:12.673+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:12.958+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:13.000+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:13.725+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:13.726+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:13.798+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-06T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:13.856+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:14.626+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:14.636+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:14.004481+00:00, run_end_date=2023-09-11 08:06:14.208266+00:00, run_duration=0.203785, state=success, executor_state=success, try_number=1, max_tries=0, job_id=265, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:10.207629+00:00, queued_by_job_id=2, pid=49863[0m
[[34m2023-09-11T08:06:14.776+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-07T00:00:00+00:00, run_after=2023-01-08T00:00:00+00:00[0m
[[34m2023-09-11T08:06:14.802+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-06 00:00:00+00:00: scheduled__2023-01-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:10.144903+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:14.802+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-06 00:00:00+00:00, run_id=scheduled__2023-01-06T00:00:00+00:00, run_start_date=2023-09-11 08:06:10.164468+00:00, run_end_date=2023-09-11 08:06:14.802728+00:00, run_duration=4.63826, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-06 00:00:00+00:00, data_interval_end=2023-01-07 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:14.806+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-07T00:00:00+00:00, run_after=2023-01-08T00:00:00+00:00[0m
[[34m2023-09-11T08:06:15.131+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-08T00:00:00+00:00, run_after=2023-01-09T00:00:00+00:00[0m
[[34m2023-09-11T08:06:15.184+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:15.185+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:15.185+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:15.187+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:15.188+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:15.188+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:15.191+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:17.116+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:17.284+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:17.474+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:17.506+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:17.986+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:17.987+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:18.059+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-07T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:18.120+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:18.779+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:18.789+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:18.210648+00:00, run_end_date=2023-09-11 08:06:18.399339+00:00, run_duration=0.188691, state=success, executor_state=success, try_number=1, max_tries=0, job_id=266, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:15.186335+00:00, queued_by_job_id=2, pid=49871[0m
[[34m2023-09-11T08:06:18.973+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-09T00:00:00+00:00, run_after=2023-01-10T00:00:00+00:00[0m
[[34m2023-09-11T08:06:19.011+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-07 00:00:00+00:00: scheduled__2023-01-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:15.126045+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:19.011+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-07 00:00:00+00:00, run_id=scheduled__2023-01-07T00:00:00+00:00, run_start_date=2023-09-11 08:06:15.146538+00:00, run_end_date=2023-09-11 08:06:19.011482+00:00, run_duration=3.864944, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-07 00:00:00+00:00, data_interval_end=2023-01-08 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:19.015+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-08T00:00:00+00:00, run_after=2023-01-09T00:00:00+00:00[0m
[[34m2023-09-11T08:06:19.032+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:19.032+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:19.032+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:19.035+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:19.035+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:19.035+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:19.038+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:20.866+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:21.016+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:21.215+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:21.251+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:21.759+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:21.760+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:21.833+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-08T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:21.892+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:22.949+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:22.962+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:21.981587+00:00, run_end_date=2023-09-11 08:06:22.423176+00:00, run_duration=0.441589, state=success, executor_state=success, try_number=1, max_tries=0, job_id=267, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:19.033579+00:00, queued_by_job_id=2, pid=49878[0m
[[34m2023-09-11T08:06:23.258+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-09T00:00:00+00:00, run_after=2023-01-10T00:00:00+00:00[0m
[[34m2023-09-11T08:06:23.290+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-08 00:00:00+00:00: scheduled__2023-01-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:18.967829+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:23.291+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-08 00:00:00+00:00, run_id=scheduled__2023-01-08T00:00:00+00:00, run_start_date=2023-09-11 08:06:18.986956+00:00, run_end_date=2023-09-11 08:06:23.290910+00:00, run_duration=4.303954, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-08 00:00:00+00:00, data_interval_end=2023-01-09 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:23.296+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-09T00:00:00+00:00, run_after=2023-01-10T00:00:00+00:00[0m
[[34m2023-09-11T08:06:24.023+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-10T00:00:00+00:00, run_after=2023-01-11T00:00:00+00:00[0m
[[34m2023-09-11T08:06:24.099+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:24.100+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:24.100+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:24.103+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:24.104+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:24.104+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:24.107+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:26.857+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:27.067+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:27.330+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:27.381+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:27.942+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:27.943+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:28.025+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-09T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:28.088+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:28.877+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:28.891+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:28.211977+00:00, run_end_date=2023-09-11 08:06:28.423965+00:00, run_duration=0.211988, state=success, executor_state=success, try_number=1, max_tries=0, job_id=268, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:24.101859+00:00, queued_by_job_id=2, pid=49888[0m
[[34m2023-09-11T08:06:29.084+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-11T00:00:00+00:00, run_after=2023-01-12T00:00:00+00:00[0m
[[34m2023-09-11T08:06:29.126+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-09 00:00:00+00:00: scheduled__2023-01-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:24.015980+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:29.127+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-09 00:00:00+00:00, run_id=scheduled__2023-01-09T00:00:00+00:00, run_start_date=2023-09-11 08:06:24.044419+00:00, run_end_date=2023-09-11 08:06:29.127202+00:00, run_duration=5.082783, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-09 00:00:00+00:00, data_interval_end=2023-01-10 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:29.133+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-10T00:00:00+00:00, run_after=2023-01-11T00:00:00+00:00[0m
[[34m2023-09-11T08:06:29.160+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:29.161+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:29.161+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:29.166+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:29.167+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:29.168+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:29.173+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:31.497+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:31.639+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:31.815+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:31.853+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:32.346+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:32.347+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:32.431+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-10T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:32.497+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:33.299+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:33.310+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:32.610115+00:00, run_end_date=2023-09-11 08:06:32.849444+00:00, run_duration=0.239329, state=success, executor_state=success, try_number=1, max_tries=0, job_id=269, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:29.163255+00:00, queued_by_job_id=2, pid=49893[0m
[[34m2023-09-11T08:06:33.568+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-11T00:00:00+00:00, run_after=2023-01-12T00:00:00+00:00[0m
[[34m2023-09-11T08:06:33.592+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-10 00:00:00+00:00: scheduled__2023-01-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:29.078182+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:33.592+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-10 00:00:00+00:00, run_id=scheduled__2023-01-10T00:00:00+00:00, run_start_date=2023-09-11 08:06:29.099681+00:00, run_end_date=2023-09-11 08:06:33.592561+00:00, run_duration=4.49288, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-10 00:00:00+00:00, data_interval_end=2023-01-11 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:33.597+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-11T00:00:00+00:00, run_after=2023-01-12T00:00:00+00:00[0m
[[34m2023-09-11T08:06:34.162+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-12T00:00:00+00:00, run_after=2023-01-13T00:00:00+00:00[0m
[[34m2023-09-11T08:06:34.209+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:34.210+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:34.210+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:34.212+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:34.213+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:34.213+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:34.217+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:36.510+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:36.653+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:36.843+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:36.880+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:37.497+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:37.498+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:37.577+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-11T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:37.636+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:38.372+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:38.384+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:37.726070+00:00, run_end_date=2023-09-11 08:06:37.952399+00:00, run_duration=0.226329, state=success, executor_state=success, try_number=1, max_tries=0, job_id=270, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:34.211036+00:00, queued_by_job_id=2, pid=49902[0m
[[34m2023-09-11T08:06:38.660+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-13T00:00:00+00:00, run_after=2023-01-14T00:00:00+00:00[0m
[[34m2023-09-11T08:06:38.701+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-11 00:00:00+00:00: scheduled__2023-01-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:34.157216+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:38.701+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-11 00:00:00+00:00, run_id=scheduled__2023-01-11T00:00:00+00:00, run_start_date=2023-09-11 08:06:34.174880+00:00, run_end_date=2023-09-11 08:06:38.701508+00:00, run_duration=4.526628, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-11 00:00:00+00:00, data_interval_end=2023-01-12 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:38.704+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-12T00:00:00+00:00, run_after=2023-01-13T00:00:00+00:00[0m
[[34m2023-09-11T08:06:38.724+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:38.724+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:38.724+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:38.726+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:38.727+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:38.727+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:38.731+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:40.690+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:40.825+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:41.013+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:41.046+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:41.530+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:41.531+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:41.614+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-12T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:41.675+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:42.482+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:42.496+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:41.805992+00:00, run_end_date=2023-09-11 08:06:42.048107+00:00, run_duration=0.242115, state=success, executor_state=success, try_number=1, max_tries=0, job_id=271, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:38.725365+00:00, queued_by_job_id=2, pid=49908[0m
[[34m2023-09-11T08:06:42.803+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-13T00:00:00+00:00, run_after=2023-01-14T00:00:00+00:00[0m
[[34m2023-09-11T08:06:42.861+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-12 00:00:00+00:00: scheduled__2023-01-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:38.646851+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:42.862+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-12 00:00:00+00:00, run_id=scheduled__2023-01-12T00:00:00+00:00, run_start_date=2023-09-11 08:06:38.674264+00:00, run_end_date=2023-09-11 08:06:42.862161+00:00, run_duration=4.187897, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-12 00:00:00+00:00, data_interval_end=2023-01-13 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:42.867+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-13T00:00:00+00:00, run_after=2023-01-14T00:00:00+00:00[0m
[[34m2023-09-11T08:06:43.603+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-14T00:00:00+00:00, run_after=2023-01-15T00:00:00+00:00[0m
[[34m2023-09-11T08:06:43.650+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:43.650+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:43.651+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:43.652+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:43.653+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:43.653+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:43.656+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:45.613+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:45.762+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:45.951+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:45.988+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:46.496+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:46.496+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:46.569+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-13T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:46.630+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:47.358+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:47.369+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:46.720679+00:00, run_end_date=2023-09-11 08:06:46.925969+00:00, run_duration=0.20529, state=success, executor_state=success, try_number=1, max_tries=0, job_id=272, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:43.651644+00:00, queued_by_job_id=2, pid=49917[0m
[[34m2023-09-11T08:06:47.688+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-15T00:00:00+00:00, run_after=2023-01-16T00:00:00+00:00[0m
[[34m2023-09-11T08:06:47.725+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-13 00:00:00+00:00: scheduled__2023-01-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:43.599218+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:47.726+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-13 00:00:00+00:00, run_id=scheduled__2023-01-13T00:00:00+00:00, run_start_date=2023-09-11 08:06:43.616882+00:00, run_end_date=2023-09-11 08:06:47.726253+00:00, run_duration=4.109371, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-13 00:00:00+00:00, data_interval_end=2023-01-14 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:47.730+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-14T00:00:00+00:00, run_after=2023-01-15T00:00:00+00:00[0m
[[34m2023-09-11T08:06:47.787+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:47.787+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:47.788+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:47.790+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:47.790+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:47.791+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:47.794+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:49.846+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:49.990+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:50.177+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:50.211+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:50.733+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:50.734+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:50.815+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-14T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:50.876+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:51.621+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:51.634+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:50.976523+00:00, run_end_date=2023-09-11 08:06:51.185012+00:00, run_duration=0.208489, state=success, executor_state=success, try_number=1, max_tries=0, job_id=273, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:47.788827+00:00, queued_by_job_id=2, pid=49925[0m
[[34m2023-09-11T08:06:51.959+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-15T00:00:00+00:00, run_after=2023-01-16T00:00:00+00:00[0m
[[34m2023-09-11T08:06:51.986+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-14 00:00:00+00:00: scheduled__2023-01-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:47.683361+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:51.987+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-14 00:00:00+00:00, run_id=scheduled__2023-01-14T00:00:00+00:00, run_start_date=2023-09-11 08:06:47.701957+00:00, run_end_date=2023-09-11 08:06:51.986986+00:00, run_duration=4.285029, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-14 00:00:00+00:00, data_interval_end=2023-01-15 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:51.990+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-15T00:00:00+00:00, run_after=2023-01-16T00:00:00+00:00[0m
[[34m2023-09-11T08:06:52.674+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-16T00:00:00+00:00, run_after=2023-01-17T00:00:00+00:00[0m
[[34m2023-09-11T08:06:52.745+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:52.746+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:52.746+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:52.749+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:52.749+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:52.750+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:52.753+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:54.804+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:54.961+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:55.148+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:55.184+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:55.672+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:06:55.672+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:55.748+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-15T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:06:55.805+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:06:56.664+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:06:56.675+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:06:55.922309+00:00, run_end_date=2023-09-11 08:06:56.146201+00:00, run_duration=0.223892, state=success, executor_state=success, try_number=1, max_tries=0, job_id=274, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:52.747198+00:00, queued_by_job_id=2, pid=49934[0m
[[34m2023-09-11T08:06:56.937+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-17T00:00:00+00:00, run_after=2023-01-18T00:00:00+00:00[0m
[[34m2023-09-11T08:06:56.983+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-15 00:00:00+00:00: scheduled__2023-01-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:52.669909+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:06:56.984+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-15 00:00:00+00:00, run_id=scheduled__2023-01-15T00:00:00+00:00, run_start_date=2023-09-11 08:06:52.711561+00:00, run_end_date=2023-09-11 08:06:56.984185+00:00, run_duration=4.272624, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-15 00:00:00+00:00, data_interval_end=2023-01-16 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:06:56.987+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-16T00:00:00+00:00, run_after=2023-01-17T00:00:00+00:00[0m
[[34m2023-09-11T08:06:57.004+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:57.004+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:06:57.004+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:06:57.007+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:06:57.007+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:06:57.007+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:57.010+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:06:59.196+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:06:59.371+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:06:59.567+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:06:59.600+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:00.124+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:00.125+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:00.199+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-16T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:00.264+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:00.999+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:01.011+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:00.354268+00:00, run_end_date=2023-09-11 08:07:00.566129+00:00, run_duration=0.211861, state=success, executor_state=success, try_number=1, max_tries=0, job_id=275, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:06:57.005564+00:00, queued_by_job_id=2, pid=49942[0m
[[34m2023-09-11T08:07:01.267+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-17T00:00:00+00:00, run_after=2023-01-18T00:00:00+00:00[0m
[[34m2023-09-11T08:07:01.290+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-16 00:00:00+00:00: scheduled__2023-01-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:06:56.932698+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:01.290+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-16 00:00:00+00:00, run_id=scheduled__2023-01-16T00:00:00+00:00, run_start_date=2023-09-11 08:06:56.953777+00:00, run_end_date=2023-09-11 08:07:01.290625+00:00, run_duration=4.336848, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-16 00:00:00+00:00, data_interval_end=2023-01-17 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:01.294+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-17T00:00:00+00:00, run_after=2023-01-18T00:00:00+00:00[0m
[[34m2023-09-11T08:07:01.946+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-18T00:00:00+00:00, run_after=2023-01-19T00:00:00+00:00[0m
[[34m2023-09-11T08:07:01.991+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:01.991+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:01.992+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:01.994+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:01.995+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:01.996+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:01.998+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:04.126+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:04.302+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:04.501+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:04.543+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:05.059+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:05.060+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:05.137+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-17T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:05.198+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:05.976+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:05.988+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:05.287642+00:00, run_end_date=2023-09-11 08:07:05.489247+00:00, run_duration=0.201605, state=success, executor_state=success, try_number=1, max_tries=0, job_id=276, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:01.992738+00:00, queued_by_job_id=2, pid=49951[0m
[[34m2023-09-11T08:07:06.259+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-19T00:00:00+00:00, run_after=2023-01-20T00:00:00+00:00[0m
[[34m2023-09-11T08:07:06.297+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-17 00:00:00+00:00: scheduled__2023-01-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:01.941436+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:06.298+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-17 00:00:00+00:00, run_id=scheduled__2023-01-17T00:00:00+00:00, run_start_date=2023-09-11 08:07:01.958331+00:00, run_end_date=2023-09-11 08:07:06.298203+00:00, run_duration=4.339872, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-17 00:00:00+00:00, data_interval_end=2023-01-18 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:06.301+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-18T00:00:00+00:00, run_after=2023-01-19T00:00:00+00:00[0m
[[34m2023-09-11T08:07:06.317+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:06.318+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:06.318+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:06.320+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:06.321+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:06.321+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:06.324+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:08.318+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:08.456+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:08.636+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:08.671+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:09.230+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:09.231+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:09.304+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-18T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:09.366+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:10.353+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:10.370+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:09.521784+00:00, run_end_date=2023-09-11 08:07:09.852887+00:00, run_duration=0.331103, state=success, executor_state=success, try_number=1, max_tries=0, job_id=277, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:06.319179+00:00, queued_by_job_id=2, pid=49959[0m
[[34m2023-09-11T08:07:10.874+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-19T00:00:00+00:00, run_after=2023-01-20T00:00:00+00:00[0m
[[34m2023-09-11T08:07:10.902+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-18 00:00:00+00:00: scheduled__2023-01-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:06.254647+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:10.902+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-18 00:00:00+00:00, run_id=scheduled__2023-01-18T00:00:00+00:00, run_start_date=2023-09-11 08:07:06.273192+00:00, run_end_date=2023-09-11 08:07:10.902654+00:00, run_duration=4.629462, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-18 00:00:00+00:00, data_interval_end=2023-01-19 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:10.906+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-19T00:00:00+00:00, run_after=2023-01-20T00:00:00+00:00[0m
[[34m2023-09-11T08:07:11.395+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-20T00:00:00+00:00, run_after=2023-01-21T00:00:00+00:00[0m
[[34m2023-09-11T08:07:11.484+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:11.484+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:11.485+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:11.487+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:11.487+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:11.488+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:11.490+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:13.950+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:14.146+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:14.416+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:14.456+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:15.047+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:15.047+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:15.132+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-19T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:15.203+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:16.015+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:16.027+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:15.301629+00:00, run_end_date=2023-09-11 08:07:15.527031+00:00, run_duration=0.225402, state=success, executor_state=success, try_number=1, max_tries=0, job_id=278, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:11.485869+00:00, queued_by_job_id=2, pid=49968[0m
[[34m2023-09-11T08:07:16.299+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-21T00:00:00+00:00, run_after=2023-01-22T00:00:00+00:00[0m
[[34m2023-09-11T08:07:16.339+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-19 00:00:00+00:00: scheduled__2023-01-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:11.388314+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:16.340+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-19 00:00:00+00:00, run_id=scheduled__2023-01-19T00:00:00+00:00, run_start_date=2023-09-11 08:07:11.444150+00:00, run_end_date=2023-09-11 08:07:16.340130+00:00, run_duration=4.89598, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-19 00:00:00+00:00, data_interval_end=2023-01-20 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:16.344+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-20T00:00:00+00:00, run_after=2023-01-21T00:00:00+00:00[0m
[[34m2023-09-11T08:07:16.367+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:16.367+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:16.367+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:16.370+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:16.370+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:16.371+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:16.373+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:18.751+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:18.916+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:19.129+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:19.170+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:19.715+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:19.716+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:19.801+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-20T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:19.871+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:20.638+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:20.649+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:19.969302+00:00, run_end_date=2023-09-11 08:07:20.206775+00:00, run_duration=0.237473, state=success, executor_state=success, try_number=1, max_tries=0, job_id=279, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:16.368602+00:00, queued_by_job_id=2, pid=49976[0m
[[34m2023-09-11T08:07:20.915+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-21T00:00:00+00:00, run_after=2023-01-22T00:00:00+00:00[0m
[[34m2023-09-11T08:07:20.941+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-20 00:00:00+00:00: scheduled__2023-01-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:16.291823+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:20.941+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-20 00:00:00+00:00, run_id=scheduled__2023-01-20T00:00:00+00:00, run_start_date=2023-09-11 08:07:16.314616+00:00, run_end_date=2023-09-11 08:07:20.941722+00:00, run_duration=4.627106, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-20 00:00:00+00:00, data_interval_end=2023-01-21 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:20.945+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-21T00:00:00+00:00, run_after=2023-01-22T00:00:00+00:00[0m
[[34m2023-09-11T08:07:21.198+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-22T00:00:00+00:00, run_after=2023-01-23T00:00:00+00:00[0m
[[34m2023-09-11T08:07:21.249+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:21.250+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:21.250+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:21.252+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:21.253+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:21.253+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:21.255+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:23.466+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:23.722+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:23.969+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:24.011+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:24.563+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:24.563+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:24.651+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-21T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:24.717+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:25.537+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:25.549+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:24.815852+00:00, run_end_date=2023-09-11 08:07:25.077233+00:00, run_duration=0.261381, state=success, executor_state=success, try_number=1, max_tries=0, job_id=280, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:21.251138+00:00, queued_by_job_id=2, pid=49985[0m
[[34m2023-09-11T08:07:25.839+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-23T00:00:00+00:00, run_after=2023-01-24T00:00:00+00:00[0m
[[34m2023-09-11T08:07:25.879+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-21 00:00:00+00:00: scheduled__2023-01-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:21.193206+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:25.879+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-21 00:00:00+00:00, run_id=scheduled__2023-01-21T00:00:00+00:00, run_start_date=2023-09-11 08:07:21.213449+00:00, run_end_date=2023-09-11 08:07:25.879433+00:00, run_duration=4.665984, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-21 00:00:00+00:00, data_interval_end=2023-01-22 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:25.883+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-22T00:00:00+00:00, run_after=2023-01-23T00:00:00+00:00[0m
[[34m2023-09-11T08:07:25.905+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:25.905+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:25.906+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:25.908+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:25.909+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:25.910+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:25.912+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:28.123+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:28.279+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:28.499+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:28.535+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:29.096+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:29.097+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:29.194+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-22T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:29.260+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:30.153+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:30.167+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:29.361800+00:00, run_end_date=2023-09-11 08:07:29.615975+00:00, run_duration=0.254175, state=success, executor_state=success, try_number=1, max_tries=0, job_id=281, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:25.906857+00:00, queued_by_job_id=2, pid=49993[0m
[[34m2023-09-11T08:07:30.522+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-23T00:00:00+00:00, run_after=2023-01-24T00:00:00+00:00[0m
[[34m2023-09-11T08:07:30.551+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-22 00:00:00+00:00: scheduled__2023-01-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:25.835015+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:30.551+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-22 00:00:00+00:00, run_id=scheduled__2023-01-22T00:00:00+00:00, run_start_date=2023-09-11 08:07:25.852509+00:00, run_end_date=2023-09-11 08:07:30.551526+00:00, run_duration=4.699017, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-22 00:00:00+00:00, data_interval_end=2023-01-23 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:30.556+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-23T00:00:00+00:00, run_after=2023-01-24T00:00:00+00:00[0m
[[34m2023-09-11T08:07:32.102+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-24T00:00:00+00:00, run_after=2023-01-25T00:00:00+00:00[0m
[[34m2023-09-11T08:07:32.154+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:32.155+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:32.155+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:32.158+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:32.159+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:32.159+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:32.162+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:34.528+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:34.678+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:34.878+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:34.921+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:35.602+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:35.602+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:35.682+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-23T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:35.752+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:36.662+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:36.677+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:35.854066+00:00, run_end_date=2023-09-11 08:07:36.087787+00:00, run_duration=0.233721, state=success, executor_state=success, try_number=1, max_tries=0, job_id=282, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:32.156757+00:00, queued_by_job_id=2, pid=50002[0m
[[34m2023-09-11T08:07:36.950+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-25T00:00:00+00:00, run_after=2023-01-26T00:00:00+00:00[0m
[[34m2023-09-11T08:07:37.035+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-23 00:00:00+00:00: scheduled__2023-01-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:32.097033+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:37.036+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-23 00:00:00+00:00, run_id=scheduled__2023-01-23T00:00:00+00:00, run_start_date=2023-09-11 08:07:32.117568+00:00, run_end_date=2023-09-11 08:07:37.036217+00:00, run_duration=4.918649, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-23 00:00:00+00:00, data_interval_end=2023-01-24 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:37.041+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-24T00:00:00+00:00, run_after=2023-01-25T00:00:00+00:00[0m
[[34m2023-09-11T08:07:37.065+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:37.065+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:37.066+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:37.068+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:37.069+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:37.070+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:37.073+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:39.448+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:39.596+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:39.797+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:39.836+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:40.378+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:40.378+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:40.464+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-24T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:40.532+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:41.348+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:41.359+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:40.635217+00:00, run_end_date=2023-09-11 08:07:40.838499+00:00, run_duration=0.203282, state=success, executor_state=success, try_number=1, max_tries=0, job_id=283, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:37.066990+00:00, queued_by_job_id=2, pid=50010[0m
[[34m2023-09-11T08:07:41.626+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-25T00:00:00+00:00, run_after=2023-01-26T00:00:00+00:00[0m
[[34m2023-09-11T08:07:41.654+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-24 00:00:00+00:00: scheduled__2023-01-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:36.944252+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:41.655+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-24 00:00:00+00:00, run_id=scheduled__2023-01-24T00:00:00+00:00, run_start_date=2023-09-11 08:07:36.997054+00:00, run_end_date=2023-09-11 08:07:41.655340+00:00, run_duration=4.658286, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-24 00:00:00+00:00, data_interval_end=2023-01-25 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:41.660+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-25T00:00:00+00:00, run_after=2023-01-26T00:00:00+00:00[0m
[[34m2023-09-11T08:07:42.054+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-26T00:00:00+00:00, run_after=2023-01-27T00:00:00+00:00[0m
[[34m2023-09-11T08:07:42.129+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:42.129+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:42.130+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:42.132+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:42.133+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:42.133+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:42.135+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:44.414+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:44.568+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:44.783+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:44.878+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:45.582+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:45.583+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:45.671+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-25T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:45.740+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:46.511+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:46.522+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:45.849537+00:00, run_end_date=2023-09-11 08:07:46.064802+00:00, run_duration=0.215265, state=success, executor_state=success, try_number=1, max_tries=0, job_id=284, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:42.130886+00:00, queued_by_job_id=2, pid=50019[0m
[[34m2023-09-11T08:07:46.824+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-27T00:00:00+00:00, run_after=2023-01-28T00:00:00+00:00[0m
[[34m2023-09-11T08:07:46.862+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-25 00:00:00+00:00: scheduled__2023-01-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:42.048502+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:46.863+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-25 00:00:00+00:00, run_id=scheduled__2023-01-25T00:00:00+00:00, run_start_date=2023-09-11 08:07:42.070096+00:00, run_end_date=2023-09-11 08:07:46.863094+00:00, run_duration=4.792998, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-25 00:00:00+00:00, data_interval_end=2023-01-26 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:46.867+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-26T00:00:00+00:00, run_after=2023-01-27T00:00:00+00:00[0m
[[34m2023-09-11T08:07:46.886+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:46.887+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:46.887+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:46.889+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:46.890+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:46.891+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:46.898+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:49.180+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:49.348+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:49.616+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:49.651+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:50.273+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:50.274+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:50.367+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-26T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:50.436+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:51.332+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:51.350+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:50.537575+00:00, run_end_date=2023-09-11 08:07:50.777158+00:00, run_duration=0.239583, state=success, executor_state=success, try_number=1, max_tries=0, job_id=285, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:46.888131+00:00, queued_by_job_id=2, pid=50027[0m
[[34m2023-09-11T08:07:51.711+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-27T00:00:00+00:00, run_after=2023-01-28T00:00:00+00:00[0m
[[34m2023-09-11T08:07:51.743+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-26 00:00:00+00:00: scheduled__2023-01-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:46.819015+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:51.744+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-26 00:00:00+00:00, run_id=scheduled__2023-01-26T00:00:00+00:00, run_start_date=2023-09-11 08:07:46.837204+00:00, run_end_date=2023-09-11 08:07:51.744369+00:00, run_duration=4.907165, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-26 00:00:00+00:00, data_interval_end=2023-01-27 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:51.749+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-27T00:00:00+00:00, run_after=2023-01-28T00:00:00+00:00[0m
[[34m2023-09-11T08:07:52.975+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-28T00:00:00+00:00, run_after=2023-01-29T00:00:00+00:00[0m
[[34m2023-09-11T08:07:53.040+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:53.041+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:53.041+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:53.043+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:53.044+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:53.044+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:53.046+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:55.132+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:55.273+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:55.477+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:55.516+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:56.039+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:07:56.040+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:56.120+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-27T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:07:56.184+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:07:56.941+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:07:56.952+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:07:56.280670+00:00, run_end_date=2023-09-11 08:07:56.487632+00:00, run_duration=0.206962, state=success, executor_state=success, try_number=1, max_tries=0, job_id=286, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:53.042318+00:00, queued_by_job_id=2, pid=50035[0m
[[34m2023-09-11T08:07:57.343+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-29T00:00:00+00:00, run_after=2023-01-30T00:00:00+00:00[0m
[[34m2023-09-11T08:07:57.381+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-27 00:00:00+00:00: scheduled__2023-01-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:52.970233+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:07:57.381+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-27 00:00:00+00:00, run_id=scheduled__2023-01-27T00:00:00+00:00, run_start_date=2023-09-11 08:07:52.987504+00:00, run_end_date=2023-09-11 08:07:57.381616+00:00, run_duration=4.394112, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-27 00:00:00+00:00, data_interval_end=2023-01-28 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:07:57.385+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-28T00:00:00+00:00, run_after=2023-01-29T00:00:00+00:00[0m
[[34m2023-09-11T08:07:57.409+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:57.409+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:07:57.409+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:07:57.412+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:07:57.413+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:07:57.413+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:57.416+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:07:59.496+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:07:59.652+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:07:59.848+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:07:59.882+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:00.427+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:00.427+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:00.514+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-28T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:00.575+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:01.311+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:01.324+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:00.668228+00:00, run_end_date=2023-09-11 08:08:00.872980+00:00, run_duration=0.204752, state=success, executor_state=success, try_number=1, max_tries=0, job_id=287, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:07:57.410945+00:00, queued_by_job_id=2, pid=50043[0m
[[34m2023-09-11T08:08:01.576+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-30T00:00:00+00:00, run_after=2023-01-31T00:00:00+00:00[0m
[[34m2023-09-11T08:08:01.613+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-28 00:00:00+00:00: scheduled__2023-01-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:07:57.337373+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:01.613+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-28 00:00:00+00:00, run_id=scheduled__2023-01-28T00:00:00+00:00, run_start_date=2023-09-11 08:07:57.355836+00:00, run_end_date=2023-09-11 08:08:01.613814+00:00, run_duration=4.257978, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-28 00:00:00+00:00, data_interval_end=2023-01-29 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:01.617+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-29T00:00:00+00:00, run_after=2023-01-30T00:00:00+00:00[0m
[[34m2023-09-11T08:08:01.633+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:01.633+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:01.633+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:01.636+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:01.636+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:01.636+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:01.639+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:03.877+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:04.031+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:04.223+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:04.268+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:04.788+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:04.789+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:04.871+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-29T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:04.940+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:05.707+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:05.719+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:05.036423+00:00, run_end_date=2023-09-11 08:08:05.252303+00:00, run_duration=0.21588, state=success, executor_state=success, try_number=1, max_tries=0, job_id=288, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:01.634534+00:00, queued_by_job_id=2, pid=50051[0m
[[34m2023-09-11T08:08:05.978+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-30T00:00:00+00:00, run_after=2023-01-31T00:00:00+00:00[0m
[[34m2023-09-11T08:08:06.002+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-29 00:00:00+00:00: scheduled__2023-01-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:01.570690+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:06.002+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-29 00:00:00+00:00, run_id=scheduled__2023-01-29T00:00:00+00:00, run_start_date=2023-09-11 08:08:01.589400+00:00, run_end_date=2023-09-11 08:08:06.002480+00:00, run_duration=4.41308, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-29 00:00:00+00:00, data_interval_end=2023-01-30 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:06.005+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-30T00:00:00+00:00, run_after=2023-01-31T00:00:00+00:00[0m
[[34m2023-09-11T08:08:07.285+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-31T00:00:00+00:00, run_after=2023-02-01T00:00:00+00:00[0m
[[34m2023-09-11T08:08:07.331+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:07.331+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:07.332+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:07.335+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:07.335+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:07.336+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:07.338+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:09.478+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:09.660+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:09.855+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:09.896+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:10.501+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:10.502+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:10.586+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-30T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:10.659+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:11.508+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:11.519+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:10.758556+00:00, run_end_date=2023-09-11 08:08:11.002964+00:00, run_duration=0.244408, state=success, executor_state=success, try_number=1, max_tries=0, job_id=289, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:07.333141+00:00, queued_by_job_id=2, pid=50060[0m
[[34m2023-09-11T08:08:11.779+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-01T00:00:00+00:00, run_after=2023-02-02T00:00:00+00:00[0m
[[34m2023-09-11T08:08:11.817+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-30 00:00:00+00:00: scheduled__2023-01-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:07.278887+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:11.817+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-30 00:00:00+00:00, run_id=scheduled__2023-01-30T00:00:00+00:00, run_start_date=2023-09-11 08:08:07.297340+00:00, run_end_date=2023-09-11 08:08:11.817788+00:00, run_duration=4.520448, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-30 00:00:00+00:00, data_interval_end=2023-01-31 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:11.821+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-01-31T00:00:00+00:00, run_after=2023-02-01T00:00:00+00:00[0m
[[34m2023-09-11T08:08:11.839+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-01-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:11.840+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:11.840+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-01-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:11.843+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:11.844+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:11.844+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:11.846+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-01-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:14.220+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:14.391+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:14.601+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:14.643+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:15.156+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:15.158+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:15.254+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-01-31T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:15.311+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-01-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:16.107+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-01-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:16.122+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-01-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:15.394343+00:00, run_end_date=2023-09-11 08:08:15.667825+00:00, run_duration=0.273482, state=success, executor_state=success, try_number=1, max_tries=0, job_id=290, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:11.842012+00:00, queued_by_job_id=2, pid=50068[0m
[[34m2023-09-11T08:08:16.382+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-01T00:00:00+00:00, run_after=2023-02-02T00:00:00+00:00[0m
[[34m2023-09-11T08:08:16.415+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-01-31 00:00:00+00:00: scheduled__2023-01-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:11.772980+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:16.415+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-01-31 00:00:00+00:00, run_id=scheduled__2023-01-31T00:00:00+00:00, run_start_date=2023-09-11 08:08:11.793625+00:00, run_end_date=2023-09-11 08:08:16.415680+00:00, run_duration=4.622055, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-31 00:00:00+00:00, data_interval_end=2023-02-01 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:16.419+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-01T00:00:00+00:00, run_after=2023-02-02T00:00:00+00:00[0m
[[34m2023-09-11T08:08:16.761+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-02T00:00:00+00:00, run_after=2023-02-03T00:00:00+00:00[0m
[[34m2023-09-11T08:08:16.831+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:16.831+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:16.832+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:16.835+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:16.836+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:16.836+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:16.839+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:19.059+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:19.188+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:19.361+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:19.398+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:20.021+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:20.022+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:20.094+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-01T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:20.156+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:21.086+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:21.099+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:20.241989+00:00, run_end_date=2023-09-11 08:08:20.582649+00:00, run_duration=0.34066, state=success, executor_state=success, try_number=1, max_tries=0, job_id=291, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:16.833242+00:00, queued_by_job_id=2, pid=50076[0m
[[34m2023-09-11T08:08:21.362+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-03T00:00:00+00:00, run_after=2023-02-04T00:00:00+00:00[0m
[[34m2023-09-11T08:08:21.413+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-01 00:00:00+00:00: scheduled__2023-02-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:16.754371+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:21.414+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-01 00:00:00+00:00, run_id=scheduled__2023-02-01T00:00:00+00:00, run_start_date=2023-09-11 08:08:16.779642+00:00, run_end_date=2023-09-11 08:08:21.414211+00:00, run_duration=4.634569, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-01 00:00:00+00:00, data_interval_end=2023-02-02 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:21.418+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-02T00:00:00+00:00, run_after=2023-02-03T00:00:00+00:00[0m
[[34m2023-09-11T08:08:21.436+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:21.437+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:21.437+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:21.439+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:21.440+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:21.441+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:21.443+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:23.529+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:23.669+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:23.863+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:23.904+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:24.447+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:24.448+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:24.528+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-02T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:24.597+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:25.344+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:25.355+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:24.694093+00:00, run_end_date=2023-09-11 08:08:24.913592+00:00, run_duration=0.219499, state=success, executor_state=success, try_number=1, max_tries=0, job_id=292, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:21.438248+00:00, queued_by_job_id=2, pid=50084[0m
[[34m2023-09-11T08:08:25.511+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-03T00:00:00+00:00, run_after=2023-02-04T00:00:00+00:00[0m
[[34m2023-09-11T08:08:25.536+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-02 00:00:00+00:00: scheduled__2023-02-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:21.355666+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:25.536+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-02 00:00:00+00:00, run_id=scheduled__2023-02-02T00:00:00+00:00, run_start_date=2023-09-11 08:08:21.377114+00:00, run_end_date=2023-09-11 08:08:25.536764+00:00, run_duration=4.15965, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-02 00:00:00+00:00, data_interval_end=2023-02-03 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:25.541+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-03T00:00:00+00:00, run_after=2023-02-04T00:00:00+00:00[0m
[[34m2023-09-11T08:08:27.459+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-04T00:00:00+00:00, run_after=2023-02-05T00:00:00+00:00[0m
[[34m2023-09-11T08:08:27.507+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:27.507+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:27.507+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:27.510+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:27.510+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:27.510+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:27.513+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:29.615+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:29.757+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:29.969+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:30.011+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:30.700+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:30.701+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:30.776+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-03T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:30.844+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:31.630+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:31.642+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:30.940484+00:00, run_end_date=2023-09-11 08:08:31.157238+00:00, run_duration=0.216754, state=success, executor_state=success, try_number=1, max_tries=0, job_id=293, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:27.508466+00:00, queued_by_job_id=2, pid=50093[0m
[[34m2023-09-11T08:08:31.869+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-05T00:00:00+00:00, run_after=2023-02-06T00:00:00+00:00[0m
[[34m2023-09-11T08:08:31.909+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-03 00:00:00+00:00: scheduled__2023-02-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:27.454348+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:31.909+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-03 00:00:00+00:00, run_id=scheduled__2023-02-03T00:00:00+00:00, run_start_date=2023-09-11 08:08:27.474004+00:00, run_end_date=2023-09-11 08:08:31.909852+00:00, run_duration=4.435848, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-03 00:00:00+00:00, data_interval_end=2023-02-04 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:31.913+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-04T00:00:00+00:00, run_after=2023-02-05T00:00:00+00:00[0m
[[34m2023-09-11T08:08:31.930+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:31.930+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:31.930+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:31.933+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:31.933+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:31.933+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:31.936+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:34.046+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:34.202+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:34.389+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:34.433+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:34.981+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:34.982+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:35.056+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-04T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:35.128+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:35.877+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:35.888+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:35.224399+00:00, run_end_date=2023-09-11 08:08:35.460432+00:00, run_duration=0.236033, state=success, executor_state=success, try_number=1, max_tries=0, job_id=294, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:31.931457+00:00, queued_by_job_id=2, pid=50101[0m
[[34m2023-09-11T08:08:36.217+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-05T00:00:00+00:00, run_after=2023-02-06T00:00:00+00:00[0m
[[34m2023-09-11T08:08:36.243+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-04 00:00:00+00:00: scheduled__2023-02-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:31.863541+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:36.244+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-04 00:00:00+00:00, run_id=scheduled__2023-02-04T00:00:00+00:00, run_start_date=2023-09-11 08:08:31.882546+00:00, run_end_date=2023-09-11 08:08:36.244156+00:00, run_duration=4.36161, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-04 00:00:00+00:00, data_interval_end=2023-02-05 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:36.247+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-05T00:00:00+00:00, run_after=2023-02-06T00:00:00+00:00[0m
[[34m2023-09-11T08:08:36.979+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-06T00:00:00+00:00, run_after=2023-02-07T00:00:00+00:00[0m
[[34m2023-09-11T08:08:37.032+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:37.032+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:37.033+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:37.035+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:37.035+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:37.036+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:37.059+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:39.148+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:39.311+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:39.516+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:39.555+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:40.084+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:40.084+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:40.169+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-05T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:40.229+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:40.986+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:40.998+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:40.327470+00:00, run_end_date=2023-09-11 08:08:40.537822+00:00, run_duration=0.210352, state=success, executor_state=success, try_number=1, max_tries=0, job_id=295, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:37.033781+00:00, queued_by_job_id=2, pid=50110[0m
[[34m2023-09-11T08:08:41.347+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-07T00:00:00+00:00, run_after=2023-02-08T00:00:00+00:00[0m
[[34m2023-09-11T08:08:41.387+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-05 00:00:00+00:00: scheduled__2023-02-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:36.973003+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:41.388+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-05 00:00:00+00:00, run_id=scheduled__2023-02-05T00:00:00+00:00, run_start_date=2023-09-11 08:08:36.998458+00:00, run_end_date=2023-09-11 08:08:41.388370+00:00, run_duration=4.389912, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-05 00:00:00+00:00, data_interval_end=2023-02-06 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:41.393+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-06T00:00:00+00:00, run_after=2023-02-07T00:00:00+00:00[0m
[[34m2023-09-11T08:08:41.409+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:41.409+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:41.409+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:41.412+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:41.413+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:41.414+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:41.416+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:43.475+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:43.605+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:43.777+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:43.807+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:44.284+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:44.284+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:44.355+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-06T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:44.410+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:45.607+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:45.618+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:44.493692+00:00, run_end_date=2023-09-11 08:08:44.685005+00:00, run_duration=0.191313, state=success, executor_state=success, try_number=1, max_tries=0, job_id=296, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:41.410841+00:00, queued_by_job_id=2, pid=50118[0m
[[34m2023-09-11T08:08:46.256+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-07T00:00:00+00:00, run_after=2023-02-08T00:00:00+00:00[0m
[[34m2023-09-11T08:08:46.278+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-06 00:00:00+00:00: scheduled__2023-02-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:41.342850+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:46.278+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-06 00:00:00+00:00, run_id=scheduled__2023-02-06T00:00:00+00:00, run_start_date=2023-09-11 08:08:41.361542+00:00, run_end_date=2023-09-11 08:08:46.278412+00:00, run_duration=4.91687, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-06 00:00:00+00:00, data_interval_end=2023-02-07 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:46.281+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-07T00:00:00+00:00, run_after=2023-02-08T00:00:00+00:00[0m
[[34m2023-09-11T08:08:46.965+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-08T00:00:00+00:00, run_after=2023-02-09T00:00:00+00:00[0m
[[34m2023-09-11T08:08:47.008+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:47.008+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:47.008+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:47.010+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:47.011+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:47.011+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:47.014+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:48.827+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:48.953+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:49.124+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:49.157+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:49.597+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:49.597+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:49.670+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-07T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:49.725+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:50.366+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:50.378+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:49.807314+00:00, run_end_date=2023-09-11 08:08:50.002883+00:00, run_duration=0.195569, state=success, executor_state=success, try_number=1, max_tries=0, job_id=297, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:47.009391+00:00, queued_by_job_id=2, pid=50127[0m
[[34m2023-09-11T08:08:50.625+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-09T00:00:00+00:00, run_after=2023-02-10T00:00:00+00:00[0m
[[34m2023-09-11T08:08:50.664+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-07 00:00:00+00:00: scheduled__2023-02-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:46.961143+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:50.664+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-07 00:00:00+00:00, run_id=scheduled__2023-02-07T00:00:00+00:00, run_start_date=2023-09-11 08:08:46.978364+00:00, run_end_date=2023-09-11 08:08:50.664514+00:00, run_duration=3.68615, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-07 00:00:00+00:00, data_interval_end=2023-02-08 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:50.668+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-08T00:00:00+00:00, run_after=2023-02-09T00:00:00+00:00[0m
[[34m2023-09-11T08:08:50.682+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:50.683+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:50.683+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:50.685+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:50.686+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:50.686+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:50.688+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:52.516+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:52.649+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:52.830+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:52.863+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:53.359+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:53.360+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:53.433+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-08T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:53.512+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:54.305+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:54.316+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:53.637678+00:00, run_end_date=2023-09-11 08:08:53.880932+00:00, run_duration=0.243254, state=success, executor_state=success, try_number=1, max_tries=0, job_id=298, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:50.683945+00:00, queued_by_job_id=2, pid=50135[0m
[[34m2023-09-11T08:08:54.598+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-09T00:00:00+00:00, run_after=2023-02-10T00:00:00+00:00[0m
[[34m2023-09-11T08:08:54.626+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-08 00:00:00+00:00: scheduled__2023-02-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:50.620153+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:54.627+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-08 00:00:00+00:00, run_id=scheduled__2023-02-08T00:00:00+00:00, run_start_date=2023-09-11 08:08:50.638907+00:00, run_end_date=2023-09-11 08:08:54.626953+00:00, run_duration=3.988046, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-08 00:00:00+00:00, data_interval_end=2023-02-09 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:54.630+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-09T00:00:00+00:00, run_after=2023-02-10T00:00:00+00:00[0m
[[34m2023-09-11T08:08:55.850+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-10T00:00:00+00:00, run_after=2023-02-11T00:00:00+00:00[0m
[[34m2023-09-11T08:08:55.893+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:55.894+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:55.894+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:55.896+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:55.896+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:55.897+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:55.899+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:57.828+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:08:57.951+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:58.116+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:58.150+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:08:58.605+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:08:58.606+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:08:58.674+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-09T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:08:58.729+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:08:59.371+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:08:59.382+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:08:58.810468+00:00, run_end_date=2023-09-11 08:08:59.003120+00:00, run_duration=0.192652, state=success, executor_state=success, try_number=1, max_tries=0, job_id=299, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:55.894807+00:00, queued_by_job_id=2, pid=50144[0m
[[34m2023-09-11T08:08:59.662+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-11T00:00:00+00:00, run_after=2023-02-12T00:00:00+00:00[0m
[[34m2023-09-11T08:08:59.696+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-09 00:00:00+00:00: scheduled__2023-02-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:55.845847+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:08:59.697+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-09 00:00:00+00:00, run_id=scheduled__2023-02-09T00:00:00+00:00, run_start_date=2023-09-11 08:08:55.863023+00:00, run_end_date=2023-09-11 08:08:59.697198+00:00, run_duration=3.834175, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-09 00:00:00+00:00, data_interval_end=2023-02-10 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:08:59.700+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-10T00:00:00+00:00, run_after=2023-02-11T00:00:00+00:00[0m
[[34m2023-09-11T08:08:59.715+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:59.715+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:08:59.715+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:08:59.717+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:08:59.718+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:08:59.718+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:08:59.721+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:01.718+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:01.850+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:02.031+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:02.069+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:02.550+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:02.551+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:02.629+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-10T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:02.698+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:03.470+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:03.481+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:02.798889+00:00, run_end_date=2023-09-11 08:09:03.029467+00:00, run_duration=0.230578, state=success, executor_state=success, try_number=1, max_tries=0, job_id=300, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:08:59.716320+00:00, queued_by_job_id=2, pid=50150[0m
[[34m2023-09-11T08:09:03.730+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-11T00:00:00+00:00, run_after=2023-02-12T00:00:00+00:00[0m
[[34m2023-09-11T08:09:03.756+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-10 00:00:00+00:00: scheduled__2023-02-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:08:59.657453+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:03.757+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-10 00:00:00+00:00, run_id=scheduled__2023-02-10T00:00:00+00:00, run_start_date=2023-09-11 08:08:59.674889+00:00, run_end_date=2023-09-11 08:09:03.756955+00:00, run_duration=4.082066, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-10 00:00:00+00:00, data_interval_end=2023-02-11 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:03.762+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-11T00:00:00+00:00, run_after=2023-02-12T00:00:00+00:00[0m
[[34m2023-09-11T08:09:04.654+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-12T00:00:00+00:00, run_after=2023-02-13T00:00:00+00:00[0m
[[34m2023-09-11T08:09:04.738+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:04.738+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:04.738+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:04.741+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:04.741+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:04.742+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:04.744+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:06.592+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:06.717+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:06.886+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:06.918+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:07.381+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:07.381+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:07.450+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-11T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:07.506+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:08.170+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:08.181+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:07.590855+00:00, run_end_date=2023-09-11 08:09:07.779543+00:00, run_duration=0.188688, state=success, executor_state=success, try_number=1, max_tries=0, job_id=301, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:04.739644+00:00, queued_by_job_id=2, pid=50157[0m
[[34m2023-09-11T08:09:08.449+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-13T00:00:00+00:00, run_after=2023-02-14T00:00:00+00:00[0m
[[34m2023-09-11T08:09:08.484+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-11 00:00:00+00:00: scheduled__2023-02-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:04.649845+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:08.484+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-11 00:00:00+00:00, run_id=scheduled__2023-02-11T00:00:00+00:00, run_start_date=2023-09-11 08:09:04.707268+00:00, run_end_date=2023-09-11 08:09:08.484754+00:00, run_duration=3.777486, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-11 00:00:00+00:00, data_interval_end=2023-02-12 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:08.488+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-12T00:00:00+00:00, run_after=2023-02-13T00:00:00+00:00[0m
[[34m2023-09-11T08:09:08.528+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:08.528+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:08.529+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:08.531+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:08.531+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:08.532+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:08.534+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:10.346+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:10.475+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:10.648+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:10.679+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:11.137+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:11.137+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:11.205+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-12T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:11.262+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:11.901+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:11.911+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:11.347324+00:00, run_end_date=2023-09-11 08:09:11.540296+00:00, run_duration=0.192972, state=success, executor_state=success, try_number=1, max_tries=0, job_id=302, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:08.529863+00:00, queued_by_job_id=2, pid=50165[0m
[[34m2023-09-11T08:09:12.166+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-13T00:00:00+00:00, run_after=2023-02-14T00:00:00+00:00[0m
[[34m2023-09-11T08:09:12.189+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-12 00:00:00+00:00: scheduled__2023-02-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:08.443885+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:12.189+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-12 00:00:00+00:00, run_id=scheduled__2023-02-12T00:00:00+00:00, run_start_date=2023-09-11 08:09:08.461855+00:00, run_end_date=2023-09-11 08:09:12.189362+00:00, run_duration=3.727507, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-12 00:00:00+00:00, data_interval_end=2023-02-13 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:12.192+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-13T00:00:00+00:00, run_after=2023-02-14T00:00:00+00:00[0m
[[34m2023-09-11T08:09:13.482+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-14T00:00:00+00:00, run_after=2023-02-15T00:00:00+00:00[0m
[[34m2023-09-11T08:09:13.525+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:13.526+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:13.526+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:13.528+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:13.529+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:13.529+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:13.532+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:15.311+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:15.442+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:15.607+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:15.639+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:16.097+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:16.098+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:16.166+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-13T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:16.231+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:16.930+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:16.940+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:16.314619+00:00, run_end_date=2023-09-11 08:09:16.517929+00:00, run_duration=0.20331, state=success, executor_state=success, try_number=1, max_tries=0, job_id=303, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:13.527166+00:00, queued_by_job_id=2, pid=50174[0m
[[34m2023-09-11T08:09:17.193+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-15T00:00:00+00:00, run_after=2023-02-16T00:00:00+00:00[0m
[[34m2023-09-11T08:09:17.230+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-13 00:00:00+00:00: scheduled__2023-02-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:13.477149+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:17.231+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-13 00:00:00+00:00, run_id=scheduled__2023-02-13T00:00:00+00:00, run_start_date=2023-09-11 08:09:13.494139+00:00, run_end_date=2023-09-11 08:09:17.230936+00:00, run_duration=3.736797, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-13 00:00:00+00:00, data_interval_end=2023-02-14 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:17.234+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-14T00:00:00+00:00, run_after=2023-02-15T00:00:00+00:00[0m
[[34m2023-09-11T08:09:17.249+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:17.250+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:17.250+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:17.252+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:17.252+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:17.253+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:17.255+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:19.022+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:19.157+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:19.327+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:19.359+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:19.819+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:19.819+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:19.890+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-14T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:19.952+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:20.600+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:20.610+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:20.035387+00:00, run_end_date=2023-09-11 08:09:20.229836+00:00, run_duration=0.194449, state=success, executor_state=success, try_number=1, max_tries=0, job_id=304, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:17.250847+00:00, queued_by_job_id=2, pid=50182[0m
[[34m2023-09-11T08:09:20.852+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-15T00:00:00+00:00, run_after=2023-02-16T00:00:00+00:00[0m
[[34m2023-09-11T08:09:20.876+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-14 00:00:00+00:00: scheduled__2023-02-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:17.188297+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:20.877+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-14 00:00:00+00:00, run_id=scheduled__2023-02-14T00:00:00+00:00, run_start_date=2023-09-11 08:09:17.207170+00:00, run_end_date=2023-09-11 08:09:20.877163+00:00, run_duration=3.669993, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-14 00:00:00+00:00, data_interval_end=2023-02-15 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:20.880+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-15T00:00:00+00:00, run_after=2023-02-16T00:00:00+00:00[0m
[[34m2023-09-11T08:09:22.149+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-16T00:00:00+00:00, run_after=2023-02-17T00:00:00+00:00[0m
[[34m2023-09-11T08:09:22.193+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:22.194+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:22.194+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:22.196+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:22.197+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:22.197+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:22.200+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:24.041+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:24.168+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:24.358+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:24.390+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:24.847+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:24.847+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:24.920+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-15T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:24.978+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:25.667+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:25.679+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:25.063470+00:00, run_end_date=2023-09-11 08:09:25.269127+00:00, run_duration=0.205657, state=success, executor_state=success, try_number=1, max_tries=0, job_id=305, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:22.195016+00:00, queued_by_job_id=2, pid=50191[0m
[[34m2023-09-11T08:09:25.941+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-17T00:00:00+00:00, run_after=2023-02-18T00:00:00+00:00[0m
[[34m2023-09-11T08:09:25.976+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-15 00:00:00+00:00: scheduled__2023-02-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:22.144468+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:25.977+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-15 00:00:00+00:00, run_id=scheduled__2023-02-15T00:00:00+00:00, run_start_date=2023-09-11 08:09:22.161373+00:00, run_end_date=2023-09-11 08:09:25.977200+00:00, run_duration=3.815827, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-15 00:00:00+00:00, data_interval_end=2023-02-16 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:25.980+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-16T00:00:00+00:00, run_after=2023-02-17T00:00:00+00:00[0m
[[34m2023-09-11T08:09:25.995+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:25.995+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:25.995+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:25.997+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:25.998+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:25.998+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:26.001+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:27.795+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:27.925+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:28.089+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:28.121+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:28.572+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:28.573+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:28.643+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-16T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:28.701+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:29.350+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:29.360+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:28.785831+00:00, run_end_date=2023-09-11 08:09:28.976556+00:00, run_duration=0.190725, state=success, executor_state=success, try_number=1, max_tries=0, job_id=306, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:25.996220+00:00, queued_by_job_id=2, pid=50197[0m
[[34m2023-09-11T08:09:29.710+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-17T00:00:00+00:00, run_after=2023-02-18T00:00:00+00:00[0m
[[34m2023-09-11T08:09:29.732+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-16 00:00:00+00:00: scheduled__2023-02-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:25.936849+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:29.733+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-16 00:00:00+00:00, run_id=scheduled__2023-02-16T00:00:00+00:00, run_start_date=2023-09-11 08:09:25.954589+00:00, run_end_date=2023-09-11 08:09:29.733353+00:00, run_duration=3.778764, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-16 00:00:00+00:00, data_interval_end=2023-02-17 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:29.736+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-17T00:00:00+00:00, run_after=2023-02-18T00:00:00+00:00[0m
[[34m2023-09-11T08:09:30.967+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-18T00:00:00+00:00, run_after=2023-02-19T00:00:00+00:00[0m
[[34m2023-09-11T08:09:31.011+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:31.012+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:31.012+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:31.014+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:31.015+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:31.015+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:31.018+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:32.832+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:32.967+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:33.135+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:33.165+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:33.606+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:33.607+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:33.674+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-17T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:33.729+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:34.393+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:34.410+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:33.813300+00:00, run_end_date=2023-09-11 08:09:34.021168+00:00, run_duration=0.207868, state=success, executor_state=success, try_number=1, max_tries=0, job_id=307, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:31.013080+00:00, queued_by_job_id=2, pid=50206[0m
[[34m2023-09-11T08:09:34.568+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-19T00:00:00+00:00, run_after=2023-02-20T00:00:00+00:00[0m
[[34m2023-09-11T08:09:34.603+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-17 00:00:00+00:00: scheduled__2023-02-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:30.963069+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:34.604+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-17 00:00:00+00:00, run_id=scheduled__2023-02-17T00:00:00+00:00, run_start_date=2023-09-11 08:09:30.979511+00:00, run_end_date=2023-09-11 08:09:34.604192+00:00, run_duration=3.624681, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-17 00:00:00+00:00, data_interval_end=2023-02-18 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:34.607+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-18T00:00:00+00:00, run_after=2023-02-19T00:00:00+00:00[0m
[[34m2023-09-11T08:09:34.622+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:34.623+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:34.623+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:34.625+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:34.626+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:34.626+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:34.629+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:36.417+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:36.546+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:36.715+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:36.747+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:37.199+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:37.199+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:37.268+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-18T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:37.323+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:37.997+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:38.008+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:37.418001+00:00, run_end_date=2023-09-11 08:09:37.607135+00:00, run_duration=0.189134, state=success, executor_state=success, try_number=1, max_tries=0, job_id=308, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:34.624128+00:00, queued_by_job_id=2, pid=50212[0m
[[34m2023-09-11T08:09:38.250+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-19T00:00:00+00:00, run_after=2023-02-20T00:00:00+00:00[0m
[[34m2023-09-11T08:09:38.272+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-18 00:00:00+00:00: scheduled__2023-02-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:34.563236+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:38.273+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-18 00:00:00+00:00, run_id=scheduled__2023-02-18T00:00:00+00:00, run_start_date=2023-09-11 08:09:34.580830+00:00, run_end_date=2023-09-11 08:09:38.272972+00:00, run_duration=3.692142, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-18 00:00:00+00:00, data_interval_end=2023-02-19 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:38.276+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-19T00:00:00+00:00, run_after=2023-02-20T00:00:00+00:00[0m
[[34m2023-09-11T08:09:39.530+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-20T00:00:00+00:00, run_after=2023-02-21T00:00:00+00:00[0m
[[34m2023-09-11T08:09:39.574+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:39.574+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:39.575+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:39.577+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:39.577+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:39.577+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:39.580+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:41.366+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:41.502+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:41.673+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:41.703+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:42.154+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:42.155+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:42.234+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-19T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:42.294+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:42.951+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:42.962+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:42.385050+00:00, run_end_date=2023-09-11 08:09:42.583155+00:00, run_duration=0.198105, state=success, executor_state=success, try_number=1, max_tries=0, job_id=309, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:39.575662+00:00, queued_by_job_id=2, pid=50221[0m
[[34m2023-09-11T08:09:43.222+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-21T00:00:00+00:00, run_after=2023-02-22T00:00:00+00:00[0m
[[34m2023-09-11T08:09:43.256+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-19 00:00:00+00:00: scheduled__2023-02-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:39.525945+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:43.257+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-19 00:00:00+00:00, run_id=scheduled__2023-02-19T00:00:00+00:00, run_start_date=2023-09-11 08:09:39.543100+00:00, run_end_date=2023-09-11 08:09:43.257037+00:00, run_duration=3.713937, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-19 00:00:00+00:00, data_interval_end=2023-02-20 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:43.260+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-20T00:00:00+00:00, run_after=2023-02-21T00:00:00+00:00[0m
[[34m2023-09-11T08:09:43.275+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:43.276+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:43.276+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:43.278+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:43.278+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:43.279+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:43.281+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:45.112+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:45.239+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:45.420+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:45.450+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:45.892+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:45.893+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:45.961+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-20T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:46.019+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:46.673+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:46.683+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:46.105223+00:00, run_end_date=2023-09-11 08:09:46.290816+00:00, run_duration=0.185593, state=success, executor_state=success, try_number=1, max_tries=0, job_id=310, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:43.277075+00:00, queued_by_job_id=2, pid=50229[0m
[[34m2023-09-11T08:09:46.925+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-21T00:00:00+00:00, run_after=2023-02-22T00:00:00+00:00[0m
[[34m2023-09-11T08:09:46.947+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-20 00:00:00+00:00: scheduled__2023-02-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:43.216957+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:46.947+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-20 00:00:00+00:00, run_id=scheduled__2023-02-20T00:00:00+00:00, run_start_date=2023-09-11 08:09:43.234630+00:00, run_end_date=2023-09-11 08:09:46.947396+00:00, run_duration=3.712766, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-20 00:00:00+00:00, data_interval_end=2023-02-21 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:46.950+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-21T00:00:00+00:00, run_after=2023-02-22T00:00:00+00:00[0m
[[34m2023-09-11T08:09:48.211+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-22T00:00:00+00:00, run_after=2023-02-23T00:00:00+00:00[0m
[[34m2023-09-11T08:09:48.262+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:48.262+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:48.262+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:48.265+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:48.265+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:48.266+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:48.269+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:50.039+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:50.167+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:50.336+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:50.369+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:50.842+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:50.843+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:50.911+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-21T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:50.966+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:51.623+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:51.634+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:51.050709+00:00, run_end_date=2023-09-11 08:09:51.237830+00:00, run_duration=0.187121, state=success, executor_state=success, try_number=1, max_tries=0, job_id=311, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:48.263665+00:00, queued_by_job_id=2, pid=50238[0m
[[34m2023-09-11T08:09:51.898+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-23T00:00:00+00:00, run_after=2023-02-24T00:00:00+00:00[0m
[[34m2023-09-11T08:09:51.933+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-21 00:00:00+00:00: scheduled__2023-02-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:48.206849+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:51.934+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-21 00:00:00+00:00, run_id=scheduled__2023-02-21T00:00:00+00:00, run_start_date=2023-09-11 08:09:48.230589+00:00, run_end_date=2023-09-11 08:09:51.934224+00:00, run_duration=3.703635, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-21 00:00:00+00:00, data_interval_end=2023-02-22 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:51.937+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-22T00:00:00+00:00, run_after=2023-02-23T00:00:00+00:00[0m
[[34m2023-09-11T08:09:51.953+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:51.953+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:51.953+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:51.956+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:51.956+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:51.957+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:51.959+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:53.757+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:53.888+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:54.065+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:54.098+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:54.567+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:09:54.570+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:54.699+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-22T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:09:54.816+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:09:55.614+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:09:55.625+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:09:54.937670+00:00, run_end_date=2023-09-11 08:09:55.205225+00:00, run_duration=0.267555, state=success, executor_state=success, try_number=1, max_tries=0, job_id=312, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:51.954628+00:00, queued_by_job_id=2, pid=50246[0m
[[34m2023-09-11T08:09:55.964+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-23T00:00:00+00:00, run_after=2023-02-24T00:00:00+00:00[0m
[[34m2023-09-11T08:09:55.987+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-22 00:00:00+00:00: scheduled__2023-02-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:51.893306+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:09:55.987+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-22 00:00:00+00:00, run_id=scheduled__2023-02-22T00:00:00+00:00, run_start_date=2023-09-11 08:09:51.910824+00:00, run_end_date=2023-09-11 08:09:55.987404+00:00, run_duration=4.07658, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-22 00:00:00+00:00, data_interval_end=2023-02-23 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:09:55.990+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-23T00:00:00+00:00, run_after=2023-02-24T00:00:00+00:00[0m
[[34m2023-09-11T08:09:56.888+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-24T00:00:00+00:00, run_after=2023-02-25T00:00:00+00:00[0m
[[34m2023-09-11T08:09:56.944+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:56.945+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:09:56.945+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:09:56.947+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:09:56.948+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:09:56.948+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:56.951+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:09:59.196+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:09:59.350+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:09:59.543+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:09:59.590+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:00.091+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:00.092+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:00.179+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-23T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:00.234+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:00.988+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:01.000+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:00.317598+00:00, run_end_date=2023-09-11 08:10:00.520694+00:00, run_duration=0.203096, state=success, executor_state=success, try_number=1, max_tries=0, job_id=313, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:09:56.945923+00:00, queued_by_job_id=2, pid=50255[0m
[[34m2023-09-11T08:10:01.278+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-25T00:00:00+00:00, run_after=2023-02-26T00:00:00+00:00[0m
[[34m2023-09-11T08:10:01.318+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-23 00:00:00+00:00: scheduled__2023-02-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:09:56.883852+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:01.319+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-23 00:00:00+00:00, run_id=scheduled__2023-02-23T00:00:00+00:00, run_start_date=2023-09-11 08:09:56.911979+00:00, run_end_date=2023-09-11 08:10:01.319307+00:00, run_duration=4.407328, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-23 00:00:00+00:00, data_interval_end=2023-02-24 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:01.324+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-24T00:00:00+00:00, run_after=2023-02-25T00:00:00+00:00[0m
[[34m2023-09-11T08:10:01.343+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:01.343+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:01.344+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:01.346+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:01.347+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:01.347+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:01.350+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:03.231+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:03.368+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:03.552+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:03.589+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:04.094+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:04.096+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:04.169+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-24T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:04.227+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:05.002+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:05.022+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:04.320920+00:00, run_end_date=2023-09-11 08:10:04.540460+00:00, run_duration=0.21954, state=success, executor_state=success, try_number=1, max_tries=0, job_id=314, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:01.345260+00:00, queued_by_job_id=2, pid=50263[0m
[[34m2023-09-11T08:10:05.039+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:10:05.294+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-26T00:00:00+00:00, run_after=2023-02-27T00:00:00+00:00[0m
[[34m2023-09-11T08:10:05.329+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-24 00:00:00+00:00: scheduled__2023-02-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:01.272131+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:05.329+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-24 00:00:00+00:00, run_id=scheduled__2023-02-24T00:00:00+00:00, run_start_date=2023-09-11 08:10:01.291700+00:00, run_end_date=2023-09-11 08:10:05.329846+00:00, run_duration=4.038146, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-24 00:00:00+00:00, data_interval_end=2023-02-25 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:05.333+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-25T00:00:00+00:00, run_after=2023-02-26T00:00:00+00:00[0m
[[34m2023-09-11T08:10:05.348+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:05.349+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:05.349+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:05.351+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:05.352+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:05.352+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:05.355+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:07.193+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:07.323+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:07.509+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:07.542+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:07.996+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:07.996+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:08.063+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-25T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:08.118+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:08.759+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:08.771+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:08.199209+00:00, run_end_date=2023-09-11 08:10:08.388701+00:00, run_duration=0.189492, state=success, executor_state=success, try_number=1, max_tries=0, job_id=315, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:05.350058+00:00, queued_by_job_id=2, pid=50269[0m
[[34m2023-09-11T08:10:09.054+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-26T00:00:00+00:00, run_after=2023-02-27T00:00:00+00:00[0m
[[34m2023-09-11T08:10:09.076+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-25 00:00:00+00:00: scheduled__2023-02-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:05.289124+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:09.076+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-25 00:00:00+00:00, run_id=scheduled__2023-02-25T00:00:00+00:00, run_start_date=2023-09-11 08:10:05.307138+00:00, run_end_date=2023-09-11 08:10:09.076754+00:00, run_duration=3.769616, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-25 00:00:00+00:00, data_interval_end=2023-02-26 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:09.080+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-26T00:00:00+00:00, run_after=2023-02-27T00:00:00+00:00[0m
[[34m2023-09-11T08:10:10.354+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-27T00:00:00+00:00, run_after=2023-02-28T00:00:00+00:00[0m
[[34m2023-09-11T08:10:10.409+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:10.409+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:10.409+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:10.411+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:10.412+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:10.412+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:10.415+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:12.234+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:12.358+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:12.530+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:12.561+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:13.012+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:13.013+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:13.081+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-26T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:13.139+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:13.870+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:13.880+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:13.224221+00:00, run_end_date=2023-09-11 08:10:13.444501+00:00, run_duration=0.22028, state=success, executor_state=success, try_number=1, max_tries=0, job_id=316, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:10.410286+00:00, queued_by_job_id=2, pid=50278[0m
[[34m2023-09-11T08:10:14.177+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-28T00:00:00+00:00, run_after=2023-03-01T00:00:00+00:00[0m
[[34m2023-09-11T08:10:14.212+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-26 00:00:00+00:00: scheduled__2023-02-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:10.350231+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:14.213+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-26 00:00:00+00:00, run_id=scheduled__2023-02-26T00:00:00+00:00, run_start_date=2023-09-11 08:10:10.366585+00:00, run_end_date=2023-09-11 08:10:14.213104+00:00, run_duration=3.846519, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-26 00:00:00+00:00, data_interval_end=2023-02-27 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:14.216+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-27T00:00:00+00:00, run_after=2023-02-28T00:00:00+00:00[0m
[[34m2023-09-11T08:10:14.231+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:14.231+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:14.232+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:14.234+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:14.235+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:14.235+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:14.238+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:16.088+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:16.223+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:16.397+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:16.429+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:16.898+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:16.898+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:16.966+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-27T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:17.022+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:17.708+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:17.719+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:17.105679+00:00, run_end_date=2023-09-11 08:10:17.300069+00:00, run_duration=0.19439, state=success, executor_state=success, try_number=1, max_tries=0, job_id=317, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:14.232872+00:00, queued_by_job_id=2, pid=50284[0m
[[34m2023-09-11T08:10:17.968+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-28T00:00:00+00:00, run_after=2023-03-01T00:00:00+00:00[0m
[[34m2023-09-11T08:10:18.211+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-27 00:00:00+00:00: scheduled__2023-02-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:14.172460+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:18.211+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-27 00:00:00+00:00, run_id=scheduled__2023-02-27T00:00:00+00:00, run_start_date=2023-09-11 08:10:14.190246+00:00, run_end_date=2023-09-11 08:10:18.211577+00:00, run_duration=4.021331, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-27 00:00:00+00:00, data_interval_end=2023-02-28 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:18.215+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-02-28T00:00:00+00:00, run_after=2023-03-01T00:00:00+00:00[0m
[[34m2023-09-11T08:10:19.263+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-01T00:00:00+00:00, run_after=2023-03-02T00:00:00+00:00[0m
[[34m2023-09-11T08:10:19.307+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-02-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:19.307+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:19.307+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-02-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:19.310+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:19.310+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:19.311+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:19.313+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-02-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:21.111+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:21.294+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:21.517+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:21.553+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:22.003+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:22.003+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:22.070+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-02-28T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:22.128+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-02-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:22.835+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-02-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:22.845+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-02-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:22.221207+00:00, run_end_date=2023-09-11 08:10:22.421396+00:00, run_duration=0.200189, state=success, executor_state=success, try_number=1, max_tries=0, job_id=318, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:19.308578+00:00, queued_by_job_id=2, pid=50294[0m
[[34m2023-09-11T08:10:23.102+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-02T00:00:00+00:00, run_after=2023-03-03T00:00:00+00:00[0m
[[34m2023-09-11T08:10:23.137+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-02-28 00:00:00+00:00: scheduled__2023-02-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:19.258708+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:23.138+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-02-28 00:00:00+00:00, run_id=scheduled__2023-02-28T00:00:00+00:00, run_start_date=2023-09-11 08:10:19.275362+00:00, run_end_date=2023-09-11 08:10:23.138085+00:00, run_duration=3.862723, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-28 00:00:00+00:00, data_interval_end=2023-03-01 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:23.141+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-01T00:00:00+00:00, run_after=2023-03-02T00:00:00+00:00[0m
[[34m2023-09-11T08:10:23.157+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:23.157+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:23.157+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:23.160+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:23.160+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:23.160+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:23.163+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:24.979+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:25.115+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:25.309+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:25.345+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:25.809+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:25.809+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:25.876+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-01T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:25.940+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:26.629+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:26.640+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:26.025840+00:00, run_end_date=2023-09-11 08:10:26.218539+00:00, run_duration=0.192699, state=success, executor_state=success, try_number=1, max_tries=0, job_id=319, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:23.158369+00:00, queued_by_job_id=2, pid=50302[0m
[[34m2023-09-11T08:10:26.949+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-02T00:00:00+00:00, run_after=2023-03-03T00:00:00+00:00[0m
[[34m2023-09-11T08:10:26.973+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-01 00:00:00+00:00: scheduled__2023-03-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:23.096490+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:26.973+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-01 00:00:00+00:00, run_id=scheduled__2023-03-01T00:00:00+00:00, run_start_date=2023-09-11 08:10:23.114862+00:00, run_end_date=2023-09-11 08:10:26.973523+00:00, run_duration=3.858661, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-01 00:00:00+00:00, data_interval_end=2023-03-02 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:26.977+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-02T00:00:00+00:00, run_after=2023-03-03T00:00:00+00:00[0m
[[34m2023-09-11T08:10:27.945+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-03T00:00:00+00:00, run_after=2023-03-04T00:00:00+00:00[0m
[[34m2023-09-11T08:10:27.989+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:27.989+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:27.989+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:27.991+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:27.992+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:27.992+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:27.995+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:29.862+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:29.987+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:30.156+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:30.189+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:30.644+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:30.644+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:30.712+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-02T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:30.769+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:31.442+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:31.454+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:30.859107+00:00, run_end_date=2023-09-11 08:10:31.048372+00:00, run_duration=0.189265, state=success, executor_state=success, try_number=1, max_tries=0, job_id=320, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:27.990377+00:00, queued_by_job_id=2, pid=50311[0m
[[34m2023-09-11T08:10:31.623+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-04T00:00:00+00:00, run_after=2023-03-05T00:00:00+00:00[0m
[[34m2023-09-11T08:10:31.663+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-02 00:00:00+00:00: scheduled__2023-03-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:27.941499+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:31.663+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-02 00:00:00+00:00, run_id=scheduled__2023-03-02T00:00:00+00:00, run_start_date=2023-09-11 08:10:27.958228+00:00, run_end_date=2023-09-11 08:10:31.663872+00:00, run_duration=3.705644, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-02 00:00:00+00:00, data_interval_end=2023-03-03 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:31.667+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-03T00:00:00+00:00, run_after=2023-03-04T00:00:00+00:00[0m
[[34m2023-09-11T08:10:31.683+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:31.684+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:31.684+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:31.686+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:31.687+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:31.687+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:31.690+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:33.547+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:33.673+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:33.845+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:33.877+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:34.337+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:34.337+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:34.418+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-03T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:34.476+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:35.174+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:35.185+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:34.561707+00:00, run_end_date=2023-09-11 08:10:34.789006+00:00, run_duration=0.227299, state=success, executor_state=success, try_number=1, max_tries=0, job_id=321, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:31.685359+00:00, queued_by_job_id=2, pid=50319[0m
[[34m2023-09-11T08:10:35.350+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-04T00:00:00+00:00, run_after=2023-03-05T00:00:00+00:00[0m
[[34m2023-09-11T08:10:35.372+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-03 00:00:00+00:00: scheduled__2023-03-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:31.618256+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:35.373+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-03 00:00:00+00:00, run_id=scheduled__2023-03-03T00:00:00+00:00, run_start_date=2023-09-11 08:10:31.635258+00:00, run_end_date=2023-09-11 08:10:35.373292+00:00, run_duration=3.738034, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-03 00:00:00+00:00, data_interval_end=2023-03-04 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:35.376+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-04T00:00:00+00:00, run_after=2023-03-05T00:00:00+00:00[0m
[[34m2023-09-11T08:10:36.640+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-05T00:00:00+00:00, run_after=2023-03-06T00:00:00+00:00[0m
[[34m2023-09-11T08:10:36.686+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:36.686+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:36.686+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:36.688+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:36.689+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:36.689+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:36.691+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:38.501+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:38.629+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:38.800+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:38.832+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:39.300+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:39.301+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:39.371+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-04T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:39.427+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:40.097+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:40.107+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:39.511913+00:00, run_end_date=2023-09-11 08:10:39.714433+00:00, run_duration=0.20252, state=success, executor_state=success, try_number=1, max_tries=0, job_id=322, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:36.687341+00:00, queued_by_job_id=2, pid=50328[0m
[[34m2023-09-11T08:10:40.364+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-06T00:00:00+00:00, run_after=2023-03-07T00:00:00+00:00[0m
[[34m2023-09-11T08:10:40.410+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-04 00:00:00+00:00: scheduled__2023-03-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:36.635066+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:40.410+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-04 00:00:00+00:00, run_id=scheduled__2023-03-04T00:00:00+00:00, run_start_date=2023-09-11 08:10:36.653348+00:00, run_end_date=2023-09-11 08:10:40.410810+00:00, run_duration=3.757462, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-04 00:00:00+00:00, data_interval_end=2023-03-05 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:40.414+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-05T00:00:00+00:00, run_after=2023-03-06T00:00:00+00:00[0m
[[34m2023-09-11T08:10:40.428+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:40.428+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:40.429+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:40.431+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:40.431+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:40.431+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:40.434+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:42.279+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:42.412+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:42.578+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:42.611+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:43.071+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:43.072+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:43.146+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-05T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:43.209+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:43.957+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:43.969+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:43.297642+00:00, run_end_date=2023-09-11 08:10:43.539610+00:00, run_duration=0.241968, state=success, executor_state=success, try_number=1, max_tries=0, job_id=323, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:40.429748+00:00, queued_by_job_id=2, pid=50334[0m
[[34m2023-09-11T08:10:44.211+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-06T00:00:00+00:00, run_after=2023-03-07T00:00:00+00:00[0m
[[34m2023-09-11T08:10:44.239+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-05 00:00:00+00:00: scheduled__2023-03-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:40.359784+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:44.239+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-05 00:00:00+00:00, run_id=scheduled__2023-03-05T00:00:00+00:00, run_start_date=2023-09-11 08:10:40.377391+00:00, run_end_date=2023-09-11 08:10:44.239765+00:00, run_duration=3.862374, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-05 00:00:00+00:00, data_interval_end=2023-03-06 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:44.243+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-06T00:00:00+00:00, run_after=2023-03-07T00:00:00+00:00[0m
[[34m2023-09-11T08:10:45.365+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-07T00:00:00+00:00, run_after=2023-03-08T00:00:00+00:00[0m
[[34m2023-09-11T08:10:45.414+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:45.415+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:45.415+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:45.417+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:45.418+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:45.418+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:45.421+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:47.275+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:47.409+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:47.575+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:47.606+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:48.071+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:48.071+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:48.142+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-06T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:48.197+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:48.900+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:48.911+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:48.288481+00:00, run_end_date=2023-09-11 08:10:48.485965+00:00, run_duration=0.197484, state=success, executor_state=success, try_number=1, max_tries=0, job_id=324, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:45.416157+00:00, queued_by_job_id=2, pid=50343[0m
[[34m2023-09-11T08:10:49.225+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-08T00:00:00+00:00, run_after=2023-03-09T00:00:00+00:00[0m
[[34m2023-09-11T08:10:49.267+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-06 00:00:00+00:00: scheduled__2023-03-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:45.361121+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:49.268+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-06 00:00:00+00:00, run_id=scheduled__2023-03-06T00:00:00+00:00, run_start_date=2023-09-11 08:10:45.378432+00:00, run_end_date=2023-09-11 08:10:49.268175+00:00, run_duration=3.889743, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-06 00:00:00+00:00, data_interval_end=2023-03-07 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:49.271+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-07T00:00:00+00:00, run_after=2023-03-08T00:00:00+00:00[0m
[[34m2023-09-11T08:10:49.287+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:49.287+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:49.287+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:49.289+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:49.290+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:49.290+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:49.293+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:51.148+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:51.281+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:51.446+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:51.480+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:51.932+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:51.933+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:52.000+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-07T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:52.056+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:52.780+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:52.791+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:52.146171+00:00, run_end_date=2023-09-11 08:10:52.346571+00:00, run_duration=0.2004, state=success, executor_state=success, try_number=1, max_tries=0, job_id=325, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:49.288401+00:00, queued_by_job_id=2, pid=50349[0m
[[34m2023-09-11T08:10:53.120+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-08T00:00:00+00:00, run_after=2023-03-09T00:00:00+00:00[0m
[[34m2023-09-11T08:10:53.142+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-07 00:00:00+00:00: scheduled__2023-03-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:49.219705+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:53.143+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-07 00:00:00+00:00, run_id=scheduled__2023-03-07T00:00:00+00:00, run_start_date=2023-09-11 08:10:49.237599+00:00, run_end_date=2023-09-11 08:10:53.142987+00:00, run_duration=3.905388, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-07 00:00:00+00:00, data_interval_end=2023-03-08 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:53.146+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-08T00:00:00+00:00, run_after=2023-03-09T00:00:00+00:00[0m
[[34m2023-09-11T08:10:54.234+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-09T00:00:00+00:00, run_after=2023-03-10T00:00:00+00:00[0m
[[34m2023-09-11T08:10:54.284+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:54.284+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:54.284+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:54.287+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:54.287+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:54.287+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:54.290+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:56.108+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:10:56.237+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:56.410+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:56.442+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:10:56.901+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:10:56.902+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:10:56.970+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-08T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:10:57.026+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:10:57.750+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:10:57.761+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:10:57.107342+00:00, run_end_date=2023-09-11 08:10:57.348962+00:00, run_duration=0.24162, state=success, executor_state=success, try_number=1, max_tries=0, job_id=326, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:54.285675+00:00, queued_by_job_id=2, pid=50358[0m
[[34m2023-09-11T08:10:58.031+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-10T00:00:00+00:00, run_after=2023-03-11T00:00:00+00:00[0m
[[34m2023-09-11T08:10:58.065+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-08 00:00:00+00:00: scheduled__2023-03-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:54.229809+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:10:58.066+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-08 00:00:00+00:00, run_id=scheduled__2023-03-08T00:00:00+00:00, run_start_date=2023-09-11 08:10:54.246823+00:00, run_end_date=2023-09-11 08:10:58.066325+00:00, run_duration=3.819502, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-08 00:00:00+00:00, data_interval_end=2023-03-09 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:10:58.070+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-09T00:00:00+00:00, run_after=2023-03-10T00:00:00+00:00[0m
[[34m2023-09-11T08:10:58.104+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:58.105+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:10:58.105+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:10:58.107+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:10:58.108+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:10:58.108+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:58.111+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:10:59.944+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:00.071+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:00.250+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:00.295+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:00.769+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:00.770+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:00.841+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-09T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:00.908+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:01.567+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:01.578+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:00.998794+00:00, run_end_date=2023-09-11 08:11:01.190836+00:00, run_duration=0.192042, state=success, executor_state=success, try_number=1, max_tries=0, job_id=327, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:10:58.106149+00:00, queued_by_job_id=2, pid=50366[0m
[[34m2023-09-11T08:11:01.836+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-10T00:00:00+00:00, run_after=2023-03-11T00:00:00+00:00[0m
[[34m2023-09-11T08:11:01.860+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-09 00:00:00+00:00: scheduled__2023-03-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:10:58.026324+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:01.860+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-09 00:00:00+00:00, run_id=scheduled__2023-03-09T00:00:00+00:00, run_start_date=2023-09-11 08:10:58.044270+00:00, run_end_date=2023-09-11 08:11:01.860555+00:00, run_duration=3.816285, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-09 00:00:00+00:00, data_interval_end=2023-03-10 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:01.864+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-10T00:00:00+00:00, run_after=2023-03-11T00:00:00+00:00[0m
[[34m2023-09-11T08:11:03.097+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-11T00:00:00+00:00, run_after=2023-03-12T00:00:00+00:00[0m
[[34m2023-09-11T08:11:03.144+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:03.144+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:03.144+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:03.146+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:03.147+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:03.147+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:03.150+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:04.988+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:05.112+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:05.289+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:05.325+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:05.822+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:05.822+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:05.894+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-10T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:05.960+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:06.667+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:06.682+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:06.049456+00:00, run_end_date=2023-09-11 08:11:06.247181+00:00, run_duration=0.197725, state=success, executor_state=success, try_number=1, max_tries=0, job_id=328, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:03.145474+00:00, queued_by_job_id=2, pid=50375[0m
[[34m2023-09-11T08:11:07.007+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-12T00:00:00+00:00, run_after=2023-03-13T00:00:00+00:00[0m
[[34m2023-09-11T08:11:07.045+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-10 00:00:00+00:00: scheduled__2023-03-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:03.093153+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:07.046+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-10 00:00:00+00:00, run_id=scheduled__2023-03-10T00:00:00+00:00, run_start_date=2023-09-11 08:11:03.108993+00:00, run_end_date=2023-09-11 08:11:07.045867+00:00, run_duration=3.936874, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-10 00:00:00+00:00, data_interval_end=2023-03-11 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:07.049+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-11T00:00:00+00:00, run_after=2023-03-12T00:00:00+00:00[0m
[[34m2023-09-11T08:11:07.064+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:07.065+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:07.065+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:07.067+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:07.068+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:07.068+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:07.071+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:08.888+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:09.023+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:09.206+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:09.247+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:09.713+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:09.714+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:09.783+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-11T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:09.838+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:10.491+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:10.502+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:09.922389+00:00, run_end_date=2023-09-11 08:11:10.109973+00:00, run_duration=0.187584, state=success, executor_state=success, try_number=1, max_tries=0, job_id=329, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:07.066058+00:00, queued_by_job_id=2, pid=50383[0m
[[34m2023-09-11T08:11:11.259+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-12T00:00:00+00:00, run_after=2023-03-13T00:00:00+00:00[0m
[[34m2023-09-11T08:11:11.282+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-11 00:00:00+00:00: scheduled__2023-03-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:07.002563+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:11.283+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-11 00:00:00+00:00, run_id=scheduled__2023-03-11T00:00:00+00:00, run_start_date=2023-09-11 08:11:07.021986+00:00, run_end_date=2023-09-11 08:11:11.283222+00:00, run_duration=4.261236, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-11 00:00:00+00:00, data_interval_end=2023-03-12 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:11.286+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-12T00:00:00+00:00, run_after=2023-03-13T00:00:00+00:00[0m
[[34m2023-09-11T08:11:12.035+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-13T00:00:00+00:00, run_after=2023-03-14T00:00:00+00:00[0m
[[34m2023-09-11T08:11:12.078+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:12.078+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:12.079+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:12.081+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:12.082+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:12.082+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:12.085+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:14.166+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:14.298+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:14.484+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:14.516+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:14.991+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:14.992+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:15.078+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-12T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:15.140+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:15.916+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:15.926+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:15.225949+00:00, run_end_date=2023-09-11 08:11:15.471126+00:00, run_duration=0.245177, state=success, executor_state=success, try_number=1, max_tries=0, job_id=330, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:12.079736+00:00, queued_by_job_id=2, pid=50392[0m
[[34m2023-09-11T08:11:16.227+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-14T00:00:00+00:00, run_after=2023-03-15T00:00:00+00:00[0m
[[34m2023-09-11T08:11:16.263+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-12 00:00:00+00:00: scheduled__2023-03-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:12.030640+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:16.264+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-12 00:00:00+00:00, run_id=scheduled__2023-03-12T00:00:00+00:00, run_start_date=2023-09-11 08:11:12.047874+00:00, run_end_date=2023-09-11 08:11:16.264143+00:00, run_duration=4.216269, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-12 00:00:00+00:00, data_interval_end=2023-03-13 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:16.267+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-13T00:00:00+00:00, run_after=2023-03-14T00:00:00+00:00[0m
[[34m2023-09-11T08:11:16.282+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:16.282+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:16.283+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:16.285+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:16.285+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:16.286+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:16.289+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:18.166+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:18.299+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:18.477+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:18.507+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:18.983+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:18.983+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:19.053+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-13T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:19.108+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:19.803+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:19.814+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:19.193729+00:00, run_end_date=2023-09-11 08:11:19.389881+00:00, run_duration=0.196152, state=success, executor_state=success, try_number=1, max_tries=0, job_id=331, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:16.283711+00:00, queued_by_job_id=2, pid=50398[0m
[[34m2023-09-11T08:11:20.474+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-14T00:00:00+00:00, run_after=2023-03-15T00:00:00+00:00[0m
[[34m2023-09-11T08:11:20.498+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-13 00:00:00+00:00: scheduled__2023-03-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:16.222980+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:20.498+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-13 00:00:00+00:00, run_id=scheduled__2023-03-13T00:00:00+00:00, run_start_date=2023-09-11 08:11:16.241187+00:00, run_end_date=2023-09-11 08:11:20.498538+00:00, run_duration=4.257351, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-13 00:00:00+00:00, data_interval_end=2023-03-14 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:20.502+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-14T00:00:00+00:00, run_after=2023-03-15T00:00:00+00:00[0m
[[34m2023-09-11T08:11:21.263+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-15T00:00:00+00:00, run_after=2023-03-16T00:00:00+00:00[0m
[[34m2023-09-11T08:11:21.316+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:21.316+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:21.316+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:21.318+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:21.319+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:21.319+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:21.322+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:23.286+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:23.423+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:23.659+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:23.691+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:24.225+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:24.226+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:24.306+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-14T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:24.363+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:25.064+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:25.075+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:24.495829+00:00, run_end_date=2023-09-11 08:11:24.689168+00:00, run_duration=0.193339, state=success, executor_state=success, try_number=1, max_tries=0, job_id=332, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:21.317424+00:00, queued_by_job_id=2, pid=50409[0m
[[34m2023-09-11T08:11:25.342+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-16T00:00:00+00:00, run_after=2023-03-17T00:00:00+00:00[0m
[[34m2023-09-11T08:11:25.376+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-14 00:00:00+00:00: scheduled__2023-03-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:21.258212+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:25.377+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-14 00:00:00+00:00, run_id=scheduled__2023-03-14T00:00:00+00:00, run_start_date=2023-09-11 08:11:21.281686+00:00, run_end_date=2023-09-11 08:11:25.377091+00:00, run_duration=4.095405, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-14 00:00:00+00:00, data_interval_end=2023-03-15 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:25.380+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-15T00:00:00+00:00, run_after=2023-03-16T00:00:00+00:00[0m
[[34m2023-09-11T08:11:25.406+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:25.406+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:25.407+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:25.409+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:25.409+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:25.410+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:25.413+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:27.234+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:27.369+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:27.545+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:27.578+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:28.036+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:28.037+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:28.108+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-15T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:28.165+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:29.113+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:29.127+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:28.248214+00:00, run_end_date=2023-09-11 08:11:28.453053+00:00, run_duration=0.204839, state=success, executor_state=success, try_number=1, max_tries=0, job_id=333, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:25.407791+00:00, queued_by_job_id=2, pid=50413[0m
[[34m2023-09-11T08:11:29.574+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-16T00:00:00+00:00, run_after=2023-03-17T00:00:00+00:00[0m
[[34m2023-09-11T08:11:29.597+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-15 00:00:00+00:00: scheduled__2023-03-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:25.336827+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:29.598+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-15 00:00:00+00:00, run_id=scheduled__2023-03-15T00:00:00+00:00, run_start_date=2023-09-11 08:11:25.354886+00:00, run_end_date=2023-09-11 08:11:29.597884+00:00, run_duration=4.242998, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-15 00:00:00+00:00, data_interval_end=2023-03-16 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:29.601+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-16T00:00:00+00:00, run_after=2023-03-17T00:00:00+00:00[0m
[[34m2023-09-11T08:11:30.346+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00[0m
[[34m2023-09-11T08:11:30.402+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:30.402+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:30.403+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:30.405+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:30.405+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:30.405+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:30.408+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:32.509+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:32.646+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:32.826+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:32.867+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:33.332+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:33.333+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:33.404+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-16T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:33.467+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:34.208+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:34.222+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:33.557891+00:00, run_end_date=2023-09-11 08:11:33.752877+00:00, run_duration=0.194986, state=success, executor_state=success, try_number=1, max_tries=0, job_id=334, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:30.403756+00:00, queued_by_job_id=2, pid=50424[0m
[[34m2023-09-11T08:11:34.433+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00[0m
[[34m2023-09-11T08:11:34.467+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-16 00:00:00+00:00: scheduled__2023-03-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:30.342258+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:34.468+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-16 00:00:00+00:00, run_id=scheduled__2023-03-16T00:00:00+00:00, run_start_date=2023-09-11 08:11:30.361546+00:00, run_end_date=2023-09-11 08:11:34.467984+00:00, run_duration=4.106438, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-16 00:00:00+00:00, data_interval_end=2023-03-17 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:34.471+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00[0m
[[34m2023-09-11T08:11:34.486+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:34.486+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:34.486+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:34.488+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:34.489+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:34.489+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:34.492+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:36.553+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:36.691+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:36.867+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:36.911+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:37.530+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:37.532+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:37.613+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-17T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:37.677+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:38.752+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:38.768+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:37.773867+00:00, run_end_date=2023-09-11 08:11:37.989811+00:00, run_duration=0.215944, state=success, executor_state=success, try_number=1, max_tries=0, job_id=335, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:34.487361+00:00, queued_by_job_id=2, pid=50430[0m
[[34m2023-09-11T08:11:39.259+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00[0m
[[34m2023-09-11T08:11:39.287+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-17 00:00:00+00:00: scheduled__2023-03-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:34.427177+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:39.288+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-17 00:00:00+00:00, run_id=scheduled__2023-03-17T00:00:00+00:00, run_start_date=2023-09-11 08:11:34.445496+00:00, run_end_date=2023-09-11 08:11:39.288400+00:00, run_duration=4.842904, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-17 00:00:00+00:00, data_interval_end=2023-03-18 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:39.293+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00[0m
[[34m2023-09-11T08:11:40.597+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-19T00:00:00+00:00, run_after=2023-03-20T00:00:00+00:00[0m
[[34m2023-09-11T08:11:40.647+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:40.647+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:40.648+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:40.650+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:40.651+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:40.652+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:40.654+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:42.803+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:42.954+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:43.213+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:43.258+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:43.962+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:43.963+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:44.044+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-18T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:44.118+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:45.063+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:45.076+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:44.228749+00:00, run_end_date=2023-09-11 08:11:44.600005+00:00, run_duration=0.371256, state=success, executor_state=success, try_number=1, max_tries=0, job_id=336, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:40.648991+00:00, queued_by_job_id=2, pid=50441[0m
[[34m2023-09-11T08:11:45.538+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-20T00:00:00+00:00, run_after=2023-03-21T00:00:00+00:00[0m
[[34m2023-09-11T08:11:45.604+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-18 00:00:00+00:00: scheduled__2023-03-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:40.591521+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:45.605+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-18 00:00:00+00:00, run_id=scheduled__2023-03-18T00:00:00+00:00, run_start_date=2023-09-11 08:11:40.609212+00:00, run_end_date=2023-09-11 08:11:45.605116+00:00, run_duration=4.995904, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-18 00:00:00+00:00, data_interval_end=2023-03-19 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:45.611+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-19T00:00:00+00:00, run_after=2023-03-20T00:00:00+00:00[0m
[[34m2023-09-11T08:11:45.638+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:45.639+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:45.640+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:45.644+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:45.645+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:45.646+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:45.651+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:48.623+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:48.860+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:49.101+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:49.142+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:49.767+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:49.768+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:49.839+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-19T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:49.912+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:50.820+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:50.831+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:50.002770+00:00, run_end_date=2023-09-11 08:11:50.244930+00:00, run_duration=0.24216, state=success, executor_state=success, try_number=1, max_tries=0, job_id=337, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:45.641452+00:00, queued_by_job_id=2, pid=50451[0m
[[34m2023-09-11T08:11:51.237+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-20T00:00:00+00:00, run_after=2023-03-21T00:00:00+00:00[0m
[[34m2023-09-11T08:11:51.261+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-19 00:00:00+00:00: scheduled__2023-03-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:45.528284+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:51.261+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-19 00:00:00+00:00, run_id=scheduled__2023-03-19T00:00:00+00:00, run_start_date=2023-09-11 08:11:45.559988+00:00, run_end_date=2023-09-11 08:11:51.261519+00:00, run_duration=5.701531, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-19 00:00:00+00:00, data_interval_end=2023-03-20 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:51.267+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-20T00:00:00+00:00, run_after=2023-03-21T00:00:00+00:00[0m
[[34m2023-09-11T08:11:52.637+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-21T00:00:00+00:00, run_after=2023-03-22T00:00:00+00:00[0m
[[34m2023-09-11T08:11:52.682+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:52.682+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:52.682+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:52.686+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:52.687+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:52.687+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:52.690+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:54.686+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:54.819+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:55.004+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:55.038+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:55.518+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:55.519+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:55.598+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-20T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:55.656+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:11:56.370+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:11:56.381+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:55.745529+00:00, run_end_date=2023-09-11 08:11:55.948298+00:00, run_duration=0.202769, state=success, executor_state=success, try_number=1, max_tries=0, job_id=338, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:52.683737+00:00, queued_by_job_id=2, pid=50460[0m
[[34m2023-09-11T08:11:56.642+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-22T00:00:00+00:00, run_after=2023-03-23T00:00:00+00:00[0m
[[34m2023-09-11T08:11:56.678+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-20 00:00:00+00:00: scheduled__2023-03-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:52.632931+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:11:56.678+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-20 00:00:00+00:00, run_id=scheduled__2023-03-20T00:00:00+00:00, run_start_date=2023-09-11 08:11:52.649398+00:00, run_end_date=2023-09-11 08:11:56.678744+00:00, run_duration=4.029346, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-20 00:00:00+00:00, data_interval_end=2023-03-21 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:11:56.682+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-21T00:00:00+00:00, run_after=2023-03-22T00:00:00+00:00[0m
[[34m2023-09-11T08:11:56.699+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:56.699+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:11:56.699+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:11:56.702+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:11:56.702+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:11:56.703+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:56.705+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:11:58.589+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:11:58.724+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:58.903+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:58.936+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:11:59.421+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:11:59.421+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:11:59.494+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-21T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:11:59.558+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:00.258+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:00.270+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:11:59.665262+00:00, run_end_date=2023-09-11 08:11:59.861273+00:00, run_duration=0.196011, state=success, executor_state=success, try_number=1, max_tries=0, job_id=339, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:11:56.700690+00:00, queued_by_job_id=2, pid=50468[0m
[[34m2023-09-11T08:12:00.424+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-22T00:00:00+00:00, run_after=2023-03-23T00:00:00+00:00[0m
[[34m2023-09-11T08:12:00.447+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-21 00:00:00+00:00: scheduled__2023-03-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:11:56.637121+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:00.448+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-21 00:00:00+00:00, run_id=scheduled__2023-03-21T00:00:00+00:00, run_start_date=2023-09-11 08:11:56.655784+00:00, run_end_date=2023-09-11 08:12:00.447913+00:00, run_duration=3.792129, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-21 00:00:00+00:00, data_interval_end=2023-03-22 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:00.451+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-22T00:00:00+00:00, run_after=2023-03-23T00:00:00+00:00[0m
[[34m2023-09-11T08:12:01.788+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-23T00:00:00+00:00, run_after=2023-03-24T00:00:00+00:00[0m
[[34m2023-09-11T08:12:01.834+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:01.834+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:01.834+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:01.836+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:01.837+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:01.837+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:01.840+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:03.721+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:03.851+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:04.030+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:04.065+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:04.552+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:04.553+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:04.623+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-22T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:04.680+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:05.421+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:05.433+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:04.772608+00:00, run_end_date=2023-09-11 08:12:04.988673+00:00, run_duration=0.216065, state=success, executor_state=success, try_number=1, max_tries=0, job_id=340, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:01.835109+00:00, queued_by_job_id=2, pid=50477[0m
[[34m2023-09-11T08:12:05.773+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-24T00:00:00+00:00, run_after=2023-03-25T00:00:00+00:00[0m
[[34m2023-09-11T08:12:05.809+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-22 00:00:00+00:00: scheduled__2023-03-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:01.783569+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:05.810+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-22 00:00:00+00:00, run_id=scheduled__2023-03-22T00:00:00+00:00, run_start_date=2023-09-11 08:12:01.801882+00:00, run_end_date=2023-09-11 08:12:05.810314+00:00, run_duration=4.008432, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-22 00:00:00+00:00, data_interval_end=2023-03-23 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:05.814+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-23T00:00:00+00:00, run_after=2023-03-24T00:00:00+00:00[0m
[[34m2023-09-11T08:12:05.829+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:05.830+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:05.830+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:05.833+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:05.833+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:05.833+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:05.836+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:07.734+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:07.876+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:08.092+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:08.159+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:08.679+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:08.680+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:08.751+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-23T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:08.808+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:09.538+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:09.550+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:08.906646+00:00, run_end_date=2023-09-11 08:12:09.127046+00:00, run_duration=0.2204, state=success, executor_state=success, try_number=1, max_tries=0, job_id=341, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:05.831279+00:00, queued_by_job_id=2, pid=50483[0m
[[34m2023-09-11T08:12:09.836+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-24T00:00:00+00:00, run_after=2023-03-25T00:00:00+00:00[0m
[[34m2023-09-11T08:12:09.868+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-23 00:00:00+00:00: scheduled__2023-03-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:05.768211+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:09.869+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-23 00:00:00+00:00, run_id=scheduled__2023-03-23T00:00:00+00:00, run_start_date=2023-09-11 08:12:05.786659+00:00, run_end_date=2023-09-11 08:12:09.869256+00:00, run_duration=4.082597, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-23 00:00:00+00:00, data_interval_end=2023-03-24 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:09.874+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-24T00:00:00+00:00, run_after=2023-03-25T00:00:00+00:00[0m
[[34m2023-09-11T08:12:10.788+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-25T00:00:00+00:00, run_after=2023-03-26T00:00:00+00:00[0m
[[34m2023-09-11T08:12:10.838+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:10.838+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:10.839+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:10.841+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:10.842+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:10.843+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:10.846+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:12.967+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:13.122+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:13.303+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:13.337+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:13.871+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:13.872+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:13.963+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-24T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:14.029+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:14.845+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:14.859+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:14.127887+00:00, run_end_date=2023-09-11 08:12:14.378473+00:00, run_duration=0.250586, state=success, executor_state=success, try_number=1, max_tries=0, job_id=342, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:10.839792+00:00, queued_by_job_id=2, pid=50492[0m
[[34m2023-09-11T08:12:15.141+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-26T00:00:00+00:00, run_after=2023-03-27T00:00:00+00:00[0m
[[34m2023-09-11T08:12:15.182+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-24 00:00:00+00:00: scheduled__2023-03-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:10.783745+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:15.183+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-24 00:00:00+00:00, run_id=scheduled__2023-03-24T00:00:00+00:00, run_start_date=2023-09-11 08:12:10.804514+00:00, run_end_date=2023-09-11 08:12:15.182958+00:00, run_duration=4.378444, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-24 00:00:00+00:00, data_interval_end=2023-03-25 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:15.186+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-25T00:00:00+00:00, run_after=2023-03-26T00:00:00+00:00[0m
[[34m2023-09-11T08:12:15.202+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:15.203+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:15.203+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:15.205+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:15.206+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:15.206+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:15.209+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:17.302+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:17.448+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:17.627+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:17.664+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:18.184+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:18.185+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:18.259+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-25T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:18.325+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:19.061+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:19.073+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:18.418085+00:00, run_end_date=2023-09-11 08:12:18.627651+00:00, run_duration=0.209566, state=success, executor_state=success, try_number=1, max_tries=0, job_id=343, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:15.204090+00:00, queued_by_job_id=2, pid=50498[0m
[[34m2023-09-11T08:12:19.350+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-26T00:00:00+00:00, run_after=2023-03-27T00:00:00+00:00[0m
[[34m2023-09-11T08:12:19.378+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-25 00:00:00+00:00: scheduled__2023-03-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:15.136387+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:19.379+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-25 00:00:00+00:00, run_id=scheduled__2023-03-25T00:00:00+00:00, run_start_date=2023-09-11 08:12:15.156145+00:00, run_end_date=2023-09-11 08:12:19.379227+00:00, run_duration=4.223082, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-25 00:00:00+00:00, data_interval_end=2023-03-26 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:19.383+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-26T00:00:00+00:00, run_after=2023-03-27T00:00:00+00:00[0m
[[34m2023-09-11T08:12:19.717+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-27T00:00:00+00:00, run_after=2023-03-28T00:00:00+00:00[0m
[[34m2023-09-11T08:12:19.763+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:19.764+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:19.764+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:19.766+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:19.767+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:19.767+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:19.770+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:21.700+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:21.834+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:22.027+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:22.065+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:22.578+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:22.579+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:22.653+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-26T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:22.714+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:23.601+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:23.613+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:22.805969+00:00, run_end_date=2023-09-11 08:12:23.075691+00:00, run_duration=0.269722, state=success, executor_state=success, try_number=1, max_tries=0, job_id=344, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:19.765389+00:00, queued_by_job_id=2, pid=50507[0m
[[34m2023-09-11T08:12:23.905+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-28T00:00:00+00:00, run_after=2023-03-29T00:00:00+00:00[0m
[[34m2023-09-11T08:12:23.942+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-26 00:00:00+00:00: scheduled__2023-03-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:19.712994+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:23.943+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-26 00:00:00+00:00, run_id=scheduled__2023-03-26T00:00:00+00:00, run_start_date=2023-09-11 08:12:19.731799+00:00, run_end_date=2023-09-11 08:12:23.942981+00:00, run_duration=4.211182, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-26 00:00:00+00:00, data_interval_end=2023-03-27 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:23.947+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-27T00:00:00+00:00, run_after=2023-03-28T00:00:00+00:00[0m
[[34m2023-09-11T08:12:23.968+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:23.968+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:23.968+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:23.971+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:23.971+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:23.971+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:23.974+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:25.938+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:26.076+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:26.255+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:26.287+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:26.786+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:26.787+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:26.859+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-27T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:26.921+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:27.703+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:27.714+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:27.014082+00:00, run_end_date=2023-09-11 08:12:27.216442+00:00, run_duration=0.20236, state=success, executor_state=success, try_number=1, max_tries=0, job_id=345, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:23.969571+00:00, queued_by_job_id=2, pid=50515[0m
[[34m2023-09-11T08:12:27.872+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-28T00:00:00+00:00, run_after=2023-03-29T00:00:00+00:00[0m
[[34m2023-09-11T08:12:27.896+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-27 00:00:00+00:00: scheduled__2023-03-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:23.899888+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:27.896+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-27 00:00:00+00:00, run_id=scheduled__2023-03-27T00:00:00+00:00, run_start_date=2023-09-11 08:12:23.918103+00:00, run_end_date=2023-09-11 08:12:27.896625+00:00, run_duration=3.978522, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-27 00:00:00+00:00, data_interval_end=2023-03-28 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:27.900+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-28T00:00:00+00:00, run_after=2023-03-29T00:00:00+00:00[0m
[[34m2023-09-11T08:12:28.807+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-29T00:00:00+00:00, run_after=2023-03-30T00:00:00+00:00[0m
[[34m2023-09-11T08:12:28.849+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:28.849+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:28.849+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:28.852+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:28.852+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:28.853+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:28.855+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:30.718+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:30.850+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:31.023+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:31.054+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:31.516+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:31.517+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:31.587+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-28T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:31.646+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:32.293+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:32.305+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:31.730413+00:00, run_end_date=2023-09-11 08:12:31.919856+00:00, run_duration=0.189443, state=success, executor_state=success, try_number=1, max_tries=0, job_id=346, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:28.850724+00:00, queued_by_job_id=2, pid=50524[0m
[[34m2023-09-11T08:12:32.471+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-30T00:00:00+00:00, run_after=2023-03-31T00:00:00+00:00[0m
[[34m2023-09-11T08:12:32.506+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-28 00:00:00+00:00: scheduled__2023-03-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:28.802563+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:32.506+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-28 00:00:00+00:00, run_id=scheduled__2023-03-28T00:00:00+00:00, run_start_date=2023-09-11 08:12:28.819478+00:00, run_end_date=2023-09-11 08:12:32.506720+00:00, run_duration=3.687242, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-28 00:00:00+00:00, data_interval_end=2023-03-29 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:32.510+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-29T00:00:00+00:00, run_after=2023-03-30T00:00:00+00:00[0m
[[34m2023-09-11T08:12:32.526+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:32.526+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:32.527+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:32.531+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:32.532+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:32.533+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:32.535+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:34.413+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:34.544+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:34.713+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:34.745+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:35.209+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:35.210+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:35.283+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-29T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:35.343+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:35.997+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:36.008+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:35.436029+00:00, run_end_date=2023-09-11 08:12:35.626599+00:00, run_duration=0.19057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=347, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:32.528521+00:00, queued_by_job_id=2, pid=50532[0m
[[34m2023-09-11T08:12:36.397+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-30T00:00:00+00:00, run_after=2023-03-31T00:00:00+00:00[0m
[[34m2023-09-11T08:12:36.418+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-29 00:00:00+00:00: scheduled__2023-03-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:32.465796+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:36.418+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-29 00:00:00+00:00, run_id=scheduled__2023-03-29T00:00:00+00:00, run_start_date=2023-09-11 08:12:32.484769+00:00, run_end_date=2023-09-11 08:12:36.418717+00:00, run_duration=3.933948, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-29 00:00:00+00:00, data_interval_end=2023-03-30 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:36.422+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-30T00:00:00+00:00, run_after=2023-03-31T00:00:00+00:00[0m
[[34m2023-09-11T08:12:37.611+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-31T00:00:00+00:00, run_after=2023-04-01T00:00:00+00:00[0m
[[34m2023-09-11T08:12:37.934+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:37.935+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:37.935+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:37.938+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:37.938+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:37.938+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:37.941+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:39.818+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:39.947+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:40.115+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:40.148+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:40.593+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:40.593+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:40.663+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-30T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:40.718+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:41.419+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:41.429+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:40.802720+00:00, run_end_date=2023-09-11 08:12:41.003326+00:00, run_duration=0.200606, state=success, executor_state=success, try_number=1, max_tries=0, job_id=348, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:37.936381+00:00, queued_by_job_id=2, pid=50541[0m
[[34m2023-09-11T08:12:41.685+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-01T00:00:00+00:00, run_after=2023-04-02T00:00:00+00:00[0m
[[34m2023-09-11T08:12:41.719+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-30 00:00:00+00:00: scheduled__2023-03-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:37.606242+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:41.719+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-30 00:00:00+00:00, run_id=scheduled__2023-03-30T00:00:00+00:00, run_start_date=2023-09-11 08:12:37.903401+00:00, run_end_date=2023-09-11 08:12:41.719795+00:00, run_duration=3.816394, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-30 00:00:00+00:00, data_interval_end=2023-03-31 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:41.723+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-03-31T00:00:00+00:00, run_after=2023-04-01T00:00:00+00:00[0m
[[34m2023-09-11T08:12:41.737+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-03-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:41.738+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:41.738+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-03-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:41.740+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:41.741+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:41.741+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:41.743+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:43.611+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:43.768+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:43.938+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:43.972+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:44.457+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:44.457+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:44.526+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-03-31T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:44.584+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-03-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:45.284+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-03-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:45.295+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-03-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:44.671492+00:00, run_end_date=2023-09-11 08:12:44.866095+00:00, run_duration=0.194603, state=success, executor_state=success, try_number=1, max_tries=0, job_id=349, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:41.739057+00:00, queued_by_job_id=2, pid=50547[0m
[[34m2023-09-11T08:12:45.564+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-01T00:00:00+00:00, run_after=2023-04-02T00:00:00+00:00[0m
[[34m2023-09-11T08:12:45.607+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-03-31 00:00:00+00:00: scheduled__2023-03-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:41.679889+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:45.608+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-03-31 00:00:00+00:00, run_id=scheduled__2023-03-31T00:00:00+00:00, run_start_date=2023-09-11 08:12:41.697703+00:00, run_end_date=2023-09-11 08:12:45.608097+00:00, run_duration=3.910394, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-31 00:00:00+00:00, data_interval_end=2023-04-01 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:45.611+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-01T00:00:00+00:00, run_after=2023-04-02T00:00:00+00:00[0m
[[34m2023-09-11T08:12:46.572+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-02T00:00:00+00:00, run_after=2023-04-03T00:00:00+00:00[0m
[[34m2023-09-11T08:12:46.613+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:46.614+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:46.614+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:46.616+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:46.617+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:46.617+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:46.619+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:48.442+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:48.573+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:48.750+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:48.784+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:49.236+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:49.237+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:49.306+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-01T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:49.392+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:50.125+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:50.143+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:49.482873+00:00, run_end_date=2023-09-11 08:12:49.690388+00:00, run_duration=0.207515, state=success, executor_state=success, try_number=1, max_tries=0, job_id=350, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:46.615042+00:00, queued_by_job_id=2, pid=50556[0m
[[34m2023-09-11T08:12:50.407+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-03T00:00:00+00:00, run_after=2023-04-04T00:00:00+00:00[0m
[[34m2023-09-11T08:12:50.448+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-01 00:00:00+00:00: scheduled__2023-04-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:46.567594+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:50.448+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-01 00:00:00+00:00, run_id=scheduled__2023-04-01T00:00:00+00:00, run_start_date=2023-09-11 08:12:46.583818+00:00, run_end_date=2023-09-11 08:12:50.448531+00:00, run_duration=3.864713, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-01 00:00:00+00:00, data_interval_end=2023-04-02 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:50.452+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-02T00:00:00+00:00, run_after=2023-04-03T00:00:00+00:00[0m
[[34m2023-09-11T08:12:50.469+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:50.470+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:50.470+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:50.472+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:50.473+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:50.473+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:50.476+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:52.377+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:52.512+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:52.691+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:52.724+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:53.234+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:53.235+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:53.313+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-02T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:53.375+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:54.124+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:54.135+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:53.472798+00:00, run_end_date=2023-09-11 08:12:53.682382+00:00, run_duration=0.209584, state=success, executor_state=success, try_number=1, max_tries=0, job_id=351, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:50.471162+00:00, queued_by_job_id=2, pid=50562[0m
[[34m2023-09-11T08:12:54.388+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-03T00:00:00+00:00, run_after=2023-04-04T00:00:00+00:00[0m
[[34m2023-09-11T08:12:54.412+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-02 00:00:00+00:00: scheduled__2023-04-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:50.401778+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:54.412+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-02 00:00:00+00:00, run_id=scheduled__2023-04-02T00:00:00+00:00, run_start_date=2023-09-11 08:12:50.420759+00:00, run_end_date=2023-09-11 08:12:54.412688+00:00, run_duration=3.991929, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-02 00:00:00+00:00, data_interval_end=2023-04-03 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:54.416+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-03T00:00:00+00:00, run_after=2023-04-04T00:00:00+00:00[0m
[[34m2023-09-11T08:12:55.410+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-04T00:00:00+00:00, run_after=2023-04-05T00:00:00+00:00[0m
[[34m2023-09-11T08:12:55.456+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:55.457+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:55.457+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:55.459+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:55.461+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:55.461+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:55.464+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:57.422+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:12:57.563+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:57.743+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:57.776+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:12:58.256+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:12:58.257+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:12:58.333+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-03T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:12:58.393+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:12:59.140+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:12:59.153+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:12:58.483119+00:00, run_end_date=2023-09-11 08:12:58.683869+00:00, run_duration=0.20075, state=success, executor_state=success, try_number=1, max_tries=0, job_id=352, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:55.457886+00:00, queued_by_job_id=2, pid=50571[0m
[[34m2023-09-11T08:12:59.409+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-05T00:00:00+00:00, run_after=2023-04-06T00:00:00+00:00[0m
[[34m2023-09-11T08:12:59.446+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-03 00:00:00+00:00: scheduled__2023-04-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:55.405893+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:12:59.446+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-03 00:00:00+00:00, run_id=scheduled__2023-04-03T00:00:00+00:00, run_start_date=2023-09-11 08:12:55.423244+00:00, run_end_date=2023-09-11 08:12:59.446799+00:00, run_duration=4.023555, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-03 00:00:00+00:00, data_interval_end=2023-04-04 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:12:59.450+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-04T00:00:00+00:00, run_after=2023-04-05T00:00:00+00:00[0m
[[34m2023-09-11T08:12:59.466+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:59.466+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:12:59.466+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:12:59.469+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:12:59.469+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:12:59.470+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:12:59.472+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:01.383+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:01.515+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:01.703+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:01.738+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:02.237+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:02.237+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:02.322+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-04T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:02.396+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:03.169+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:03.182+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:02.485440+00:00, run_end_date=2023-09-11 08:13:02.726000+00:00, run_duration=0.24056, state=success, executor_state=success, try_number=1, max_tries=0, job_id=353, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:12:59.467545+00:00, queued_by_job_id=2, pid=50579[0m
[[34m2023-09-11T08:13:03.457+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-05T00:00:00+00:00, run_after=2023-04-06T00:00:00+00:00[0m
[[34m2023-09-11T08:13:03.481+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-04 00:00:00+00:00: scheduled__2023-04-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:12:59.404377+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:03.481+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-04 00:00:00+00:00, run_id=scheduled__2023-04-04T00:00:00+00:00, run_start_date=2023-09-11 08:12:59.422954+00:00, run_end_date=2023-09-11 08:13:03.481604+00:00, run_duration=4.05865, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-04 00:00:00+00:00, data_interval_end=2023-04-05 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:03.485+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-05T00:00:00+00:00, run_after=2023-04-06T00:00:00+00:00[0m
[[34m2023-09-11T08:13:04.415+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-06T00:00:00+00:00, run_after=2023-04-07T00:00:00+00:00[0m
[[34m2023-09-11T08:13:04.461+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:04.461+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:04.462+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:04.464+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:04.464+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:04.465+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:04.467+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:06.346+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:06.478+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:06.650+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:06.682+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:07.170+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:07.170+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:07.248+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-05T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:07.305+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:08.032+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:08.043+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:07.405706+00:00, run_end_date=2023-09-11 08:13:07.604455+00:00, run_duration=0.198749, state=success, executor_state=success, try_number=1, max_tries=0, job_id=354, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:04.462806+00:00, queued_by_job_id=2, pid=50588[0m
[[34m2023-09-11T08:13:08.992+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-07T00:00:00+00:00, run_after=2023-04-08T00:00:00+00:00[0m
[[34m2023-09-11T08:13:09.049+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-05 00:00:00+00:00: scheduled__2023-04-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:04.410951+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:09.050+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-05 00:00:00+00:00, run_id=scheduled__2023-04-05T00:00:00+00:00, run_start_date=2023-09-11 08:13:04.428630+00:00, run_end_date=2023-09-11 08:13:09.050101+00:00, run_duration=4.621471, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-05 00:00:00+00:00, data_interval_end=2023-04-06 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:09.054+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-06T00:00:00+00:00, run_after=2023-04-07T00:00:00+00:00[0m
[[34m2023-09-11T08:13:09.070+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:09.071+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:09.071+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:09.073+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:09.074+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:09.074+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:09.078+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:10.977+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:11.113+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:11.296+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:11.329+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:11.823+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:11.823+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:11.900+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-06T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:11.958+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:12.682+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:12.694+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:12.051474+00:00, run_end_date=2023-09-11 08:13:12.250954+00:00, run_duration=0.19948, state=success, executor_state=success, try_number=1, max_tries=0, job_id=355, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:09.072309+00:00, queued_by_job_id=2, pid=50596[0m
[[34m2023-09-11T08:13:12.962+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-07T00:00:00+00:00, run_after=2023-04-08T00:00:00+00:00[0m
[[34m2023-09-11T08:13:12.986+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-06 00:00:00+00:00: scheduled__2023-04-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:08.986997+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:12.987+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-06 00:00:00+00:00, run_id=scheduled__2023-04-06T00:00:00+00:00, run_start_date=2023-09-11 08:13:09.022810+00:00, run_end_date=2023-09-11 08:13:12.987133+00:00, run_duration=3.964323, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-06 00:00:00+00:00, data_interval_end=2023-04-07 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:12.990+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-07T00:00:00+00:00, run_after=2023-04-08T00:00:00+00:00[0m
[[34m2023-09-11T08:13:13.652+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-08T00:00:00+00:00, run_after=2023-04-09T00:00:00+00:00[0m
[[34m2023-09-11T08:13:13.698+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:13.699+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:13.699+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:13.701+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:13.702+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:13.702+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:13.705+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:15.679+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:15.818+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:15.994+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:16.027+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:16.508+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:16.509+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:16.582+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-07T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:16.643+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:17.363+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:17.374+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:16.733975+00:00, run_end_date=2023-09-11 08:13:16.932379+00:00, run_duration=0.198404, state=success, executor_state=success, try_number=1, max_tries=0, job_id=356, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:13.699817+00:00, queued_by_job_id=2, pid=50605[0m
[[34m2023-09-11T08:13:17.680+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-09T00:00:00+00:00, run_after=2023-04-10T00:00:00+00:00[0m
[[34m2023-09-11T08:13:17.718+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-07 00:00:00+00:00: scheduled__2023-04-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:13.647648+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:17.718+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-07 00:00:00+00:00, run_id=scheduled__2023-04-07T00:00:00+00:00, run_start_date=2023-09-11 08:13:13.665922+00:00, run_end_date=2023-09-11 08:13:17.718849+00:00, run_duration=4.052927, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-07 00:00:00+00:00, data_interval_end=2023-04-08 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:17.722+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-08T00:00:00+00:00, run_after=2023-04-09T00:00:00+00:00[0m
[[34m2023-09-11T08:13:17.738+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:17.739+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:17.739+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:17.741+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:17.742+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:17.743+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:17.746+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:19.689+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:19.819+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:19.995+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:20.028+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:20.501+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:20.502+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:20.574+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-08T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:20.634+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:21.327+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:21.338+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:20.734329+00:00, run_end_date=2023-09-11 08:13:20.929284+00:00, run_duration=0.194955, state=success, executor_state=success, try_number=1, max_tries=0, job_id=357, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:17.740306+00:00, queued_by_job_id=2, pid=50613[0m
[[34m2023-09-11T08:13:21.580+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-09T00:00:00+00:00, run_after=2023-04-10T00:00:00+00:00[0m
[[34m2023-09-11T08:13:21.603+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-08 00:00:00+00:00: scheduled__2023-04-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:17.674765+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:21.604+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-08 00:00:00+00:00, run_id=scheduled__2023-04-08T00:00:00+00:00, run_start_date=2023-09-11 08:13:17.695024+00:00, run_end_date=2023-09-11 08:13:21.603937+00:00, run_duration=3.908913, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-08 00:00:00+00:00, data_interval_end=2023-04-09 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:21.607+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-09T00:00:00+00:00, run_after=2023-04-10T00:00:00+00:00[0m
[[34m2023-09-11T08:13:22.679+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-10T00:00:00+00:00, run_after=2023-04-11T00:00:00+00:00[0m
[[34m2023-09-11T08:13:22.725+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:22.725+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:22.726+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:22.728+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:22.728+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:22.729+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:22.731+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:24.808+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:24.962+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:25.196+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:25.239+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:25.739+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:25.739+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:25.812+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-09T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:25.873+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:26.596+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:26.606+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:25.968186+00:00, run_end_date=2023-09-11 08:13:26.170222+00:00, run_duration=0.202036, state=success, executor_state=success, try_number=1, max_tries=0, job_id=358, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:22.726781+00:00, queued_by_job_id=2, pid=50622[0m
[[34m2023-09-11T08:13:26.876+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-11T00:00:00+00:00, run_after=2023-04-12T00:00:00+00:00[0m
[[34m2023-09-11T08:13:26.914+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-09 00:00:00+00:00: scheduled__2023-04-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:22.674475+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:26.915+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-09 00:00:00+00:00, run_id=scheduled__2023-04-09T00:00:00+00:00, run_start_date=2023-09-11 08:13:22.692817+00:00, run_end_date=2023-09-11 08:13:26.915226+00:00, run_duration=4.222409, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-09 00:00:00+00:00, data_interval_end=2023-04-10 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:26.918+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-10T00:00:00+00:00, run_after=2023-04-11T00:00:00+00:00[0m
[[34m2023-09-11T08:13:26.935+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:26.935+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:26.936+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:26.938+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:26.938+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:26.939+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:26.941+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:28.878+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:29.012+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:29.194+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:29.229+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:29.715+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:29.715+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:29.785+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-10T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:29.845+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:30.550+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:30.563+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:29.931172+00:00, run_end_date=2023-09-11 08:13:30.124151+00:00, run_duration=0.192979, state=success, executor_state=success, try_number=1, max_tries=0, job_id=359, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:26.936798+00:00, queued_by_job_id=2, pid=50628[0m
[[34m2023-09-11T08:13:30.915+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-11T00:00:00+00:00, run_after=2023-04-12T00:00:00+00:00[0m
[[34m2023-09-11T08:13:30.938+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-10 00:00:00+00:00: scheduled__2023-04-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:26.869705+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:30.938+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-10 00:00:00+00:00, run_id=scheduled__2023-04-10T00:00:00+00:00, run_start_date=2023-09-11 08:13:26.890574+00:00, run_end_date=2023-09-11 08:13:30.938808+00:00, run_duration=4.048234, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-10 00:00:00+00:00, data_interval_end=2023-04-11 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:30.942+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-11T00:00:00+00:00, run_after=2023-04-12T00:00:00+00:00[0m
[[34m2023-09-11T08:13:31.877+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-12T00:00:00+00:00, run_after=2023-04-13T00:00:00+00:00[0m
[[34m2023-09-11T08:13:31.923+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:31.923+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:31.924+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:31.926+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:31.927+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:31.927+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:31.930+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:33.811+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:33.949+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:34.135+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:34.171+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:34.677+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:34.678+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:34.750+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-11T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:34.806+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:35.538+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:35.549+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:34.898139+00:00, run_end_date=2023-09-11 08:13:35.116019+00:00, run_duration=0.21788, state=success, executor_state=success, try_number=1, max_tries=0, job_id=360, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:31.924772+00:00, queued_by_job_id=2, pid=50637[0m
[[34m2023-09-11T08:13:35.707+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-13T00:00:00+00:00, run_after=2023-04-14T00:00:00+00:00[0m
[[34m2023-09-11T08:13:35.745+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-11 00:00:00+00:00: scheduled__2023-04-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:31.872205+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:35.745+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-11 00:00:00+00:00, run_id=scheduled__2023-04-11T00:00:00+00:00, run_start_date=2023-09-11 08:13:31.889391+00:00, run_end_date=2023-09-11 08:13:35.745740+00:00, run_duration=3.856349, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-11 00:00:00+00:00, data_interval_end=2023-04-12 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:35.748+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-12T00:00:00+00:00, run_after=2023-04-13T00:00:00+00:00[0m
[[34m2023-09-11T08:13:35.764+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:35.764+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:35.765+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:35.767+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:35.768+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:35.768+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:35.771+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:37.636+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:37.769+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:37.951+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:37.983+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:38.471+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:38.472+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:38.563+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-12T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:38.624+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:39.380+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:39.390+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:38.720497+00:00, run_end_date=2023-09-11 08:13:38.935310+00:00, run_duration=0.214813, state=success, executor_state=success, try_number=1, max_tries=0, job_id=361, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:35.765917+00:00, queued_by_job_id=2, pid=50643[0m
[[34m2023-09-11T08:13:39.639+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-13T00:00:00+00:00, run_after=2023-04-14T00:00:00+00:00[0m
[[34m2023-09-11T08:13:39.663+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-12 00:00:00+00:00: scheduled__2023-04-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:35.702412+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:39.664+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-12 00:00:00+00:00, run_id=scheduled__2023-04-12T00:00:00+00:00, run_start_date=2023-09-11 08:13:35.721285+00:00, run_end_date=2023-09-11 08:13:39.663995+00:00, run_duration=3.94271, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-12 00:00:00+00:00, data_interval_end=2023-04-13 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:39.668+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-13T00:00:00+00:00, run_after=2023-04-14T00:00:00+00:00[0m
[[34m2023-09-11T08:13:40.874+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-14T00:00:00+00:00, run_after=2023-04-15T00:00:00+00:00[0m
[[34m2023-09-11T08:13:40.927+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:40.928+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:40.928+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:40.930+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:40.931+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:40.931+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:40.933+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:43.074+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:43.215+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:43.395+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:43.433+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:43.934+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:43.934+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:44.018+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-13T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:44.080+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:44.812+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:44.823+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:44.178495+00:00, run_end_date=2023-09-11 08:13:44.391899+00:00, run_duration=0.213404, state=success, executor_state=success, try_number=1, max_tries=0, job_id=362, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:40.929104+00:00, queued_by_job_id=2, pid=50652[0m
[[34m2023-09-11T08:13:45.122+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-15T00:00:00+00:00, run_after=2023-04-16T00:00:00+00:00[0m
[[34m2023-09-11T08:13:45.156+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-13 00:00:00+00:00: scheduled__2023-04-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:40.869521+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:45.157+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-13 00:00:00+00:00, run_id=scheduled__2023-04-13T00:00:00+00:00, run_start_date=2023-09-11 08:13:40.886845+00:00, run_end_date=2023-09-11 08:13:45.157375+00:00, run_duration=4.27053, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-13 00:00:00+00:00, data_interval_end=2023-04-14 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:45.162+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-14T00:00:00+00:00, run_after=2023-04-15T00:00:00+00:00[0m
[[34m2023-09-11T08:13:45.180+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:45.180+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:45.180+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:45.182+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:45.183+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:45.183+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:45.186+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:47.178+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:47.311+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:47.498+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:47.530+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:48.024+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:48.025+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:48.101+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-14T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:48.166+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:48.895+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:48.905+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:48.255401+00:00, run_end_date=2023-09-11 08:13:48.464222+00:00, run_duration=0.208821, state=success, executor_state=success, try_number=1, max_tries=0, job_id=363, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:45.181365+00:00, queued_by_job_id=2, pid=50660[0m
[[34m2023-09-11T08:13:49.149+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-15T00:00:00+00:00, run_after=2023-04-16T00:00:00+00:00[0m
[[34m2023-09-11T08:13:49.175+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-14 00:00:00+00:00: scheduled__2023-04-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:45.115161+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:49.176+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-14 00:00:00+00:00, run_id=scheduled__2023-04-14T00:00:00+00:00, run_start_date=2023-09-11 08:13:45.134429+00:00, run_end_date=2023-09-11 08:13:49.175922+00:00, run_duration=4.041493, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-14 00:00:00+00:00, data_interval_end=2023-04-15 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:49.180+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-15T00:00:00+00:00, run_after=2023-04-16T00:00:00+00:00[0m
[[34m2023-09-11T08:13:50.100+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-16T00:00:00+00:00, run_after=2023-04-17T00:00:00+00:00[0m
[[34m2023-09-11T08:13:50.167+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:50.167+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:50.167+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:50.170+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:50.170+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:50.171+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:50.173+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:52.151+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:52.289+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:52.475+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:52.508+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:53.022+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:53.022+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:53.104+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-15T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:53.174+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:53.911+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:53.922+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:53.271517+00:00, run_end_date=2023-09-11 08:13:53.479526+00:00, run_duration=0.208009, state=success, executor_state=success, try_number=1, max_tries=0, job_id=364, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:50.168530+00:00, queued_by_job_id=2, pid=50669[0m
[[34m2023-09-11T08:13:54.183+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-17T00:00:00+00:00, run_after=2023-04-18T00:00:00+00:00[0m
[[34m2023-09-11T08:13:54.222+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-15 00:00:00+00:00: scheduled__2023-04-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:50.095794+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:54.222+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-15 00:00:00+00:00, run_id=scheduled__2023-04-15T00:00:00+00:00, run_start_date=2023-09-11 08:13:50.130741+00:00, run_end_date=2023-09-11 08:13:54.222495+00:00, run_duration=4.091754, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-15 00:00:00+00:00, data_interval_end=2023-04-16 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:54.226+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-16T00:00:00+00:00, run_after=2023-04-17T00:00:00+00:00[0m
[[34m2023-09-11T08:13:54.241+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:54.241+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:54.242+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:54.243+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:54.244+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:54.245+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:54.247+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:56.265+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:13:56.407+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:56.589+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:56.624+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:13:57.122+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:13:57.124+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:13:57.247+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-16T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:13:57.305+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:13:57.983+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:13:57.994+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:13:57.388404+00:00, run_end_date=2023-09-11 08:13:57.589570+00:00, run_duration=0.201166, state=success, executor_state=success, try_number=1, max_tries=0, job_id=365, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:54.242661+00:00, queued_by_job_id=2, pid=50677[0m
[[34m2023-09-11T08:13:58.337+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-17T00:00:00+00:00, run_after=2023-04-18T00:00:00+00:00[0m
[[34m2023-09-11T08:13:58.360+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-16 00:00:00+00:00: scheduled__2023-04-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:54.177742+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:13:58.360+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-16 00:00:00+00:00, run_id=scheduled__2023-04-16T00:00:00+00:00, run_start_date=2023-09-11 08:13:54.197818+00:00, run_end_date=2023-09-11 08:13:58.360461+00:00, run_duration=4.162643, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-16 00:00:00+00:00, data_interval_end=2023-04-17 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:13:58.363+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-17T00:00:00+00:00, run_after=2023-04-18T00:00:00+00:00[0m
[[34m2023-09-11T08:13:59.179+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-18T00:00:00+00:00, run_after=2023-04-19T00:00:00+00:00[0m
[[34m2023-09-11T08:13:59.224+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:59.224+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:13:59.225+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:13:59.227+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:13:59.227+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:13:59.227+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:13:59.230+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:01.067+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:01.191+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:01.361+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:01.392+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:01.838+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:01.838+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:01.913+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-17T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:01.969+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:02.669+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:02.679+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:02.055375+00:00, run_end_date=2023-09-11 08:14:02.254449+00:00, run_duration=0.199074, state=success, executor_state=success, try_number=1, max_tries=0, job_id=366, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:13:59.225687+00:00, queued_by_job_id=2, pid=50686[0m
[[34m2023-09-11T08:14:02.954+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-19T00:00:00+00:00, run_after=2023-04-20T00:00:00+00:00[0m
[[34m2023-09-11T08:14:02.989+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-17 00:00:00+00:00: scheduled__2023-04-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:13:59.175212+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:02.990+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-17 00:00:00+00:00, run_id=scheduled__2023-04-17T00:00:00+00:00, run_start_date=2023-09-11 08:13:59.193155+00:00, run_end_date=2023-09-11 08:14:02.990056+00:00, run_duration=3.796901, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-17 00:00:00+00:00, data_interval_end=2023-04-18 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:02.994+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-18T00:00:00+00:00, run_after=2023-04-19T00:00:00+00:00[0m
[[34m2023-09-11T08:14:03.010+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:03.010+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:03.011+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:03.013+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:03.013+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:03.013+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:03.016+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:05.321+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:05.476+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:05.655+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:05.694+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:06.281+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:06.282+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:06.414+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-18T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:06.487+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:07.577+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:07.587+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:06.609572+00:00, run_end_date=2023-09-11 08:14:06.991837+00:00, run_duration=0.382265, state=success, executor_state=success, try_number=1, max_tries=0, job_id=367, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:03.011706+00:00, queued_by_job_id=2, pid=50694[0m
[[34m2023-09-11T08:14:08.111+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-20T00:00:00+00:00, run_after=2023-04-21T00:00:00+00:00[0m
[[34m2023-09-11T08:14:08.149+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-18 00:00:00+00:00: scheduled__2023-04-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:02.949692+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:08.150+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-18 00:00:00+00:00, run_id=scheduled__2023-04-18T00:00:00+00:00, run_start_date=2023-09-11 08:14:02.966900+00:00, run_end_date=2023-09-11 08:14:08.150504+00:00, run_duration=5.183604, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-18 00:00:00+00:00, data_interval_end=2023-04-19 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:08.155+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-19T00:00:00+00:00, run_after=2023-04-20T00:00:00+00:00[0m
[[34m2023-09-11T08:14:08.171+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:08.172+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:08.172+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:08.175+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:08.175+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:08.176+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:08.178+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:10.499+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:10.653+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:10.861+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:10.903+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:11.435+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:11.436+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:11.523+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-19T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:11.593+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:12.482+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:12.494+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:11.709146+00:00, run_end_date=2023-09-11 08:14:12.026338+00:00, run_duration=0.317192, state=success, executor_state=success, try_number=1, max_tries=0, job_id=368, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:08.172993+00:00, queued_by_job_id=2, pid=50702[0m
[[34m2023-09-11T08:14:12.772+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-20T00:00:00+00:00, run_after=2023-04-21T00:00:00+00:00[0m
[[34m2023-09-11T08:14:12.797+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-19 00:00:00+00:00: scheduled__2023-04-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:08.106115+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:12.798+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-19 00:00:00+00:00, run_id=scheduled__2023-04-19T00:00:00+00:00, run_start_date=2023-09-11 08:14:08.125368+00:00, run_end_date=2023-09-11 08:14:12.797947+00:00, run_duration=4.672579, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-19 00:00:00+00:00, data_interval_end=2023-04-20 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:12.801+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-20T00:00:00+00:00, run_after=2023-04-21T00:00:00+00:00[0m
[[34m2023-09-11T08:14:14.266+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-21T00:00:00+00:00, run_after=2023-04-22T00:00:00+00:00[0m
[[34m2023-09-11T08:14:14.316+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:14.317+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:14.317+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:14.319+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:14.320+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:14.320+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:14.349+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:16.259+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:16.409+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:16.599+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:16.634+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:17.125+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:17.125+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:17.200+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-20T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:17.261+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:17.971+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:17.982+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:17.352406+00:00, run_end_date=2023-09-11 08:14:17.561435+00:00, run_duration=0.209029, state=success, executor_state=success, try_number=1, max_tries=0, job_id=369, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:14.318100+00:00, queued_by_job_id=2, pid=50712[0m
[[34m2023-09-11T08:14:18.251+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-22T00:00:00+00:00, run_after=2023-04-23T00:00:00+00:00[0m
[[34m2023-09-11T08:14:18.289+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-20 00:00:00+00:00: scheduled__2023-04-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:14.262146+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:18.290+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-20 00:00:00+00:00, run_id=scheduled__2023-04-20T00:00:00+00:00, run_start_date=2023-09-11 08:14:14.280382+00:00, run_end_date=2023-09-11 08:14:18.290241+00:00, run_duration=4.009859, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-20 00:00:00+00:00, data_interval_end=2023-04-21 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:18.293+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-21T00:00:00+00:00, run_after=2023-04-22T00:00:00+00:00[0m
[[34m2023-09-11T08:14:18.313+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:18.313+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:18.314+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:18.316+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:18.317+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:18.317+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:18.320+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:20.191+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:20.327+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:20.525+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:20.556+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:21.065+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:21.066+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:21.139+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-21T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:21.200+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:21.933+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:21.943+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:21.294388+00:00, run_end_date=2023-09-11 08:14:21.504235+00:00, run_duration=0.209847, state=success, executor_state=success, try_number=1, max_tries=0, job_id=370, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:18.315208+00:00, queued_by_job_id=2, pid=50720[0m
[[34m2023-09-11T08:14:22.182+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-22T00:00:00+00:00, run_after=2023-04-23T00:00:00+00:00[0m
[[34m2023-09-11T08:14:22.223+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-21 00:00:00+00:00: scheduled__2023-04-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:18.246120+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:22.224+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-21 00:00:00+00:00, run_id=scheduled__2023-04-21T00:00:00+00:00, run_start_date=2023-09-11 08:14:18.265380+00:00, run_end_date=2023-09-11 08:14:22.224085+00:00, run_duration=3.958705, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-21 00:00:00+00:00, data_interval_end=2023-04-22 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:22.227+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-22T00:00:00+00:00, run_after=2023-04-23T00:00:00+00:00[0m
[[34m2023-09-11T08:14:22.862+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-23T00:00:00+00:00, run_after=2023-04-24T00:00:00+00:00[0m
[[34m2023-09-11T08:14:22.916+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:22.917+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:22.917+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:22.919+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:22.920+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:22.921+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:22.925+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:24.839+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:24.976+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:25.158+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:25.203+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:25.759+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:25.759+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:25.851+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-22T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:25.937+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:26.783+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:26.804+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:26.059413+00:00, run_end_date=2023-09-11 08:14:26.299787+00:00, run_duration=0.240374, state=success, executor_state=success, try_number=1, max_tries=0, job_id=371, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:22.918548+00:00, queued_by_job_id=2, pid=50729[0m
[[34m2023-09-11T08:14:27.189+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-24T00:00:00+00:00, run_after=2023-04-25T00:00:00+00:00[0m
[[34m2023-09-11T08:14:27.265+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-22 00:00:00+00:00: scheduled__2023-04-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:22.858110+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:27.266+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-22 00:00:00+00:00, run_id=scheduled__2023-04-22T00:00:00+00:00, run_start_date=2023-09-11 08:14:22.878959+00:00, run_end_date=2023-09-11 08:14:27.266003+00:00, run_duration=4.387044, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-22 00:00:00+00:00, data_interval_end=2023-04-23 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:27.271+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-23T00:00:00+00:00, run_after=2023-04-24T00:00:00+00:00[0m
[[34m2023-09-11T08:14:27.297+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:27.298+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:27.298+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:27.301+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:27.301+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:27.301+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:27.304+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:29.610+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:29.739+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:30.096+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:30.191+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:31.110+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:31.111+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:31.211+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-23T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:31.301+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:32.307+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:32.320+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:31.448142+00:00, run_end_date=2023-09-11 08:14:31.687779+00:00, run_duration=0.239637, state=success, executor_state=success, try_number=1, max_tries=0, job_id=372, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:27.299335+00:00, queued_by_job_id=2, pid=50737[0m
[[34m2023-09-11T08:14:32.481+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-24T00:00:00+00:00, run_after=2023-04-25T00:00:00+00:00[0m
[[34m2023-09-11T08:14:32.510+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-23 00:00:00+00:00: scheduled__2023-04-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:27.181461+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:32.511+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-23 00:00:00+00:00, run_id=scheduled__2023-04-23T00:00:00+00:00, run_start_date=2023-09-11 08:14:27.213206+00:00, run_end_date=2023-09-11 08:14:32.511151+00:00, run_duration=5.297945, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-23 00:00:00+00:00, data_interval_end=2023-04-24 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:32.516+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-24T00:00:00+00:00, run_after=2023-04-25T00:00:00+00:00[0m
[[34m2023-09-11T08:14:33.850+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-25T00:00:00+00:00, run_after=2023-04-26T00:00:00+00:00[0m
[[34m2023-09-11T08:14:33.929+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:33.929+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:33.929+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:33.932+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:33.932+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:33.933+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:33.935+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:35.917+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:36.042+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:36.213+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:36.244+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:36.706+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:36.707+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:36.774+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-24T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:36.830+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:37.530+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:37.541+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:36.918331+00:00, run_end_date=2023-09-11 08:14:37.113272+00:00, run_duration=0.194941, state=success, executor_state=success, try_number=1, max_tries=0, job_id=373, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:33.930521+00:00, queued_by_job_id=2, pid=50747[0m
[[34m2023-09-11T08:14:37.713+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-26T00:00:00+00:00, run_after=2023-04-27T00:00:00+00:00[0m
[[34m2023-09-11T08:14:37.750+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-24 00:00:00+00:00: scheduled__2023-04-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:33.845665+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:37.750+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-24 00:00:00+00:00, run_id=scheduled__2023-04-24T00:00:00+00:00, run_start_date=2023-09-11 08:14:33.888598+00:00, run_end_date=2023-09-11 08:14:37.750568+00:00, run_duration=3.86197, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-24 00:00:00+00:00, data_interval_end=2023-04-25 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:37.754+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-25T00:00:00+00:00, run_after=2023-04-26T00:00:00+00:00[0m
[[34m2023-09-11T08:14:37.768+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:37.769+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:37.769+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:37.771+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:37.772+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:37.772+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:37.775+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:39.645+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:39.775+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:39.951+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:39.983+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:40.453+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:40.454+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:40.540+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-25T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:40.602+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:41.306+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:41.317+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:40.691423+00:00, run_end_date=2023-09-11 08:14:40.926358+00:00, run_duration=0.234935, state=success, executor_state=success, try_number=1, max_tries=0, job_id=374, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:37.770131+00:00, queued_by_job_id=2, pid=50755[0m
[[34m2023-09-11T08:14:41.455+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-26T00:00:00+00:00, run_after=2023-04-27T00:00:00+00:00[0m
[[34m2023-09-11T08:14:41.479+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-25 00:00:00+00:00: scheduled__2023-04-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:37.707989+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:41.479+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-25 00:00:00+00:00, run_id=scheduled__2023-04-25T00:00:00+00:00, run_start_date=2023-09-11 08:14:37.726281+00:00, run_end_date=2023-09-11 08:14:41.479647+00:00, run_duration=3.753366, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-25 00:00:00+00:00, data_interval_end=2023-04-26 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:41.483+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-26T00:00:00+00:00, run_after=2023-04-27T00:00:00+00:00[0m
[[34m2023-09-11T08:14:42.084+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-27T00:00:00+00:00, run_after=2023-04-28T00:00:00+00:00[0m
[[34m2023-09-11T08:14:42.130+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:42.130+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:42.131+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:42.133+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:42.133+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:42.134+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:42.136+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:44.003+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:44.135+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:44.302+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:44.336+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:44.780+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:44.780+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:44.848+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-26T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:44.903+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:45.562+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:45.573+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:44.987194+00:00, run_end_date=2023-09-11 08:14:45.173762+00:00, run_duration=0.186568, state=success, executor_state=success, try_number=1, max_tries=0, job_id=375, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:42.131737+00:00, queued_by_job_id=2, pid=50762[0m
[[34m2023-09-11T08:14:45.849+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-28T00:00:00+00:00, run_after=2023-04-29T00:00:00+00:00[0m
[[34m2023-09-11T08:14:45.886+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-26 00:00:00+00:00: scheduled__2023-04-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:42.079599+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:45.886+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-26 00:00:00+00:00, run_id=scheduled__2023-04-26T00:00:00+00:00, run_start_date=2023-09-11 08:14:42.096386+00:00, run_end_date=2023-09-11 08:14:45.886567+00:00, run_duration=3.790181, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-26 00:00:00+00:00, data_interval_end=2023-04-27 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:45.890+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-27T00:00:00+00:00, run_after=2023-04-28T00:00:00+00:00[0m
[[34m2023-09-11T08:14:45.905+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:45.906+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:45.906+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:45.908+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:45.909+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:45.909+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:45.911+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:47.821+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:47.973+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:48.150+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:48.183+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:48.780+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:48.781+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:48.850+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-27T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:48.907+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:49.666+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:49.677+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:49.036486+00:00, run_end_date=2023-09-11 08:14:49.278543+00:00, run_duration=0.242057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=376, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:45.906903+00:00, queued_by_job_id=2, pid=50768[0m
[[34m2023-09-11T08:14:50.069+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-28T00:00:00+00:00, run_after=2023-04-29T00:00:00+00:00[0m
[[34m2023-09-11T08:14:50.094+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-27 00:00:00+00:00: scheduled__2023-04-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:45.844887+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:50.095+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-27 00:00:00+00:00, run_id=scheduled__2023-04-27T00:00:00+00:00, run_start_date=2023-09-11 08:14:45.863047+00:00, run_end_date=2023-09-11 08:14:50.095265+00:00, run_duration=4.232218, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-27 00:00:00+00:00, data_interval_end=2023-04-28 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:50.098+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-28T00:00:00+00:00, run_after=2023-04-29T00:00:00+00:00[0m
[[34m2023-09-11T08:14:50.858+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-29T00:00:00+00:00, run_after=2023-04-30T00:00:00+00:00[0m
[[34m2023-09-11T08:14:50.907+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:50.908+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:50.908+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:50.911+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:50.911+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:50.911+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:50.914+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:53.120+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:53.251+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:53.447+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:53.496+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:53.984+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:53.985+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:54.054+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-28T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:54.122+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:54.830+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:54.841+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:54.212604+00:00, run_end_date=2023-09-11 08:14:54.427786+00:00, run_duration=0.215182, state=success, executor_state=success, try_number=1, max_tries=0, job_id=377, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:50.909456+00:00, queued_by_job_id=2, pid=50777[0m
[[34m2023-09-11T08:14:55.111+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-30T00:00:00+00:00, run_after=2023-05-01T00:00:00+00:00[0m
[[34m2023-09-11T08:14:55.155+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-28 00:00:00+00:00: scheduled__2023-04-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:50.851763+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:55.155+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-28 00:00:00+00:00, run_id=scheduled__2023-04-28T00:00:00+00:00, run_start_date=2023-09-11 08:14:50.870166+00:00, run_end_date=2023-09-11 08:14:55.155687+00:00, run_duration=4.285521, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-28 00:00:00+00:00, data_interval_end=2023-04-29 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:55.159+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-29T00:00:00+00:00, run_after=2023-04-30T00:00:00+00:00[0m
[[34m2023-09-11T08:14:55.175+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:55.175+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:55.175+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:55.177+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:55.178+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:55.178+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:55.181+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:57.154+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:14:57.321+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:57.507+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:57.538+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:14:58.069+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:14:58.070+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:14:58.149+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-29T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:14:58.214+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:14:58.952+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:14:58.968+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:14:58.308747+00:00, run_end_date=2023-09-11 08:14:58.523093+00:00, run_duration=0.214346, state=success, executor_state=success, try_number=1, max_tries=0, job_id=378, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:55.176364+00:00, queued_by_job_id=2, pid=50785[0m
[[34m2023-09-11T08:14:59.264+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-30T00:00:00+00:00, run_after=2023-05-01T00:00:00+00:00[0m
[[34m2023-09-11T08:14:59.290+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-29 00:00:00+00:00: scheduled__2023-04-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:55.103230+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:14:59.291+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-29 00:00:00+00:00, run_id=scheduled__2023-04-29T00:00:00+00:00, run_start_date=2023-09-11 08:14:55.129617+00:00, run_end_date=2023-09-11 08:14:59.291167+00:00, run_duration=4.16155, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-29 00:00:00+00:00, data_interval_end=2023-04-30 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:14:59.294+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-04-30T00:00:00+00:00, run_after=2023-05-01T00:00:00+00:00[0m
[[34m2023-09-11T08:14:59.940+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-01T00:00:00+00:00, run_after=2023-05-02T00:00:00+00:00[0m
[[34m2023-09-11T08:14:59.993+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-04-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:59.993+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:14:59.994+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-04-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:14:59.996+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:14:59.996+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:14:59.997+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:14:59.999+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-04-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:01.933+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:02.069+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:02.237+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:02.274+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:02.719+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:02.720+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:02.790+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-04-30T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:02.846+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-04-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:03.549+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-04-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:03.564+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-04-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:02.938525+00:00, run_end_date=2023-09-11 08:15:03.141565+00:00, run_duration=0.20304, state=success, executor_state=success, try_number=1, max_tries=0, job_id=379, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:14:59.994741+00:00, queued_by_job_id=2, pid=50794[0m
[[34m2023-09-11T08:15:03.834+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-02T00:00:00+00:00, run_after=2023-05-03T00:00:00+00:00[0m
[[34m2023-09-11T08:15:03.872+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-04-30 00:00:00+00:00: scheduled__2023-04-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:14:59.934898+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:03.873+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-04-30 00:00:00+00:00, run_id=scheduled__2023-04-30T00:00:00+00:00, run_start_date=2023-09-11 08:14:59.960949+00:00, run_end_date=2023-09-11 08:15:03.873126+00:00, run_duration=3.912177, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-30 00:00:00+00:00, data_interval_end=2023-05-01 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:03.879+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-01T00:00:00+00:00, run_after=2023-05-02T00:00:00+00:00[0m
[[34m2023-09-11T08:15:03.914+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:03.914+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:03.914+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:03.916+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:03.917+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:03.917+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:03.920+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:05.816+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:05.949+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:06.125+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:06.159+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:06.611+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:06.612+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:06.682+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-01T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:06.741+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:07.440+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:07.452+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:06.828418+00:00, run_end_date=2023-09-11 08:15:07.023220+00:00, run_duration=0.194802, state=success, executor_state=success, try_number=1, max_tries=0, job_id=380, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:03.915210+00:00, queued_by_job_id=2, pid=50802[0m
[[34m2023-09-11T08:15:07.464+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:15:07.698+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-02T00:00:00+00:00, run_after=2023-05-03T00:00:00+00:00[0m
[[34m2023-09-11T08:15:07.720+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-01 00:00:00+00:00: scheduled__2023-05-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:03.829318+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:07.721+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-01 00:00:00+00:00, run_id=scheduled__2023-05-01T00:00:00+00:00, run_start_date=2023-09-11 08:15:03.847634+00:00, run_end_date=2023-09-11 08:15:07.721419+00:00, run_duration=3.873785, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-01 00:00:00+00:00, data_interval_end=2023-05-02 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:07.724+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-02T00:00:00+00:00, run_after=2023-05-03T00:00:00+00:00[0m
[[34m2023-09-11T08:15:08.887+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-03T00:00:00+00:00, run_after=2023-05-04T00:00:00+00:00[0m
[[34m2023-09-11T08:15:08.930+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:08.930+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:08.931+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:08.933+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:08.933+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:08.934+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:08.937+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:10.781+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:10.910+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:11.083+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:11.116+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:11.578+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:11.578+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:11.646+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-02T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:11.704+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:12.432+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:12.442+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:11.790377+00:00, run_end_date=2023-09-11 08:15:12.001929+00:00, run_duration=0.211552, state=success, executor_state=success, try_number=1, max_tries=0, job_id=381, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:08.931879+00:00, queued_by_job_id=2, pid=50811[0m
[[34m2023-09-11T08:15:12.698+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-04T00:00:00+00:00, run_after=2023-05-05T00:00:00+00:00[0m
[[34m2023-09-11T08:15:12.735+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-02 00:00:00+00:00: scheduled__2023-05-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:08.882723+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:12.735+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-02 00:00:00+00:00, run_id=scheduled__2023-05-02T00:00:00+00:00, run_start_date=2023-09-11 08:15:08.899401+00:00, run_end_date=2023-09-11 08:15:12.735887+00:00, run_duration=3.836486, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-02 00:00:00+00:00, data_interval_end=2023-05-03 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:12.739+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-03T00:00:00+00:00, run_after=2023-05-04T00:00:00+00:00[0m
[[34m2023-09-11T08:15:12.754+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:12.754+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:12.755+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:12.757+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:12.757+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:12.758+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:12.761+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:14.785+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:14.918+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:15.095+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:15.131+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:15.583+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:15.583+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:15.653+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-03T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:15.708+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:16.414+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:16.426+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:15.795343+00:00, run_end_date=2023-09-11 08:15:15.993512+00:00, run_duration=0.198169, state=success, executor_state=success, try_number=1, max_tries=0, job_id=382, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:12.755729+00:00, queued_by_job_id=2, pid=50819[0m
[[34m2023-09-11T08:15:16.714+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-04T00:00:00+00:00, run_after=2023-05-05T00:00:00+00:00[0m
[[34m2023-09-11T08:15:16.736+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-03 00:00:00+00:00: scheduled__2023-05-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:12.693573+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:16.737+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-03 00:00:00+00:00, run_id=scheduled__2023-05-03T00:00:00+00:00, run_start_date=2023-09-11 08:15:12.711928+00:00, run_end_date=2023-09-11 08:15:16.736969+00:00, run_duration=4.025041, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-03 00:00:00+00:00, data_interval_end=2023-05-04 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:16.740+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-04T00:00:00+00:00, run_after=2023-05-05T00:00:00+00:00[0m
[[34m2023-09-11T08:15:17.939+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-05T00:00:00+00:00, run_after=2023-05-06T00:00:00+00:00[0m
[[34m2023-09-11T08:15:17.985+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:17.985+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:17.985+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:17.988+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:17.988+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:17.989+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:17.991+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:19.801+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:19.931+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:20.103+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:20.135+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:20.590+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:20.591+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:20.657+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-04T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:20.711+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:21.362+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:21.373+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:20.792910+00:00, run_end_date=2023-09-11 08:15:20.982259+00:00, run_duration=0.189349, state=success, executor_state=success, try_number=1, max_tries=0, job_id=383, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:17.986312+00:00, queued_by_job_id=2, pid=50829[0m
[[34m2023-09-11T08:15:21.654+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-06T00:00:00+00:00, run_after=2023-05-07T00:00:00+00:00[0m
[[34m2023-09-11T08:15:21.697+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-04 00:00:00+00:00: scheduled__2023-05-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:17.934695+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:21.697+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-04 00:00:00+00:00, run_id=scheduled__2023-05-04T00:00:00+00:00, run_start_date=2023-09-11 08:15:17.952748+00:00, run_end_date=2023-09-11 08:15:21.697391+00:00, run_duration=3.744643, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-04 00:00:00+00:00, data_interval_end=2023-05-05 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:21.701+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-05T00:00:00+00:00, run_after=2023-05-06T00:00:00+00:00[0m
[[34m2023-09-11T08:15:21.716+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:21.716+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:21.717+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:21.719+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:21.719+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:21.720+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:21.723+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:23.685+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:23.820+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:23.996+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:24.026+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:24.491+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:24.492+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:24.562+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-05T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:24.616+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:25.325+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:25.337+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:24.707021+00:00, run_end_date=2023-09-11 08:15:24.903401+00:00, run_duration=0.19638, state=success, executor_state=success, try_number=1, max_tries=0, job_id=384, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:21.717732+00:00, queued_by_job_id=2, pid=50835[0m
[[34m2023-09-11T08:15:25.939+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-06T00:00:00+00:00, run_after=2023-05-07T00:00:00+00:00[0m
[[34m2023-09-11T08:15:25.988+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-05 00:00:00+00:00: scheduled__2023-05-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:21.647422+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:25.988+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-05 00:00:00+00:00, run_id=scheduled__2023-05-05T00:00:00+00:00, run_start_date=2023-09-11 08:15:21.672840+00:00, run_end_date=2023-09-11 08:15:25.988706+00:00, run_duration=4.315866, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-05 00:00:00+00:00, data_interval_end=2023-05-06 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:25.993+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-06T00:00:00+00:00, run_after=2023-05-07T00:00:00+00:00[0m
[[34m2023-09-11T08:15:26.849+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-07T00:00:00+00:00, run_after=2023-05-08T00:00:00+00:00[0m
[[34m2023-09-11T08:15:26.901+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:26.902+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:26.902+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:26.905+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:26.905+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:26.906+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:26.908+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:28.759+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:28.891+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:29.071+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:29.103+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:29.553+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:29.554+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:29.625+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-06T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:29.682+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:30.407+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:30.418+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:29.769957+00:00, run_end_date=2023-09-11 08:15:29.968769+00:00, run_duration=0.198812, state=success, executor_state=success, try_number=1, max_tries=0, job_id=385, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:26.903200+00:00, queued_by_job_id=2, pid=50844[0m
[[34m2023-09-11T08:15:30.874+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-08T00:00:00+00:00, run_after=2023-05-09T00:00:00+00:00[0m
[[34m2023-09-11T08:15:30.911+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-06 00:00:00+00:00: scheduled__2023-05-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:26.844383+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:30.911+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-06 00:00:00+00:00, run_id=scheduled__2023-05-06T00:00:00+00:00, run_start_date=2023-09-11 08:15:26.863591+00:00, run_end_date=2023-09-11 08:15:30.911681+00:00, run_duration=4.04809, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-06 00:00:00+00:00, data_interval_end=2023-05-07 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:30.915+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-07T00:00:00+00:00, run_after=2023-05-08T00:00:00+00:00[0m
[[34m2023-09-11T08:15:30.930+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:30.930+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:30.931+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:30.933+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:30.933+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:30.934+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:30.937+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:32.834+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:32.960+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:33.154+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:33.187+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:33.649+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:33.650+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:33.718+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-07T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:33.774+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:34.446+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:34.461+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:33.859866+00:00, run_end_date=2023-09-11 08:15:34.058261+00:00, run_duration=0.198395, state=success, executor_state=success, try_number=1, max_tries=0, job_id=386, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:30.931818+00:00, queued_by_job_id=2, pid=50850[0m
[[34m2023-09-11T08:15:34.817+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-08T00:00:00+00:00, run_after=2023-05-09T00:00:00+00:00[0m
[[34m2023-09-11T08:15:34.840+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-07 00:00:00+00:00: scheduled__2023-05-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:30.869167+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:34.841+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-07 00:00:00+00:00, run_id=scheduled__2023-05-07T00:00:00+00:00, run_start_date=2023-09-11 08:15:30.887934+00:00, run_end_date=2023-09-11 08:15:34.841161+00:00, run_duration=3.953227, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-07 00:00:00+00:00, data_interval_end=2023-05-08 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:34.844+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-08T00:00:00+00:00, run_after=2023-05-09T00:00:00+00:00[0m
[[34m2023-09-11T08:15:35.992+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-09T00:00:00+00:00, run_after=2023-05-10T00:00:00+00:00[0m
[[34m2023-09-11T08:15:36.038+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:36.038+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:36.039+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:36.041+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:36.041+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:36.042+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:36.045+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:37.909+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:38.042+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:38.211+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:38.245+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:38.712+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:38.713+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:38.788+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-08T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:38.846+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:39.618+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:39.629+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:38.932506+00:00, run_end_date=2023-09-11 08:15:39.133438+00:00, run_duration=0.200932, state=success, executor_state=success, try_number=1, max_tries=0, job_id=387, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:36.039741+00:00, queued_by_job_id=2, pid=50859[0m
[[34m2023-09-11T08:15:39.795+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-10T00:00:00+00:00, run_after=2023-05-11T00:00:00+00:00[0m
[[34m2023-09-11T08:15:39.831+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-08 00:00:00+00:00: scheduled__2023-05-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:35.987244+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:39.831+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-08 00:00:00+00:00, run_id=scheduled__2023-05-08T00:00:00+00:00, run_start_date=2023-09-11 08:15:36.005870+00:00, run_end_date=2023-09-11 08:15:39.831778+00:00, run_duration=3.825908, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-08 00:00:00+00:00, data_interval_end=2023-05-09 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:39.835+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-09T00:00:00+00:00, run_after=2023-05-10T00:00:00+00:00[0m
[[34m2023-09-11T08:15:39.850+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:39.851+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:39.851+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:39.854+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:39.854+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:39.854+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:39.857+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:41.706+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:41.837+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:42.007+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:42.039+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:42.512+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:42.513+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:42.582+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-09T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:42.639+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:43.514+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:43.525+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:42.727120+00:00, run_end_date=2023-09-11 08:15:42.974010+00:00, run_duration=0.24689, state=success, executor_state=success, try_number=1, max_tries=0, job_id=388, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:39.852224+00:00, queued_by_job_id=2, pid=50867[0m
[[34m2023-09-11T08:15:43.772+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-10T00:00:00+00:00, run_after=2023-05-11T00:00:00+00:00[0m
[[34m2023-09-11T08:15:43.794+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-09 00:00:00+00:00: scheduled__2023-05-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:39.790475+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:43.795+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-09 00:00:00+00:00, run_id=scheduled__2023-05-09T00:00:00+00:00, run_start_date=2023-09-11 08:15:39.808971+00:00, run_end_date=2023-09-11 08:15:43.795308+00:00, run_duration=3.986337, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-09 00:00:00+00:00, data_interval_end=2023-05-10 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:43.798+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-10T00:00:00+00:00, run_after=2023-05-11T00:00:00+00:00[0m
[[34m2023-09-11T08:15:44.911+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-11T00:00:00+00:00, run_after=2023-05-12T00:00:00+00:00[0m
[[34m2023-09-11T08:15:44.957+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:44.957+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:44.957+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:44.960+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:44.961+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:44.961+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:44.964+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:46.818+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:46.947+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:47.123+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:47.157+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:47.609+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:47.610+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:47.679+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-10T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:47.734+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:48.622+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:48.633+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:47.819691+00:00, run_end_date=2023-09-11 08:15:48.011303+00:00, run_duration=0.191612, state=success, executor_state=success, try_number=1, max_tries=0, job_id=389, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:44.958583+00:00, queued_by_job_id=2, pid=50876[0m
[[34m2023-09-11T08:15:48.904+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-12T00:00:00+00:00, run_after=2023-05-13T00:00:00+00:00[0m
[[34m2023-09-11T08:15:48.941+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-10 00:00:00+00:00: scheduled__2023-05-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:44.907152+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:48.942+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-10 00:00:00+00:00, run_id=scheduled__2023-05-10T00:00:00+00:00, run_start_date=2023-09-11 08:15:44.924986+00:00, run_end_date=2023-09-11 08:15:48.942117+00:00, run_duration=4.017131, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-10 00:00:00+00:00, data_interval_end=2023-05-11 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:48.954+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-11T00:00:00+00:00, run_after=2023-05-12T00:00:00+00:00[0m
[[34m2023-09-11T08:15:48.971+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:48.971+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:48.971+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:48.974+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:48.974+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:48.975+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:48.998+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:50.900+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:51.030+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:51.205+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:51.240+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:51.705+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:51.706+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:51.777+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-11T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:51.834+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:52.574+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:52.586+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:51.926909+00:00, run_end_date=2023-09-11 08:15:52.125038+00:00, run_duration=0.198129, state=success, executor_state=success, try_number=1, max_tries=0, job_id=390, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:48.972463+00:00, queued_by_job_id=2, pid=50884[0m
[[34m2023-09-11T08:15:52.829+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-12T00:00:00+00:00, run_after=2023-05-13T00:00:00+00:00[0m
[[34m2023-09-11T08:15:52.853+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-11 00:00:00+00:00: scheduled__2023-05-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:48.899098+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:52.853+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-11 00:00:00+00:00, run_id=scheduled__2023-05-11T00:00:00+00:00, run_start_date=2023-09-11 08:15:48.917431+00:00, run_end_date=2023-09-11 08:15:52.853502+00:00, run_duration=3.936071, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-11 00:00:00+00:00, data_interval_end=2023-05-12 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:52.857+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-12T00:00:00+00:00, run_after=2023-05-13T00:00:00+00:00[0m
[[34m2023-09-11T08:15:53.784+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-13T00:00:00+00:00, run_after=2023-05-14T00:00:00+00:00[0m
[[34m2023-09-11T08:15:53.840+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:53.840+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:53.841+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:53.843+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:53.843+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:53.844+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:53.846+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:55.688+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:55.815+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:55.982+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:56.016+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:56.499+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:15:56.500+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:56.571+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-12T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:15:56.628+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:15:57.312+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:15:57.322+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:15:56.713933+00:00, run_end_date=2023-09-11 08:15:56.900990+00:00, run_duration=0.187057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=391, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:53.841712+00:00, queued_by_job_id=2, pid=50893[0m
[[34m2023-09-11T08:15:57.647+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-14T00:00:00+00:00, run_after=2023-05-15T00:00:00+00:00[0m
[[34m2023-09-11T08:15:57.682+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-12 00:00:00+00:00: scheduled__2023-05-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:53.779231+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:15:57.683+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-12 00:00:00+00:00, run_id=scheduled__2023-05-12T00:00:00+00:00, run_start_date=2023-09-11 08:15:53.797513+00:00, run_end_date=2023-09-11 08:15:57.683177+00:00, run_duration=3.885664, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-12 00:00:00+00:00, data_interval_end=2023-05-13 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:15:57.686+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-13T00:00:00+00:00, run_after=2023-05-14T00:00:00+00:00[0m
[[34m2023-09-11T08:15:57.701+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:57.702+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:15:57.702+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:15:57.704+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:15:57.705+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:15:57.705+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:57.708+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:15:59.557+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:15:59.682+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:15:59.845+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:15:59.876+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:00.336+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:00.337+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:00.415+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-13T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:00.473+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:01.175+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:01.185+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:00.561483+00:00, run_end_date=2023-09-11 08:16:00.759935+00:00, run_duration=0.198452, state=success, executor_state=success, try_number=1, max_tries=0, job_id=392, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:15:57.702971+00:00, queued_by_job_id=2, pid=50899[0m
[[34m2023-09-11T08:16:01.354+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-14T00:00:00+00:00, run_after=2023-05-15T00:00:00+00:00[0m
[[34m2023-09-11T08:16:01.377+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-13 00:00:00+00:00: scheduled__2023-05-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:15:57.643037+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:01.378+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-13 00:00:00+00:00, run_id=scheduled__2023-05-13T00:00:00+00:00, run_start_date=2023-09-11 08:15:57.659967+00:00, run_end_date=2023-09-11 08:16:01.378272+00:00, run_duration=3.718305, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-13 00:00:00+00:00, data_interval_end=2023-05-14 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:01.381+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-14T00:00:00+00:00, run_after=2023-05-15T00:00:00+00:00[0m
[[34m2023-09-11T08:16:02.611+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-15T00:00:00+00:00, run_after=2023-05-16T00:00:00+00:00[0m
[[34m2023-09-11T08:16:02.655+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:02.656+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:02.656+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:02.658+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:02.658+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:02.659+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:02.661+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:04.555+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:04.688+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:04.858+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:04.889+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:05.353+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:05.353+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:05.421+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-14T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:05.480+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:06.205+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:06.216+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:05.566436+00:00, run_end_date=2023-09-11 08:16:05.768875+00:00, run_duration=0.202439, state=success, executor_state=success, try_number=1, max_tries=0, job_id=393, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:02.657013+00:00, queued_by_job_id=2, pid=50908[0m
[[34m2023-09-11T08:16:06.474+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-16T00:00:00+00:00, run_after=2023-05-17T00:00:00+00:00[0m
[[34m2023-09-11T08:16:06.511+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-14 00:00:00+00:00: scheduled__2023-05-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:02.606696+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:06.512+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-14 00:00:00+00:00, run_id=scheduled__2023-05-14T00:00:00+00:00, run_start_date=2023-09-11 08:16:02.624138+00:00, run_end_date=2023-09-11 08:16:06.512240+00:00, run_duration=3.888102, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-14 00:00:00+00:00, data_interval_end=2023-05-15 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:06.515+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-15T00:00:00+00:00, run_after=2023-05-16T00:00:00+00:00[0m
[[34m2023-09-11T08:16:06.531+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:06.531+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:06.531+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:06.533+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:06.534+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:06.534+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:06.537+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:08.404+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:08.535+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:08.701+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:08.731+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:09.183+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:09.184+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:09.252+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-15T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:09.309+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:09.999+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:10.012+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:09.422272+00:00, run_end_date=2023-09-11 08:16:09.616881+00:00, run_duration=0.194609, state=success, executor_state=success, try_number=1, max_tries=0, job_id=394, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:06.532289+00:00, queued_by_job_id=2, pid=50914[0m
[[34m2023-09-11T08:16:10.257+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-16T00:00:00+00:00, run_after=2023-05-17T00:00:00+00:00[0m
[[34m2023-09-11T08:16:10.279+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-15 00:00:00+00:00: scheduled__2023-05-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:06.469517+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:10.279+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-15 00:00:00+00:00, run_id=scheduled__2023-05-15T00:00:00+00:00, run_start_date=2023-09-11 08:16:06.488430+00:00, run_end_date=2023-09-11 08:16:10.279498+00:00, run_duration=3.791068, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-15 00:00:00+00:00, data_interval_end=2023-05-16 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:10.283+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-16T00:00:00+00:00, run_after=2023-05-17T00:00:00+00:00[0m
[[34m2023-09-11T08:16:11.478+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-17T00:00:00+00:00, run_after=2023-05-18T00:00:00+00:00[0m
[[34m2023-09-11T08:16:11.523+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:11.524+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:11.524+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:11.526+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:11.527+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:11.527+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:11.530+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:13.375+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:13.511+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:13.707+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:13.738+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:14.214+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:14.215+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:14.282+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-16T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:14.345+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:15.061+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:15.072+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:14.430743+00:00, run_end_date=2023-09-11 08:16:14.653798+00:00, run_duration=0.223055, state=success, executor_state=success, try_number=1, max_tries=0, job_id=395, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:11.525151+00:00, queued_by_job_id=2, pid=50923[0m
[[34m2023-09-11T08:16:15.331+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-18T00:00:00+00:00, run_after=2023-05-19T00:00:00+00:00[0m
[[34m2023-09-11T08:16:15.370+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-16 00:00:00+00:00: scheduled__2023-05-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:11.473139+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:15.371+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-16 00:00:00+00:00, run_id=scheduled__2023-05-16T00:00:00+00:00, run_start_date=2023-09-11 08:16:11.491213+00:00, run_end_date=2023-09-11 08:16:15.371322+00:00, run_duration=3.880109, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-16 00:00:00+00:00, data_interval_end=2023-05-17 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:15.375+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-17T00:00:00+00:00, run_after=2023-05-18T00:00:00+00:00[0m
[[34m2023-09-11T08:16:15.391+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:15.392+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:15.392+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:15.394+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:15.395+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:15.395+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:15.398+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:17.347+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:17.480+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:17.646+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:17.676+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:18.122+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:18.123+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:18.191+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-17T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:18.249+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:18.974+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:18.985+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:18.338549+00:00, run_end_date=2023-09-11 08:16:18.541489+00:00, run_duration=0.20294, state=success, executor_state=success, try_number=1, max_tries=0, job_id=396, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:15.393224+00:00, queued_by_job_id=2, pid=50931[0m
[[34m2023-09-11T08:16:19.237+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-18T00:00:00+00:00, run_after=2023-05-19T00:00:00+00:00[0m
[[34m2023-09-11T08:16:19.259+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-17 00:00:00+00:00: scheduled__2023-05-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:15.326376+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:19.259+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-17 00:00:00+00:00, run_id=scheduled__2023-05-17T00:00:00+00:00, run_start_date=2023-09-11 08:16:15.345753+00:00, run_end_date=2023-09-11 08:16:19.259355+00:00, run_duration=3.913602, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-17 00:00:00+00:00, data_interval_end=2023-05-18 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:19.263+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-18T00:00:00+00:00, run_after=2023-05-19T00:00:00+00:00[0m
[[34m2023-09-11T08:16:20.185+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-19T00:00:00+00:00, run_after=2023-05-20T00:00:00+00:00[0m
[[34m2023-09-11T08:16:20.228+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:20.228+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:20.228+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:20.231+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:20.231+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:20.231+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:20.234+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:22.090+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:22.246+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:22.472+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:22.506+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:22.978+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:22.979+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:23.052+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-18T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:23.110+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:23.824+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:23.836+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:23.200055+00:00, run_end_date=2023-09-11 08:16:23.408286+00:00, run_duration=0.208231, state=success, executor_state=success, try_number=1, max_tries=0, job_id=397, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:20.229553+00:00, queued_by_job_id=2, pid=50940[0m
[[34m2023-09-11T08:16:24.211+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-20T00:00:00+00:00, run_after=2023-05-21T00:00:00+00:00[0m
[[34m2023-09-11T08:16:24.319+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-18 00:00:00+00:00: scheduled__2023-05-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:20.180200+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:24.320+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-18 00:00:00+00:00, run_id=scheduled__2023-05-18T00:00:00+00:00, run_start_date=2023-09-11 08:16:20.197107+00:00, run_end_date=2023-09-11 08:16:24.320133+00:00, run_duration=4.123026, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-18 00:00:00+00:00, data_interval_end=2023-05-19 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:24.323+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-19T00:00:00+00:00, run_after=2023-05-20T00:00:00+00:00[0m
[[34m2023-09-11T08:16:24.339+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:24.340+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:24.340+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:24.342+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:24.343+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:24.343+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:24.346+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:26.272+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:26.409+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:26.585+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:26.618+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:27.094+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:27.094+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:27.171+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-19T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:27.228+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:27.952+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:27.964+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:27.323605+00:00, run_end_date=2023-09-11 08:16:27.530730+00:00, run_duration=0.207125, state=success, executor_state=success, try_number=1, max_tries=0, job_id=398, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:24.341292+00:00, queued_by_job_id=2, pid=50948[0m
[[34m2023-09-11T08:16:28.108+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-20T00:00:00+00:00, run_after=2023-05-21T00:00:00+00:00[0m
[[34m2023-09-11T08:16:28.132+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-19 00:00:00+00:00: scheduled__2023-05-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:24.206386+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:28.133+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-19 00:00:00+00:00, run_id=scheduled__2023-05-19T00:00:00+00:00, run_start_date=2023-09-11 08:16:24.294725+00:00, run_end_date=2023-09-11 08:16:28.132988+00:00, run_duration=3.838263, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-19 00:00:00+00:00, data_interval_end=2023-05-20 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:28.136+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-20T00:00:00+00:00, run_after=2023-05-21T00:00:00+00:00[0m
[[34m2023-09-11T08:16:28.999+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-21T00:00:00+00:00, run_after=2023-05-22T00:00:00+00:00[0m
[[34m2023-09-11T08:16:29.045+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:29.045+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:29.045+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:29.047+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:29.048+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:29.048+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:29.051+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:30.864+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:30.990+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:31.156+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:31.187+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:31.637+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:31.638+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:31.705+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-20T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:31.760+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:32.450+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:32.462+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:31.842980+00:00, run_end_date=2023-09-11 08:16:32.039886+00:00, run_duration=0.196906, state=success, executor_state=success, try_number=1, max_tries=0, job_id=399, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:29.046518+00:00, queued_by_job_id=2, pid=50957[0m
[[34m2023-09-11T08:16:32.632+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-22T00:00:00+00:00, run_after=2023-05-23T00:00:00+00:00[0m
[[34m2023-09-11T08:16:32.668+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-20 00:00:00+00:00: scheduled__2023-05-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:28.994716+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:32.669+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-20 00:00:00+00:00, run_id=scheduled__2023-05-20T00:00:00+00:00, run_start_date=2023-09-11 08:16:29.013310+00:00, run_end_date=2023-09-11 08:16:32.669261+00:00, run_duration=3.655951, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-20 00:00:00+00:00, data_interval_end=2023-05-21 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:32.673+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-21T00:00:00+00:00, run_after=2023-05-22T00:00:00+00:00[0m
[[34m2023-09-11T08:16:32.688+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:32.688+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:32.688+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:32.690+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:32.691+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:32.691+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:32.694+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:34.529+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:34.665+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:34.829+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:34.860+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:35.321+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:35.322+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:35.402+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-21T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:35.464+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:36.197+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:36.207+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:35.548762+00:00, run_end_date=2023-09-11 08:16:35.743573+00:00, run_duration=0.194811, state=success, executor_state=success, try_number=1, max_tries=0, job_id=400, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:32.689412+00:00, queued_by_job_id=2, pid=50963[0m
[[34m2023-09-11T08:16:36.571+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-22T00:00:00+00:00, run_after=2023-05-23T00:00:00+00:00[0m
[[34m2023-09-11T08:16:36.593+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-21 00:00:00+00:00: scheduled__2023-05-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:32.627818+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:36.594+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-21 00:00:00+00:00, run_id=scheduled__2023-05-21T00:00:00+00:00, run_start_date=2023-09-11 08:16:32.645893+00:00, run_end_date=2023-09-11 08:16:36.594084+00:00, run_duration=3.948191, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-21 00:00:00+00:00, data_interval_end=2023-05-22 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:36.597+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-22T00:00:00+00:00, run_after=2023-05-23T00:00:00+00:00[0m
[[34m2023-09-11T08:16:37.690+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-23T00:00:00+00:00, run_after=2023-05-24T00:00:00+00:00[0m
[[34m2023-09-11T08:16:37.732+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:37.732+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:37.732+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:37.734+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:37.735+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:37.735+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:37.738+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:39.557+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:39.686+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:39.853+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:39.885+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:40.364+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:40.364+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:40.446+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-22T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:40.509+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:41.178+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:41.189+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:40.596226+00:00, run_end_date=2023-09-11 08:16:40.784104+00:00, run_duration=0.187878, state=success, executor_state=success, try_number=1, max_tries=0, job_id=401, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:37.733377+00:00, queued_by_job_id=2, pid=50972[0m
[[34m2023-09-11T08:16:41.471+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-24T00:00:00+00:00, run_after=2023-05-25T00:00:00+00:00[0m
[[34m2023-09-11T08:16:41.506+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-22 00:00:00+00:00: scheduled__2023-05-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:37.685652+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:41.506+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-22 00:00:00+00:00, run_id=scheduled__2023-05-22T00:00:00+00:00, run_start_date=2023-09-11 08:16:37.702036+00:00, run_end_date=2023-09-11 08:16:41.506721+00:00, run_duration=3.804685, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-22 00:00:00+00:00, data_interval_end=2023-05-23 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:41.510+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-23T00:00:00+00:00, run_after=2023-05-24T00:00:00+00:00[0m
[[34m2023-09-11T08:16:41.524+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:41.525+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:41.525+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:41.527+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:41.527+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:41.527+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:41.530+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:43.410+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:43.537+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:43.713+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:43.746+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:44.230+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:44.230+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:44.303+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-23T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:44.358+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:45.039+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:45.050+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:44.441207+00:00, run_end_date=2023-09-11 08:16:44.634245+00:00, run_duration=0.193038, state=success, executor_state=success, try_number=1, max_tries=0, job_id=402, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:41.525858+00:00, queued_by_job_id=2, pid=50978[0m
[[34m2023-09-11T08:16:45.290+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-24T00:00:00+00:00, run_after=2023-05-25T00:00:00+00:00[0m
[[34m2023-09-11T08:16:45.313+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-23 00:00:00+00:00: scheduled__2023-05-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:41.465616+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:45.314+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-23 00:00:00+00:00, run_id=scheduled__2023-05-23T00:00:00+00:00, run_start_date=2023-09-11 08:16:41.483735+00:00, run_end_date=2023-09-11 08:16:45.314123+00:00, run_duration=3.830388, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-23 00:00:00+00:00, data_interval_end=2023-05-24 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:45.318+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-24T00:00:00+00:00, run_after=2023-05-25T00:00:00+00:00[0m
[[34m2023-09-11T08:16:46.444+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-25T00:00:00+00:00, run_after=2023-05-26T00:00:00+00:00[0m
[[34m2023-09-11T08:16:46.490+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:46.490+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:46.490+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:46.492+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:46.493+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:46.493+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:46.496+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:48.394+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:48.525+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:48.694+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:48.726+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:49.209+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:49.210+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:49.281+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-24T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:49.340+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:50.046+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:50.056+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:49.425341+00:00, run_end_date=2023-09-11 08:16:49.627240+00:00, run_duration=0.201899, state=success, executor_state=success, try_number=1, max_tries=0, job_id=403, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:46.491505+00:00, queued_by_job_id=2, pid=50987[0m
[[34m2023-09-11T08:16:50.331+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-26T00:00:00+00:00, run_after=2023-05-27T00:00:00+00:00[0m
[[34m2023-09-11T08:16:50.365+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-24 00:00:00+00:00: scheduled__2023-05-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:46.440111+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:50.366+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-24 00:00:00+00:00, run_id=scheduled__2023-05-24T00:00:00+00:00, run_start_date=2023-09-11 08:16:46.458403+00:00, run_end_date=2023-09-11 08:16:50.366011+00:00, run_duration=3.907608, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-24 00:00:00+00:00, data_interval_end=2023-05-25 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:50.369+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-25T00:00:00+00:00, run_after=2023-05-26T00:00:00+00:00[0m
[[34m2023-09-11T08:16:50.384+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:50.385+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:50.385+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:50.387+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:50.388+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:50.388+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:50.391+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:52.193+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:52.322+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:52.511+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:52.560+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:53.106+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:53.107+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:53.179+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-25T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:53.239+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:54.014+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:54.026+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:53.333304+00:00, run_end_date=2023-09-11 08:16:53.603031+00:00, run_duration=0.269727, state=success, executor_state=success, try_number=1, max_tries=0, job_id=404, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:50.386030+00:00, queued_by_job_id=2, pid=50995[0m
[[34m2023-09-11T08:16:54.329+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-26T00:00:00+00:00, run_after=2023-05-27T00:00:00+00:00[0m
[[34m2023-09-11T08:16:54.350+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-25 00:00:00+00:00: scheduled__2023-05-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:50.327002+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:54.351+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-25 00:00:00+00:00, run_id=scheduled__2023-05-25T00:00:00+00:00, run_start_date=2023-09-11 08:16:50.344279+00:00, run_end_date=2023-09-11 08:16:54.351328+00:00, run_duration=4.007049, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-25 00:00:00+00:00, data_interval_end=2023-05-26 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:54.354+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-26T00:00:00+00:00, run_after=2023-05-27T00:00:00+00:00[0m
[[34m2023-09-11T08:16:55.322+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-27T00:00:00+00:00, run_after=2023-05-28T00:00:00+00:00[0m
[[34m2023-09-11T08:16:55.370+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:55.370+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:55.371+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:55.373+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:55.374+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:55.374+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:55.376+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:57.230+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:16:57.416+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:57.598+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:57.637+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:16:58.275+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:16:58.275+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:16:58.348+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-26T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:16:58.409+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:16:59.121+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:16:59.132+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:16:58.507284+00:00, run_end_date=2023-09-11 08:16:58.711255+00:00, run_duration=0.203971, state=success, executor_state=success, try_number=1, max_tries=0, job_id=405, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:55.371827+00:00, queued_by_job_id=2, pid=51004[0m
[[34m2023-09-11T08:16:59.394+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-28T00:00:00+00:00, run_after=2023-05-29T00:00:00+00:00[0m
[[34m2023-09-11T08:16:59.452+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-26 00:00:00+00:00: scheduled__2023-05-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:55.316538+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:16:59.452+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-26 00:00:00+00:00, run_id=scheduled__2023-05-26T00:00:00+00:00, run_start_date=2023-09-11 08:16:55.336532+00:00, run_end_date=2023-09-11 08:16:59.452728+00:00, run_duration=4.116196, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-26 00:00:00+00:00, data_interval_end=2023-05-27 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:16:59.456+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-27T00:00:00+00:00, run_after=2023-05-28T00:00:00+00:00[0m
[[34m2023-09-11T08:16:59.471+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:59.471+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:16:59.471+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:16:59.474+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:16:59.474+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:16:59.474+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:16:59.477+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:01.356+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:01.495+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:01.673+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:01.705+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:02.200+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:02.201+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:02.272+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-27T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:02.326+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:02.983+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:02.994+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:02.420434+00:00, run_end_date=2023-09-11 08:17:02.609451+00:00, run_duration=0.189017, state=success, executor_state=success, try_number=1, max_tries=0, job_id=406, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:16:59.472550+00:00, queued_by_job_id=2, pid=51015[0m
[[34m2023-09-11T08:17:03.260+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-28T00:00:00+00:00, run_after=2023-05-29T00:00:00+00:00[0m
[[34m2023-09-11T08:17:03.283+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-27 00:00:00+00:00: scheduled__2023-05-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:16:59.388064+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:03.284+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-27 00:00:00+00:00, run_id=scheduled__2023-05-27T00:00:00+00:00, run_start_date=2023-09-11 08:16:59.406747+00:00, run_end_date=2023-09-11 08:17:03.284275+00:00, run_duration=3.877528, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-27 00:00:00+00:00, data_interval_end=2023-05-28 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:03.287+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-28T00:00:00+00:00, run_after=2023-05-29T00:00:00+00:00[0m
[[34m2023-09-11T08:17:03.834+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-29T00:00:00+00:00, run_after=2023-05-30T00:00:00+00:00[0m
[[34m2023-09-11T08:17:03.877+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:03.878+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:03.878+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:03.880+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:03.881+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:03.882+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:03.884+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:05.741+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:05.872+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:06.056+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:06.095+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:06.612+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:06.612+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:06.680+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-28T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:06.748+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:07.576+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:07.590+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:06.835362+00:00, run_end_date=2023-09-11 08:17:07.040915+00:00, run_duration=0.205553, state=success, executor_state=success, try_number=1, max_tries=0, job_id=407, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:03.879256+00:00, queued_by_job_id=2, pid=51024[0m
[[34m2023-09-11T08:17:07.862+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-30T00:00:00+00:00, run_after=2023-05-31T00:00:00+00:00[0m
[[34m2023-09-11T08:17:07.897+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-28 00:00:00+00:00: scheduled__2023-05-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:03.829460+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:07.897+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-28 00:00:00+00:00, run_id=scheduled__2023-05-28T00:00:00+00:00, run_start_date=2023-09-11 08:17:03.847931+00:00, run_end_date=2023-09-11 08:17:07.897741+00:00, run_duration=4.04981, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-28 00:00:00+00:00, data_interval_end=2023-05-29 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:07.902+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-29T00:00:00+00:00, run_after=2023-05-30T00:00:00+00:00[0m
[[34m2023-09-11T08:17:07.916+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:07.917+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:07.917+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:07.919+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:07.920+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:07.921+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:07.923+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:09.872+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:09.997+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:10.174+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:10.205+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:10.669+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:10.670+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:10.740+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-29T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:10.796+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:11.461+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:11.472+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:10.881222+00:00, run_end_date=2023-09-11 08:17:11.075910+00:00, run_duration=0.194688, state=success, executor_state=success, try_number=1, max_tries=0, job_id=408, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:07.918201+00:00, queued_by_job_id=2, pid=51030[0m
[[34m2023-09-11T08:17:11.709+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-30T00:00:00+00:00, run_after=2023-05-31T00:00:00+00:00[0m
[[34m2023-09-11T08:17:11.732+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-29 00:00:00+00:00: scheduled__2023-05-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:07.856793+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:11.733+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-29 00:00:00+00:00, run_id=scheduled__2023-05-29T00:00:00+00:00, run_start_date=2023-09-11 08:17:07.874508+00:00, run_end_date=2023-09-11 08:17:11.733274+00:00, run_duration=3.858766, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-29 00:00:00+00:00, data_interval_end=2023-05-30 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:11.736+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-30T00:00:00+00:00, run_after=2023-05-31T00:00:00+00:00[0m
[[34m2023-09-11T08:17:12.866+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-31T00:00:00+00:00, run_after=2023-06-01T00:00:00+00:00[0m
[[34m2023-09-11T08:17:12.909+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:12.910+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:12.910+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:12.912+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:12.913+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:12.913+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:12.915+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:15.014+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:15.150+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:15.327+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:15.359+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:15.826+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:15.826+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:15.905+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-30T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:15.961+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:16.997+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:17.008+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:16.044959+00:00, run_end_date=2023-09-11 08:17:16.294936+00:00, run_duration=0.249977, state=success, executor_state=success, try_number=1, max_tries=0, job_id=409, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:12.910998+00:00, queued_by_job_id=2, pid=51039[0m
[[34m2023-09-11T08:17:17.273+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-01T00:00:00+00:00, run_after=2023-06-02T00:00:00+00:00[0m
[[34m2023-09-11T08:17:17.323+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-30 00:00:00+00:00: scheduled__2023-05-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:12.861221+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:17.323+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-30 00:00:00+00:00, run_id=scheduled__2023-05-30T00:00:00+00:00, run_start_date=2023-09-11 08:17:12.878800+00:00, run_end_date=2023-09-11 08:17:17.323811+00:00, run_duration=4.445011, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-30 00:00:00+00:00, data_interval_end=2023-05-31 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:17.327+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-05-31T00:00:00+00:00, run_after=2023-06-01T00:00:00+00:00[0m
[[34m2023-09-11T08:17:17.341+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-05-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:17.342+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:17.342+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-05-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:17.344+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:17.345+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:17.345+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:17.347+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:19.179+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:19.318+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:19.486+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:19.517+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:19.973+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:19.974+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:20.041+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-05-31T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:20.098+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-05-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:20.801+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-05-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:20.812+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-05-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:20.192343+00:00, run_end_date=2023-09-11 08:17:20.386507+00:00, run_duration=0.194164, state=success, executor_state=success, try_number=1, max_tries=0, job_id=410, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:17.343378+00:00, queued_by_job_id=2, pid=51047[0m
[[34m2023-09-11T08:17:21.067+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-01T00:00:00+00:00, run_after=2023-06-02T00:00:00+00:00[0m
[[34m2023-09-11T08:17:21.089+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-05-31 00:00:00+00:00: scheduled__2023-05-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:17.267670+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:21.089+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-05-31 00:00:00+00:00, run_id=scheduled__2023-05-31T00:00:00+00:00, run_start_date=2023-09-11 08:17:17.299720+00:00, run_end_date=2023-09-11 08:17:21.089533+00:00, run_duration=3.789813, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-31 00:00:00+00:00, data_interval_end=2023-06-01 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:21.093+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-01T00:00:00+00:00, run_after=2023-06-02T00:00:00+00:00[0m
[[34m2023-09-11T08:17:22.277+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-02T00:00:00+00:00, run_after=2023-06-03T00:00:00+00:00[0m
[[34m2023-09-11T08:17:22.326+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:22.326+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:22.327+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:22.329+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:22.329+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:22.329+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:22.332+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:24.193+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:24.321+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:24.497+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:24.530+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:24.980+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:24.981+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:25.051+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-01T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:25.108+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:25.808+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:25.819+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:25.195493+00:00, run_end_date=2023-09-11 08:17:25.391129+00:00, run_duration=0.195636, state=success, executor_state=success, try_number=1, max_tries=0, job_id=411, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:22.327794+00:00, queued_by_job_id=2, pid=51056[0m
[[34m2023-09-11T08:17:26.100+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-03T00:00:00+00:00, run_after=2023-06-04T00:00:00+00:00[0m
[[34m2023-09-11T08:17:26.134+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-01 00:00:00+00:00: scheduled__2023-06-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:22.272472+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:26.134+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-01 00:00:00+00:00, run_id=scheduled__2023-06-01T00:00:00+00:00, run_start_date=2023-09-11 08:17:22.288991+00:00, run_end_date=2023-09-11 08:17:26.134866+00:00, run_duration=3.845875, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-01 00:00:00+00:00, data_interval_end=2023-06-02 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:26.138+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-02T00:00:00+00:00, run_after=2023-06-03T00:00:00+00:00[0m
[[34m2023-09-11T08:17:26.152+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:26.153+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:26.153+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:26.155+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:26.156+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:26.156+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:26.158+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:27.995+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:28.124+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:28.302+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:28.334+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:28.795+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:28.795+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:28.864+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-02T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:28.929+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:29.642+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:29.653+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:29.019051+00:00, run_end_date=2023-09-11 08:17:29.214516+00:00, run_duration=0.195465, state=success, executor_state=success, try_number=1, max_tries=0, job_id=412, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:26.154114+00:00, queued_by_job_id=2, pid=51062[0m
[[34m2023-09-11T08:17:29.997+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-03T00:00:00+00:00, run_after=2023-06-04T00:00:00+00:00[0m
[[34m2023-09-11T08:17:30.018+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-02 00:00:00+00:00: scheduled__2023-06-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:26.095059+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:30.019+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-02 00:00:00+00:00, run_id=scheduled__2023-06-02T00:00:00+00:00, run_start_date=2023-09-11 08:17:26.112570+00:00, run_end_date=2023-09-11 08:17:30.019311+00:00, run_duration=3.906741, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-02 00:00:00+00:00, data_interval_end=2023-06-03 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:30.022+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-03T00:00:00+00:00, run_after=2023-06-04T00:00:00+00:00[0m
[[34m2023-09-11T08:17:31.081+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-04T00:00:00+00:00, run_after=2023-06-05T00:00:00+00:00[0m
[[34m2023-09-11T08:17:31.144+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:31.144+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:31.145+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:31.147+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:31.147+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:31.148+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:31.150+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:33.018+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:33.151+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:33.335+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:33.372+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:33.838+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:33.838+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:33.913+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-03T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:33.968+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:34.676+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:34.688+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:34.049645+00:00, run_end_date=2023-09-11 08:17:34.260857+00:00, run_duration=0.211212, state=success, executor_state=success, try_number=1, max_tries=0, job_id=413, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:31.145745+00:00, queued_by_job_id=2, pid=51071[0m
[[34m2023-09-11T08:17:34.954+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-05T00:00:00+00:00, run_after=2023-06-06T00:00:00+00:00[0m
[[34m2023-09-11T08:17:34.993+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-03 00:00:00+00:00: scheduled__2023-06-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:31.076339+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:34.993+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-03 00:00:00+00:00, run_id=scheduled__2023-06-03T00:00:00+00:00, run_start_date=2023-09-11 08:17:31.093054+00:00, run_end_date=2023-09-11 08:17:34.993743+00:00, run_duration=3.900689, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-03 00:00:00+00:00, data_interval_end=2023-06-04 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:34.998+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-04T00:00:00+00:00, run_after=2023-06-05T00:00:00+00:00[0m
[[34m2023-09-11T08:17:35.015+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:35.016+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:35.016+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:35.019+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:35.019+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:35.019+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:35.022+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:36.957+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:37.088+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:37.264+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:37.297+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:37.779+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:37.780+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:37.862+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-04T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:37.940+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:38.777+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:38.788+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:38.026946+00:00, run_end_date=2023-09-11 08:17:38.373117+00:00, run_duration=0.346171, state=success, executor_state=success, try_number=1, max_tries=0, job_id=414, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:35.017322+00:00, queued_by_job_id=2, pid=51079[0m
[[34m2023-09-11T08:17:39.028+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-05T00:00:00+00:00, run_after=2023-06-06T00:00:00+00:00[0m
[[34m2023-09-11T08:17:39.054+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-04 00:00:00+00:00: scheduled__2023-06-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:34.948709+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:39.054+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-04 00:00:00+00:00, run_id=scheduled__2023-06-04T00:00:00+00:00, run_start_date=2023-09-11 08:17:34.967319+00:00, run_end_date=2023-09-11 08:17:39.054618+00:00, run_duration=4.087299, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-04 00:00:00+00:00, data_interval_end=2023-06-05 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:39.058+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-05T00:00:00+00:00, run_after=2023-06-06T00:00:00+00:00[0m
[[34m2023-09-11T08:17:39.901+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-06T00:00:00+00:00, run_after=2023-06-07T00:00:00+00:00[0m
[[34m2023-09-11T08:17:39.952+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:39.952+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:39.953+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:39.955+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:39.956+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:39.956+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:39.959+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:42.022+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:42.151+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:42.324+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:42.354+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:42.818+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:42.818+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:42.888+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-05T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:42.946+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:43.669+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:43.680+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:43.033454+00:00, run_end_date=2023-09-11 08:17:43.228483+00:00, run_duration=0.195029, state=success, executor_state=success, try_number=1, max_tries=0, job_id=415, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:39.953895+00:00, queued_by_job_id=2, pid=51088[0m
[[34m2023-09-11T08:17:43.943+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-07T00:00:00+00:00, run_after=2023-06-08T00:00:00+00:00[0m
[[34m2023-09-11T08:17:43.977+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-05 00:00:00+00:00: scheduled__2023-06-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:39.895774+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:43.978+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-05 00:00:00+00:00, run_id=scheduled__2023-06-05T00:00:00+00:00, run_start_date=2023-09-11 08:17:39.912962+00:00, run_end_date=2023-09-11 08:17:43.978131+00:00, run_duration=4.065169, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-05 00:00:00+00:00, data_interval_end=2023-06-06 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:43.981+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-06T00:00:00+00:00, run_after=2023-06-07T00:00:00+00:00[0m
[[34m2023-09-11T08:17:43.995+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:43.996+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:43.996+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:43.998+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:43.998+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:43.999+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:44.001+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:45.835+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:45.963+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:46.130+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:46.161+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:46.613+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:46.613+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:46.690+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-06T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:46.749+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:47.475+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:47.486+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:46.835972+00:00, run_end_date=2023-09-11 08:17:47.049815+00:00, run_duration=0.213843, state=success, executor_state=success, try_number=1, max_tries=0, job_id=416, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:43.997099+00:00, queued_by_job_id=2, pid=51096[0m
[[34m2023-09-11T08:17:47.729+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-07T00:00:00+00:00, run_after=2023-06-08T00:00:00+00:00[0m
[[34m2023-09-11T08:17:47.752+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-06 00:00:00+00:00: scheduled__2023-06-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:43.938410+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:47.753+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-06 00:00:00+00:00, run_id=scheduled__2023-06-06T00:00:00+00:00, run_start_date=2023-09-11 08:17:43.955604+00:00, run_end_date=2023-09-11 08:17:47.752982+00:00, run_duration=3.797378, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-06 00:00:00+00:00, data_interval_end=2023-06-07 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:47.756+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-07T00:00:00+00:00, run_after=2023-06-08T00:00:00+00:00[0m
[[34m2023-09-11T08:17:48.966+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-08T00:00:00+00:00, run_after=2023-06-09T00:00:00+00:00[0m
[[34m2023-09-11T08:17:49.008+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:49.008+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:49.008+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:49.011+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:49.012+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:49.012+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:49.014+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:50.935+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:51.057+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:51.228+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:51.261+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:51.820+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:51.821+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:51.923+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-07T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:51.993+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:52.860+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:52.871+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:52.089213+00:00, run_end_date=2023-09-11 08:17:52.312311+00:00, run_duration=0.223098, state=success, executor_state=success, try_number=1, max_tries=0, job_id=417, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:49.009573+00:00, queued_by_job_id=2, pid=51105[0m
[[34m2023-09-11T08:17:53.343+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-09T00:00:00+00:00, run_after=2023-06-10T00:00:00+00:00[0m
[[34m2023-09-11T08:17:53.379+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-07 00:00:00+00:00: scheduled__2023-06-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:48.961891+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:53.379+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-07 00:00:00+00:00, run_id=scheduled__2023-06-07T00:00:00+00:00, run_start_date=2023-09-11 08:17:48.978834+00:00, run_end_date=2023-09-11 08:17:53.379661+00:00, run_duration=4.400827, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-07 00:00:00+00:00, data_interval_end=2023-06-08 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:53.383+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-08T00:00:00+00:00, run_after=2023-06-09T00:00:00+00:00[0m
[[34m2023-09-11T08:17:53.398+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:53.400+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:53.400+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:53.402+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:53.402+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:53.403+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:53.450+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:55.283+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:17:55.426+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:55.598+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:55.630+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:17:56.082+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:17:56.083+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:17:56.166+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-08T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:17:56.222+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:17:57.031+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:17:57.044+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:17:56.312179+00:00, run_end_date=2023-09-11 08:17:56.506095+00:00, run_duration=0.193916, state=success, executor_state=success, try_number=1, max_tries=0, job_id=418, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:53.401015+00:00, queued_by_job_id=2, pid=51113[0m
[[34m2023-09-11T08:17:57.400+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-09T00:00:00+00:00, run_after=2023-06-10T00:00:00+00:00[0m
[[34m2023-09-11T08:17:57.422+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-08 00:00:00+00:00: scheduled__2023-06-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:53.338490+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:17:57.423+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-08 00:00:00+00:00, run_id=scheduled__2023-06-08T00:00:00+00:00, run_start_date=2023-09-11 08:17:53.357481+00:00, run_end_date=2023-09-11 08:17:57.423070+00:00, run_duration=4.065589, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-08 00:00:00+00:00, data_interval_end=2023-06-09 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:17:57.426+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-09T00:00:00+00:00, run_after=2023-06-10T00:00:00+00:00[0m
[[34m2023-09-11T08:17:58.149+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-10T00:00:00+00:00, run_after=2023-06-11T00:00:00+00:00[0m
[[34m2023-09-11T08:17:58.196+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:58.197+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:17:58.197+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:17:58.199+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:17:58.200+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:17:58.200+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:17:58.202+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:00.417+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:00.651+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:00.850+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:00.886+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:01.415+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:01.416+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:01.494+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-09T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:01.558+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:02.327+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:02.339+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:01.652900+00:00, run_end_date=2023-09-11 08:18:01.872321+00:00, run_duration=0.219421, state=success, executor_state=success, try_number=1, max_tries=0, job_id=419, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:17:58.198177+00:00, queued_by_job_id=2, pid=51122[0m
[[34m2023-09-11T08:18:02.745+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-11T00:00:00+00:00, run_after=2023-06-12T00:00:00+00:00[0m
[[34m2023-09-11T08:18:02.793+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-09 00:00:00+00:00: scheduled__2023-06-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:17:58.143526+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:02.793+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-09 00:00:00+00:00, run_id=scheduled__2023-06-09T00:00:00+00:00, run_start_date=2023-09-11 08:17:58.162725+00:00, run_end_date=2023-09-11 08:18:02.793760+00:00, run_duration=4.631035, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-09 00:00:00+00:00, data_interval_end=2023-06-10 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:02.801+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-10T00:00:00+00:00, run_after=2023-06-11T00:00:00+00:00[0m
[[34m2023-09-11T08:18:02.824+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:02.825+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:02.825+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:02.828+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:02.829+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:02.830+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:02.834+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:04.923+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:05.068+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:05.247+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:05.286+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:05.750+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:05.751+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:05.826+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-10T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:05.885+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:06.617+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:06.632+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:05.972304+00:00, run_end_date=2023-09-11 08:18:06.169169+00:00, run_duration=0.196865, state=success, executor_state=success, try_number=1, max_tries=0, job_id=420, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:02.826466+00:00, queued_by_job_id=2, pid=51128[0m
[[34m2023-09-11T08:18:06.879+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-11T00:00:00+00:00, run_after=2023-06-12T00:00:00+00:00[0m
[[34m2023-09-11T08:18:06.903+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-10 00:00:00+00:00: scheduled__2023-06-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:02.740365+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:06.903+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-10 00:00:00+00:00, run_id=scheduled__2023-06-10T00:00:00+00:00, run_start_date=2023-09-11 08:18:02.758881+00:00, run_end_date=2023-09-11 08:18:06.903813+00:00, run_duration=4.144932, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-10 00:00:00+00:00, data_interval_end=2023-06-11 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:06.907+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-11T00:00:00+00:00, run_after=2023-06-12T00:00:00+00:00[0m
[[34m2023-09-11T08:18:07.342+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-12T00:00:00+00:00, run_after=2023-06-13T00:00:00+00:00[0m
[[34m2023-09-11T08:18:07.393+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:07.394+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:07.395+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:07.400+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:07.401+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:07.401+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:07.404+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:09.274+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:09.405+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:09.571+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:09.603+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:10.060+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:10.060+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:10.132+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-11T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:10.190+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:10.891+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:10.909+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:10.282583+00:00, run_end_date=2023-09-11 08:18:10.512573+00:00, run_duration=0.22999, state=success, executor_state=success, try_number=1, max_tries=0, job_id=421, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:07.398327+00:00, queued_by_job_id=2, pid=51135[0m
[[34m2023-09-11T08:18:11.228+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-13T00:00:00+00:00, run_after=2023-06-14T00:00:00+00:00[0m
[[34m2023-09-11T08:18:11.265+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-11 00:00:00+00:00: scheduled__2023-06-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:07.338048+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:11.266+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-11 00:00:00+00:00, run_id=scheduled__2023-06-11T00:00:00+00:00, run_start_date=2023-09-11 08:18:07.354633+00:00, run_end_date=2023-09-11 08:18:11.266148+00:00, run_duration=3.911515, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-11 00:00:00+00:00, data_interval_end=2023-06-12 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:11.269+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-12T00:00:00+00:00, run_after=2023-06-13T00:00:00+00:00[0m
[[34m2023-09-11T08:18:11.285+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:11.286+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:11.286+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:11.288+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:11.289+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:11.289+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:11.291+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:13.230+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:13.364+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:13.544+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:13.577+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:14.063+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:14.063+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:14.138+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-12T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:14.196+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:14.853+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:14.864+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:14.281222+00:00, run_end_date=2023-09-11 08:18:14.472437+00:00, run_duration=0.191215, state=success, executor_state=success, try_number=1, max_tries=0, job_id=422, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:11.287064+00:00, queued_by_job_id=2, pid=51143[0m
[[34m2023-09-11T08:18:15.107+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-13T00:00:00+00:00, run_after=2023-06-14T00:00:00+00:00[0m
[[34m2023-09-11T08:18:15.139+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-12 00:00:00+00:00: scheduled__2023-06-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:11.223219+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:15.140+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-12 00:00:00+00:00, run_id=scheduled__2023-06-12T00:00:00+00:00, run_start_date=2023-09-11 08:18:11.241272+00:00, run_end_date=2023-09-11 08:18:15.140036+00:00, run_duration=3.898764, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-12 00:00:00+00:00, data_interval_end=2023-06-13 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:15.143+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-13T00:00:00+00:00, run_after=2023-06-14T00:00:00+00:00[0m
[[34m2023-09-11T08:18:16.146+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-14T00:00:00+00:00, run_after=2023-06-15T00:00:00+00:00[0m
[[34m2023-09-11T08:18:16.190+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:16.191+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:16.191+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:16.194+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:16.195+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:16.195+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:16.198+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:18.141+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:18.312+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:18.491+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:18.524+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:18.996+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:18.997+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:19.071+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-13T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:19.134+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:19.849+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:19.859+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:19.262205+00:00, run_end_date=2023-09-11 08:18:19.460068+00:00, run_duration=0.197863, state=success, executor_state=success, try_number=1, max_tries=0, job_id=423, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:16.192465+00:00, queued_by_job_id=2, pid=51152[0m
[[34m2023-09-11T08:18:20.125+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-15T00:00:00+00:00, run_after=2023-06-16T00:00:00+00:00[0m
[[34m2023-09-11T08:18:20.160+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-13 00:00:00+00:00: scheduled__2023-06-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:16.141235+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:20.160+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-13 00:00:00+00:00, run_id=scheduled__2023-06-13T00:00:00+00:00, run_start_date=2023-09-11 08:18:16.158979+00:00, run_end_date=2023-09-11 08:18:20.160872+00:00, run_duration=4.001893, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-13 00:00:00+00:00, data_interval_end=2023-06-14 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:20.164+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-14T00:00:00+00:00, run_after=2023-06-15T00:00:00+00:00[0m
[[34m2023-09-11T08:18:20.178+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:20.179+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:20.179+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:20.181+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:20.182+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:20.182+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:20.184+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:22.033+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:22.164+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:22.336+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:22.369+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:22.841+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:22.841+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:22.912+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-14T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:22.967+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:23.686+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:23.697+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:23.055591+00:00, run_end_date=2023-09-11 08:18:23.259668+00:00, run_duration=0.204077, state=success, executor_state=success, try_number=1, max_tries=0, job_id=424, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:20.179991+00:00, queued_by_job_id=2, pid=51160[0m
[[34m2023-09-11T08:18:23.851+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-15T00:00:00+00:00, run_after=2023-06-16T00:00:00+00:00[0m
[[34m2023-09-11T08:18:23.873+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-14 00:00:00+00:00: scheduled__2023-06-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:20.120893+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:23.873+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-14 00:00:00+00:00, run_id=scheduled__2023-06-14T00:00:00+00:00, run_start_date=2023-09-11 08:18:20.138427+00:00, run_end_date=2023-09-11 08:18:23.873525+00:00, run_duration=3.735098, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-14 00:00:00+00:00, data_interval_end=2023-06-15 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:23.877+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-15T00:00:00+00:00, run_after=2023-06-16T00:00:00+00:00[0m
[[34m2023-09-11T08:18:25.332+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-16T00:00:00+00:00, run_after=2023-06-17T00:00:00+00:00[0m
[[34m2023-09-11T08:18:25.416+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:25.416+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:25.416+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:25.418+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:25.419+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:25.419+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:25.422+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:27.416+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:27.549+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:27.736+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:27.775+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:28.273+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:28.274+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:28.348+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-15T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:28.413+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:29.103+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:29.117+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:28.498138+00:00, run_end_date=2023-09-11 08:18:28.704212+00:00, run_duration=0.206074, state=success, executor_state=success, try_number=1, max_tries=0, job_id=425, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:25.417514+00:00, queued_by_job_id=2, pid=51169[0m
[[34m2023-09-11T08:18:29.401+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-17T00:00:00+00:00, run_after=2023-06-18T00:00:00+00:00[0m
[[34m2023-09-11T08:18:29.437+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-15 00:00:00+00:00: scheduled__2023-06-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:25.326194+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:29.437+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-15 00:00:00+00:00, run_id=scheduled__2023-06-15T00:00:00+00:00, run_start_date=2023-09-11 08:18:25.346390+00:00, run_end_date=2023-09-11 08:18:29.437910+00:00, run_duration=4.09152, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-15 00:00:00+00:00, data_interval_end=2023-06-16 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:29.441+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-16T00:00:00+00:00, run_after=2023-06-17T00:00:00+00:00[0m
[[34m2023-09-11T08:18:29.456+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:29.457+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:29.457+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:29.459+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:29.460+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:29.460+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:29.463+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:31.379+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:31.532+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:31.716+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:31.751+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:32.267+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:32.268+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:32.335+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-16T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:32.390+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:33.099+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:33.109+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:32.483658+00:00, run_end_date=2023-09-11 08:18:32.680329+00:00, run_duration=0.196671, state=success, executor_state=success, try_number=1, max_tries=0, job_id=426, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:29.458101+00:00, queued_by_job_id=2, pid=51177[0m
[[34m2023-09-11T08:18:33.245+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-17T00:00:00+00:00, run_after=2023-06-18T00:00:00+00:00[0m
[[34m2023-09-11T08:18:33.268+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-16 00:00:00+00:00: scheduled__2023-06-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:29.396397+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:33.268+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-16 00:00:00+00:00, run_id=scheduled__2023-06-16T00:00:00+00:00, run_start_date=2023-09-11 08:18:29.414358+00:00, run_end_date=2023-09-11 08:18:33.268688+00:00, run_duration=3.85433, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-16 00:00:00+00:00, data_interval_end=2023-06-17 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:33.272+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-17T00:00:00+00:00, run_after=2023-06-18T00:00:00+00:00[0m
[[34m2023-09-11T08:18:34.272+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-18T00:00:00+00:00, run_after=2023-06-19T00:00:00+00:00[0m
[[34m2023-09-11T08:18:34.323+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:34.323+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:34.323+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:34.326+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:34.326+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:34.326+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:34.329+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:36.440+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:36.585+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:36.774+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:36.813+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:37.306+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:37.307+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:37.387+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-17T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:37.453+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:38.198+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:38.209+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:37.570550+00:00, run_end_date=2023-09-11 08:18:37.787196+00:00, run_duration=0.216646, state=success, executor_state=success, try_number=1, max_tries=0, job_id=427, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:34.324721+00:00, queued_by_job_id=2, pid=51186[0m
[[34m2023-09-11T08:18:38.482+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-19T00:00:00+00:00, run_after=2023-06-20T00:00:00+00:00[0m
[[34m2023-09-11T08:18:38.518+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-17 00:00:00+00:00: scheduled__2023-06-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:34.265924+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:38.518+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-17 00:00:00+00:00, run_id=scheduled__2023-06-17T00:00:00+00:00, run_start_date=2023-09-11 08:18:34.291505+00:00, run_end_date=2023-09-11 08:18:38.518465+00:00, run_duration=4.22696, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-17 00:00:00+00:00, data_interval_end=2023-06-18 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:38.521+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-18T00:00:00+00:00, run_after=2023-06-19T00:00:00+00:00[0m
[[34m2023-09-11T08:18:38.537+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:38.538+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:38.538+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:38.540+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:38.541+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:38.541+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:38.543+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:40.481+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:40.626+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:40.919+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:40.957+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:41.492+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:41.493+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:41.571+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-18T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:41.648+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:42.420+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:42.432+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:41.777050+00:00, run_end_date=2023-09-11 08:18:41.981433+00:00, run_duration=0.204383, state=success, executor_state=success, try_number=1, max_tries=0, job_id=428, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:38.538938+00:00, queued_by_job_id=2, pid=51194[0m
[[34m2023-09-11T08:18:42.676+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-19T00:00:00+00:00, run_after=2023-06-20T00:00:00+00:00[0m
[[34m2023-09-11T08:18:42.746+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-18 00:00:00+00:00: scheduled__2023-06-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:38.477137+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:42.746+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-18 00:00:00+00:00, run_id=scheduled__2023-06-18T00:00:00+00:00, run_start_date=2023-09-11 08:18:38.495092+00:00, run_end_date=2023-09-11 08:18:42.746629+00:00, run_duration=4.251537, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-18 00:00:00+00:00, data_interval_end=2023-06-19 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:42.750+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-19T00:00:00+00:00, run_after=2023-06-20T00:00:00+00:00[0m
[[34m2023-09-11T08:18:43.580+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-20T00:00:00+00:00, run_after=2023-06-21T00:00:00+00:00[0m
[[34m2023-09-11T08:18:43.630+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:43.630+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:43.630+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:43.633+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:43.633+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:43.634+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:43.636+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:45.649+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:45.859+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:46.036+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:46.068+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:46.567+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:46.568+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:46.639+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-19T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:46.702+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:47.520+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:47.531+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:46.813004+00:00, run_end_date=2023-09-11 08:18:47.071330+00:00, run_duration=0.258326, state=success, executor_state=success, try_number=1, max_tries=0, job_id=429, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:43.631616+00:00, queued_by_job_id=2, pid=51203[0m
[[34m2023-09-11T08:18:47.797+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-21T00:00:00+00:00, run_after=2023-06-22T00:00:00+00:00[0m
[[34m2023-09-11T08:18:47.833+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-19 00:00:00+00:00: scheduled__2023-06-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:43.575547+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:47.833+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-19 00:00:00+00:00, run_id=scheduled__2023-06-19T00:00:00+00:00, run_start_date=2023-09-11 08:18:43.594713+00:00, run_end_date=2023-09-11 08:18:47.833915+00:00, run_duration=4.239202, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-19 00:00:00+00:00, data_interval_end=2023-06-20 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:47.837+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-20T00:00:00+00:00, run_after=2023-06-21T00:00:00+00:00[0m
[[34m2023-09-11T08:18:47.853+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:47.853+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:47.854+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:47.855+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:47.856+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:47.856+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:47.859+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:49.730+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:49.858+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:50.069+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:50.119+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:50.676+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:50.677+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:50.766+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-20T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:50.838+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:51.699+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:51.710+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:50.988113+00:00, run_end_date=2023-09-11 08:18:51.257262+00:00, run_duration=0.269149, state=success, executor_state=success, try_number=1, max_tries=0, job_id=430, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:47.854692+00:00, queued_by_job_id=2, pid=51209[0m
[[34m2023-09-11T08:18:51.970+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-21T00:00:00+00:00, run_after=2023-06-22T00:00:00+00:00[0m
[[34m2023-09-11T08:18:51.992+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-20 00:00:00+00:00: scheduled__2023-06-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:47.792220+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:51.993+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-20 00:00:00+00:00, run_id=scheduled__2023-06-20T00:00:00+00:00, run_start_date=2023-09-11 08:18:47.811226+00:00, run_end_date=2023-09-11 08:18:51.993036+00:00, run_duration=4.18181, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-20 00:00:00+00:00, data_interval_end=2023-06-21 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:51.997+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-21T00:00:00+00:00, run_after=2023-06-22T00:00:00+00:00[0m
[[34m2023-09-11T08:18:52.909+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-22T00:00:00+00:00, run_after=2023-06-23T00:00:00+00:00[0m
[[34m2023-09-11T08:18:52.971+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:52.972+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:52.972+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:52.975+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:52.976+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:52.976+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:52.980+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:55.230+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:55.392+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:55.576+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:55.609+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:56.054+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:18:56.055+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:56.126+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-21T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:18:56.185+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:18:56.919+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:18:56.931+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:18:56.272306+00:00, run_end_date=2023-09-11 08:18:56.463635+00:00, run_duration=0.191329, state=success, executor_state=success, try_number=1, max_tries=0, job_id=431, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:52.973584+00:00, queued_by_job_id=2, pid=51218[0m
[[34m2023-09-11T08:18:57.204+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-23T00:00:00+00:00, run_after=2023-06-24T00:00:00+00:00[0m
[[34m2023-09-11T08:18:57.241+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-21 00:00:00+00:00: scheduled__2023-06-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:52.901843+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:18:57.242+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-21 00:00:00+00:00, run_id=scheduled__2023-06-21T00:00:00+00:00, run_start_date=2023-09-11 08:18:52.929493+00:00, run_end_date=2023-09-11 08:18:57.242007+00:00, run_duration=4.312514, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-21 00:00:00+00:00, data_interval_end=2023-06-22 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:18:57.245+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-22T00:00:00+00:00, run_after=2023-06-23T00:00:00+00:00[0m
[[34m2023-09-11T08:18:57.263+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:57.263+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:18:57.263+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:18:57.265+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:18:57.266+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:18:57.266+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:57.269+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:18:59.313+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:18:59.541+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:18:59.753+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:18:59.788+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:00.336+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:00.336+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:00.422+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-22T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:00.487+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:01.232+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:01.242+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:00.573828+00:00, run_end_date=2023-09-11 08:19:00.766095+00:00, run_duration=0.192267, state=success, executor_state=success, try_number=1, max_tries=0, job_id=432, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:18:57.264423+00:00, queued_by_job_id=2, pid=51226[0m
[[34m2023-09-11T08:19:01.593+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-23T00:00:00+00:00, run_after=2023-06-24T00:00:00+00:00[0m
[[34m2023-09-11T08:19:01.625+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-22 00:00:00+00:00: scheduled__2023-06-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:18:57.197379+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:01.626+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-22 00:00:00+00:00, run_id=scheduled__2023-06-22T00:00:00+00:00, run_start_date=2023-09-11 08:18:57.218840+00:00, run_end_date=2023-09-11 08:19:01.626450+00:00, run_duration=4.40761, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-22 00:00:00+00:00, data_interval_end=2023-06-23 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:01.633+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-23T00:00:00+00:00, run_after=2023-06-24T00:00:00+00:00[0m
[[34m2023-09-11T08:19:01.971+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-24T00:00:00+00:00, run_after=2023-06-25T00:00:00+00:00[0m
[[34m2023-09-11T08:19:02.022+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:02.022+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:02.022+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:02.024+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:02.025+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:02.025+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:02.029+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:04.290+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:04.428+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:04.629+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:04.670+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:05.159+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:05.160+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:05.231+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-23T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:05.292+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:05.994+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:06.004+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:05.383223+00:00, run_end_date=2023-09-11 08:19:05.582429+00:00, run_duration=0.199206, state=success, executor_state=success, try_number=1, max_tries=0, job_id=433, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:02.023363+00:00, queued_by_job_id=2, pid=51233[0m
[[34m2023-09-11T08:19:06.262+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-25T00:00:00+00:00, run_after=2023-06-26T00:00:00+00:00[0m
[[34m2023-09-11T08:19:06.299+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-23 00:00:00+00:00: scheduled__2023-06-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:01.966551+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:06.300+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-23 00:00:00+00:00, run_id=scheduled__2023-06-23T00:00:00+00:00, run_start_date=2023-09-11 08:19:01.986612+00:00, run_end_date=2023-09-11 08:19:06.299942+00:00, run_duration=4.31333, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-23 00:00:00+00:00, data_interval_end=2023-06-24 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:06.303+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-24T00:00:00+00:00, run_after=2023-06-25T00:00:00+00:00[0m
[[34m2023-09-11T08:19:06.342+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:06.342+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:06.342+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:06.346+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:06.347+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:06.348+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:06.352+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:08.323+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:08.469+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:08.644+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:08.675+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:09.148+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:09.148+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:09.224+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-24T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:09.291+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:09.972+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:09.983+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:09.376804+00:00, run_end_date=2023-09-11 08:19:09.565351+00:00, run_duration=0.188547, state=success, executor_state=success, try_number=1, max_tries=0, job_id=434, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:06.344013+00:00, queued_by_job_id=2, pid=51241[0m
[[34m2023-09-11T08:19:10.226+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-25T00:00:00+00:00, run_after=2023-06-26T00:00:00+00:00[0m
[[34m2023-09-11T08:19:10.249+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-24 00:00:00+00:00: scheduled__2023-06-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:06.257532+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:10.249+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-24 00:00:00+00:00, run_id=scheduled__2023-06-24T00:00:00+00:00, run_start_date=2023-09-11 08:19:06.274956+00:00, run_end_date=2023-09-11 08:19:10.249679+00:00, run_duration=3.974723, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-24 00:00:00+00:00, data_interval_end=2023-06-25 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:10.253+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-25T00:00:00+00:00, run_after=2023-06-26T00:00:00+00:00[0m
[[34m2023-09-11T08:19:11.283+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-26T00:00:00+00:00, run_after=2023-06-27T00:00:00+00:00[0m
[[34m2023-09-11T08:19:11.337+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:11.338+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:11.338+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:11.340+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:11.341+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:11.341+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:11.348+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:13.327+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:13.468+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:13.670+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:13.703+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:14.186+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:14.187+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:14.264+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-25T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:14.323+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:15.057+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:15.068+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:14.413697+00:00, run_end_date=2023-09-11 08:19:14.609607+00:00, run_duration=0.19591, state=success, executor_state=success, try_number=1, max_tries=0, job_id=435, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:11.339181+00:00, queued_by_job_id=2, pid=51250[0m
[[34m2023-09-11T08:19:15.332+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-27T00:00:00+00:00, run_after=2023-06-28T00:00:00+00:00[0m
[[34m2023-09-11T08:19:15.369+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-25 00:00:00+00:00: scheduled__2023-06-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:11.276401+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:15.369+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-25 00:00:00+00:00, run_id=scheduled__2023-06-25T00:00:00+00:00, run_start_date=2023-09-11 08:19:11.299378+00:00, run_end_date=2023-09-11 08:19:15.369760+00:00, run_duration=4.070382, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-25 00:00:00+00:00, data_interval_end=2023-06-26 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:15.373+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-26T00:00:00+00:00, run_after=2023-06-27T00:00:00+00:00[0m
[[34m2023-09-11T08:19:15.388+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:15.388+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:15.388+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:15.391+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:15.391+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:15.391+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:15.394+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:17.271+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:17.417+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:17.592+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:17.623+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:18.080+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:18.081+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:18.167+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-26T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:18.222+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:18.940+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:18.951+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:18.312751+00:00, run_end_date=2023-09-11 08:19:18.518978+00:00, run_duration=0.206227, state=success, executor_state=success, try_number=1, max_tries=0, job_id=436, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:15.389616+00:00, queued_by_job_id=2, pid=51258[0m
[[34m2023-09-11T08:19:19.198+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-27T00:00:00+00:00, run_after=2023-06-28T00:00:00+00:00[0m
[[34m2023-09-11T08:19:19.222+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-26 00:00:00+00:00: scheduled__2023-06-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:15.326814+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:19.222+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-26 00:00:00+00:00, run_id=scheduled__2023-06-26T00:00:00+00:00, run_start_date=2023-09-11 08:19:15.346082+00:00, run_end_date=2023-09-11 08:19:19.222547+00:00, run_duration=3.876465, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-26 00:00:00+00:00, data_interval_end=2023-06-27 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:19.226+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-27T00:00:00+00:00, run_after=2023-06-28T00:00:00+00:00[0m
[[34m2023-09-11T08:19:20.353+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-28T00:00:00+00:00, run_after=2023-06-29T00:00:00+00:00[0m
[[34m2023-09-11T08:19:20.400+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:20.400+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:20.401+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:20.403+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:20.404+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:20.404+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:20.406+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:22.239+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:22.370+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:22.541+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:22.572+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:23.022+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:23.023+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:23.093+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-27T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:23.152+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:23.926+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:23.938+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:23.250035+00:00, run_end_date=2023-09-11 08:19:23.462487+00:00, run_duration=0.212452, state=success, executor_state=success, try_number=1, max_tries=0, job_id=437, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:20.402041+00:00, queued_by_job_id=2, pid=51267[0m
[[34m2023-09-11T08:19:24.231+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-29T00:00:00+00:00, run_after=2023-06-30T00:00:00+00:00[0m
[[34m2023-09-11T08:19:24.269+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-27 00:00:00+00:00: scheduled__2023-06-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:20.349198+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:24.269+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-27 00:00:00+00:00, run_id=scheduled__2023-06-27T00:00:00+00:00, run_start_date=2023-09-11 08:19:20.365834+00:00, run_end_date=2023-09-11 08:19:24.269890+00:00, run_duration=3.904056, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-27 00:00:00+00:00, data_interval_end=2023-06-28 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:24.273+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-28T00:00:00+00:00, run_after=2023-06-29T00:00:00+00:00[0m
[[34m2023-09-11T08:19:24.288+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:24.288+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:24.289+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:24.291+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:24.291+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:24.292+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:24.294+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:26.108+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:26.240+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:26.416+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:26.449+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:26.895+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:26.895+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:26.963+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-28T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:27.019+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:27.682+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:27.693+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:27.106605+00:00, run_end_date=2023-09-11 08:19:27.297965+00:00, run_duration=0.19136, state=success, executor_state=success, try_number=1, max_tries=0, job_id=438, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:24.289721+00:00, queued_by_job_id=2, pid=51275[0m
[[34m2023-09-11T08:19:28.268+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-29T00:00:00+00:00, run_after=2023-06-30T00:00:00+00:00[0m
[[34m2023-09-11T08:19:28.294+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-28 00:00:00+00:00: scheduled__2023-06-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:24.226320+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:28.294+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-28 00:00:00+00:00, run_id=scheduled__2023-06-28T00:00:00+00:00, run_start_date=2023-09-11 08:19:24.246425+00:00, run_end_date=2023-09-11 08:19:28.294694+00:00, run_duration=4.048269, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-28 00:00:00+00:00, data_interval_end=2023-06-29 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:28.298+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-29T00:00:00+00:00, run_after=2023-06-30T00:00:00+00:00[0m
[[34m2023-09-11T08:19:29.219+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-30T00:00:00+00:00, run_after=2023-07-01T00:00:00+00:00[0m
[[34m2023-09-11T08:19:29.269+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:29.270+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:29.270+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:29.273+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:29.274+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:29.274+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:29.278+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:31.149+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:31.285+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:31.470+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:31.504+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:31.959+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:31.960+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:32.029+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-29T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:32.086+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:32.823+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:32.834+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:32.176860+00:00, run_end_date=2023-09-11 08:19:32.390729+00:00, run_duration=0.213869, state=success, executor_state=success, try_number=1, max_tries=0, job_id=439, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:29.271240+00:00, queued_by_job_id=2, pid=51284[0m
[[34m2023-09-11T08:19:33.026+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-01T00:00:00+00:00, run_after=2023-07-02T00:00:00+00:00[0m
[[34m2023-09-11T08:19:33.062+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-29 00:00:00+00:00: scheduled__2023-06-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:29.213835+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:33.063+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-29 00:00:00+00:00, run_id=scheduled__2023-06-29T00:00:00+00:00, run_start_date=2023-09-11 08:19:29.233086+00:00, run_end_date=2023-09-11 08:19:33.063155+00:00, run_duration=3.830069, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-29 00:00:00+00:00, data_interval_end=2023-06-30 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:33.066+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-06-30T00:00:00+00:00, run_after=2023-07-01T00:00:00+00:00[0m
[[34m2023-09-11T08:19:33.082+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-06-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:33.082+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:33.082+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-06-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:33.084+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:33.085+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:33.085+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:33.087+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:35.102+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:35.238+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:35.410+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:35.442+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:35.946+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:35.946+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:36.022+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-06-30T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:36.081+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-06-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:36.787+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:36.798+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-06-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:36.169873+00:00, run_end_date=2023-09-11 08:19:36.367672+00:00, run_duration=0.197799, state=success, executor_state=success, try_number=1, max_tries=0, job_id=440, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:33.083216+00:00, queued_by_job_id=2, pid=51290[0m
[[34m2023-09-11T08:19:37.077+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-01T00:00:00+00:00, run_after=2023-07-02T00:00:00+00:00[0m
[[34m2023-09-11T08:19:37.100+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-06-30 00:00:00+00:00: scheduled__2023-06-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:33.017837+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:37.101+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-06-30 00:00:00+00:00, run_id=scheduled__2023-06-30T00:00:00+00:00, run_start_date=2023-09-11 08:19:33.038227+00:00, run_end_date=2023-09-11 08:19:37.101082+00:00, run_duration=4.062855, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-30 00:00:00+00:00, data_interval_end=2023-07-01 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:37.104+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-01T00:00:00+00:00, run_after=2023-07-02T00:00:00+00:00[0m
[[34m2023-09-11T08:19:38.118+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-02T00:00:00+00:00, run_after=2023-07-03T00:00:00+00:00[0m
[[34m2023-09-11T08:19:38.183+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:38.184+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:38.184+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:38.186+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:38.187+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:38.187+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:38.190+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:39.973+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:40.101+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:40.276+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:40.310+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:40.787+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:40.788+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:40.866+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-01T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:40.923+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:41.598+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:41.609+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:41.007190+00:00, run_end_date=2023-09-11 08:19:41.197246+00:00, run_duration=0.190056, state=success, executor_state=success, try_number=1, max_tries=0, job_id=441, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:38.185202+00:00, queued_by_job_id=2, pid=51299[0m
[[34m2023-09-11T08:19:41.883+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-03T00:00:00+00:00, run_after=2023-07-04T00:00:00+00:00[0m
[[34m2023-09-11T08:19:41.925+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-01 00:00:00+00:00: scheduled__2023-07-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:38.113661+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:41.925+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-01 00:00:00+00:00, run_id=scheduled__2023-07-01T00:00:00+00:00, run_start_date=2023-09-11 08:19:38.131765+00:00, run_end_date=2023-09-11 08:19:41.925758+00:00, run_duration=3.793993, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-01 00:00:00+00:00, data_interval_end=2023-07-02 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:41.929+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-02T00:00:00+00:00, run_after=2023-07-03T00:00:00+00:00[0m
[[34m2023-09-11T08:19:41.949+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:41.950+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:41.950+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:41.952+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:41.953+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:41.953+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:41.956+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:43.874+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:44.018+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:44.199+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:44.232+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:44.693+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:44.694+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:44.761+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-02T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:44.818+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:45.513+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:45.530+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:44.913852+00:00, run_end_date=2023-09-11 08:19:45.106519+00:00, run_duration=0.192667, state=success, executor_state=success, try_number=1, max_tries=0, job_id=442, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:41.951150+00:00, queued_by_job_id=2, pid=51305[0m
[[34m2023-09-11T08:19:45.782+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-03T00:00:00+00:00, run_after=2023-07-04T00:00:00+00:00[0m
[[34m2023-09-11T08:19:45.805+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-02 00:00:00+00:00: scheduled__2023-07-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:41.878248+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:45.806+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-02 00:00:00+00:00, run_id=scheduled__2023-07-02T00:00:00+00:00, run_start_date=2023-09-11 08:19:41.902348+00:00, run_end_date=2023-09-11 08:19:45.806278+00:00, run_duration=3.90393, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-02 00:00:00+00:00, data_interval_end=2023-07-03 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:45.810+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-03T00:00:00+00:00, run_after=2023-07-04T00:00:00+00:00[0m
[[34m2023-09-11T08:19:47.046+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-04T00:00:00+00:00, run_after=2023-07-05T00:00:00+00:00[0m
[[34m2023-09-11T08:19:47.091+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:47.091+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:47.091+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:47.094+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:47.095+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:47.095+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:47.097+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:48.988+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:49.135+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:49.307+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:49.339+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:49.845+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:49.846+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:49.935+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-03T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:49.994+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:50.689+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:50.700+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:50.079058+00:00, run_end_date=2023-09-11 08:19:50.271111+00:00, run_duration=0.192053, state=success, executor_state=success, try_number=1, max_tries=0, job_id=443, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:47.092878+00:00, queued_by_job_id=2, pid=51314[0m
[[34m2023-09-11T08:19:50.958+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-05T00:00:00+00:00, run_after=2023-07-06T00:00:00+00:00[0m
[[34m2023-09-11T08:19:50.994+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-03 00:00:00+00:00: scheduled__2023-07-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:47.041361+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:50.994+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-03 00:00:00+00:00, run_id=scheduled__2023-07-03T00:00:00+00:00, run_start_date=2023-09-11 08:19:47.058901+00:00, run_end_date=2023-09-11 08:19:50.994592+00:00, run_duration=3.935691, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-03 00:00:00+00:00, data_interval_end=2023-07-04 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:50.998+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-04T00:00:00+00:00, run_after=2023-07-05T00:00:00+00:00[0m
[[34m2023-09-11T08:19:51.012+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:51.013+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:51.013+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:51.015+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:51.015+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:51.016+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:51.018+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:53.097+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:53.246+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:53.429+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:53.469+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:54.106+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:54.106+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:54.219+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-04T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:54.306+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:55.142+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:55.159+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:54.413133+00:00, run_end_date=2023-09-11 08:19:54.653998+00:00, run_duration=0.240865, state=success, executor_state=success, try_number=1, max_tries=0, job_id=444, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:51.013874+00:00, queued_by_job_id=2, pid=51322[0m
[[34m2023-09-11T08:19:55.309+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-05T00:00:00+00:00, run_after=2023-07-06T00:00:00+00:00[0m
[[34m2023-09-11T08:19:55.331+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-04 00:00:00+00:00: scheduled__2023-07-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:50.953345+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:55.332+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-04 00:00:00+00:00, run_id=scheduled__2023-07-04T00:00:00+00:00, run_start_date=2023-09-11 08:19:50.971189+00:00, run_end_date=2023-09-11 08:19:55.332058+00:00, run_duration=4.360869, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-04 00:00:00+00:00, data_interval_end=2023-07-05 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:55.335+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-05T00:00:00+00:00, run_after=2023-07-06T00:00:00+00:00[0m
[[34m2023-09-11T08:19:55.788+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-06T00:00:00+00:00, run_after=2023-07-07T00:00:00+00:00[0m
[[34m2023-09-11T08:19:55.832+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:55.832+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:55.833+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:55.835+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:55.835+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:55.836+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:55.838+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:57.871+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:19:58.005+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:58.170+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:58.202+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:19:58.719+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:19:58.720+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:19:58.796+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-05T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:19:58.859+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:19:59.600+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:19:59.612+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:19:58.952001+00:00, run_end_date=2023-09-11 08:19:59.153538+00:00, run_duration=0.201537, state=success, executor_state=success, try_number=1, max_tries=0, job_id=445, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:55.833890+00:00, queued_by_job_id=2, pid=51331[0m
[[34m2023-09-11T08:19:59.905+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-07T00:00:00+00:00, run_after=2023-07-08T00:00:00+00:00[0m
[[34m2023-09-11T08:19:59.942+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-05 00:00:00+00:00: scheduled__2023-07-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:55.784383+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:19:59.942+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-05 00:00:00+00:00, run_id=scheduled__2023-07-05T00:00:00+00:00, run_start_date=2023-09-11 08:19:55.800640+00:00, run_end_date=2023-09-11 08:19:59.942641+00:00, run_duration=4.142001, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-05 00:00:00+00:00, data_interval_end=2023-07-06 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:19:59.946+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-06T00:00:00+00:00, run_after=2023-07-07T00:00:00+00:00[0m
[[34m2023-09-11T08:19:59.961+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:59.962+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:19:59.962+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:19:59.964+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:19:59.965+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:19:59.965+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:19:59.968+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:01.876+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:02.032+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:02.213+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:02.245+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:02.714+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:02.714+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:02.788+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-06T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:02.842+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:03.535+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:03.548+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:02.948249+00:00, run_end_date=2023-09-11 08:20:03.144501+00:00, run_duration=0.196252, state=success, executor_state=success, try_number=1, max_tries=0, job_id=446, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:19:59.963064+00:00, queued_by_job_id=2, pid=51339[0m
[[34m2023-09-11T08:20:03.791+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-07T00:00:00+00:00, run_after=2023-07-08T00:00:00+00:00[0m
[[34m2023-09-11T08:20:03.821+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-06 00:00:00+00:00: scheduled__2023-07-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:19:59.900495+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:03.822+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-06 00:00:00+00:00, run_id=scheduled__2023-07-06T00:00:00+00:00, run_start_date=2023-09-11 08:19:59.918719+00:00, run_end_date=2023-09-11 08:20:03.822393+00:00, run_duration=3.903674, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-06 00:00:00+00:00, data_interval_end=2023-07-07 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:03.827+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-07T00:00:00+00:00, run_after=2023-07-08T00:00:00+00:00[0m
[[34m2023-09-11T08:20:04.917+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-08T00:00:00+00:00, run_after=2023-07-09T00:00:00+00:00[0m
[[34m2023-09-11T08:20:04.960+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:04.960+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:04.961+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:04.963+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:04.964+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:04.964+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:04.967+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:06.873+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:07.010+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:07.179+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:07.213+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:07.707+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:07.708+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:07.778+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-07T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:07.835+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:08.530+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:08.546+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:07.926381+00:00, run_end_date=2023-09-11 08:20:08.115275+00:00, run_duration=0.188894, state=success, executor_state=success, try_number=1, max_tries=0, job_id=447, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:04.961769+00:00, queued_by_job_id=2, pid=51348[0m
[[34m2023-09-11T08:20:08.584+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:20:08.922+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-09T00:00:00+00:00, run_after=2023-07-10T00:00:00+00:00[0m
[[34m2023-09-11T08:20:08.958+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-07 00:00:00+00:00: scheduled__2023-07-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:04.912734+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:08.958+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-07 00:00:00+00:00, run_id=scheduled__2023-07-07T00:00:00+00:00, run_start_date=2023-09-11 08:20:04.929200+00:00, run_end_date=2023-09-11 08:20:08.958429+00:00, run_duration=4.029229, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-07 00:00:00+00:00, data_interval_end=2023-07-08 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:08.962+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-08T00:00:00+00:00, run_after=2023-07-09T00:00:00+00:00[0m
[[34m2023-09-11T08:20:08.977+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:08.978+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:08.978+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:08.980+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:08.981+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:08.981+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:08.984+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:10.998+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:11.139+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:11.313+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:11.351+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:11.856+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:11.856+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:11.924+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-08T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:11.979+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:12.715+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:12.727+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:12.077894+00:00, run_end_date=2023-09-11 08:20:12.297971+00:00, run_duration=0.220077, state=success, executor_state=success, try_number=1, max_tries=0, job_id=448, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:08.978943+00:00, queued_by_job_id=2, pid=51356[0m
[[34m2023-09-11T08:20:12.875+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-09T00:00:00+00:00, run_after=2023-07-10T00:00:00+00:00[0m
[[34m2023-09-11T08:20:12.897+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-08 00:00:00+00:00: scheduled__2023-07-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:08.917832+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:12.898+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-08 00:00:00+00:00, run_id=scheduled__2023-07-08T00:00:00+00:00, run_start_date=2023-09-11 08:20:08.935387+00:00, run_end_date=2023-09-11 08:20:12.898257+00:00, run_duration=3.96287, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-08 00:00:00+00:00, data_interval_end=2023-07-09 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:12.901+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-09T00:00:00+00:00, run_after=2023-07-10T00:00:00+00:00[0m
[[34m2023-09-11T08:20:13.714+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-10T00:00:00+00:00, run_after=2023-07-11T00:00:00+00:00[0m
[[34m2023-09-11T08:20:13.765+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:13.766+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:13.766+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:13.768+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:13.769+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:13.769+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:13.772+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:15.845+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:15.983+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:16.154+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:16.186+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:16.646+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:16.647+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:16.713+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-09T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:16.770+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:17.499+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:17.509+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:16.853258+00:00, run_end_date=2023-09-11 08:20:17.050045+00:00, run_duration=0.196787, state=success, executor_state=success, try_number=1, max_tries=0, job_id=449, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:13.767257+00:00, queued_by_job_id=2, pid=51363[0m
[[34m2023-09-11T08:20:17.774+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-11T00:00:00+00:00, run_after=2023-07-12T00:00:00+00:00[0m
[[34m2023-09-11T08:20:17.809+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-09 00:00:00+00:00: scheduled__2023-07-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:13.708085+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:17.810+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-09 00:00:00+00:00, run_id=scheduled__2023-07-09T00:00:00+00:00, run_start_date=2023-09-11 08:20:13.731608+00:00, run_end_date=2023-09-11 08:20:17.810323+00:00, run_duration=4.078715, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-09 00:00:00+00:00, data_interval_end=2023-07-10 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:17.814+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-10T00:00:00+00:00, run_after=2023-07-11T00:00:00+00:00[0m
[[34m2023-09-11T08:20:17.851+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:17.851+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:17.851+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:17.853+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:17.854+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:17.854+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:17.857+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:20.065+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:20.206+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:20.379+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:20.412+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:20.952+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:20.952+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:21.026+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-10T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:21.091+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:21.750+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:21.762+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:21.183007+00:00, run_end_date=2023-09-11 08:20:21.371849+00:00, run_duration=0.188842, state=success, executor_state=success, try_number=1, max_tries=0, job_id=450, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:17.852356+00:00, queued_by_job_id=2, pid=51372[0m
[[34m2023-09-11T08:20:21.913+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-11T00:00:00+00:00, run_after=2023-07-12T00:00:00+00:00[0m
[[34m2023-09-11T08:20:21.941+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-10 00:00:00+00:00: scheduled__2023-07-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:17.769139+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:21.941+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-10 00:00:00+00:00, run_id=scheduled__2023-07-10T00:00:00+00:00, run_start_date=2023-09-11 08:20:17.786156+00:00, run_end_date=2023-09-11 08:20:21.941605+00:00, run_duration=4.155449, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-10 00:00:00+00:00, data_interval_end=2023-07-11 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:21.946+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-11T00:00:00+00:00, run_after=2023-07-12T00:00:00+00:00[0m
[[34m2023-09-11T08:20:22.683+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-12T00:00:00+00:00, run_after=2023-07-13T00:00:00+00:00[0m
[[34m2023-09-11T08:20:22.727+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:22.727+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:22.727+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:22.730+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:22.730+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:22.731+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:22.733+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:24.733+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:24.886+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:25.070+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:25.103+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:25.565+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:25.565+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:25.636+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-11T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:25.695+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:26.452+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:26.464+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:25.793664+00:00, run_end_date=2023-09-11 08:20:26.022543+00:00, run_duration=0.228879, state=success, executor_state=success, try_number=1, max_tries=0, job_id=451, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:22.728610+00:00, queued_by_job_id=2, pid=51379[0m
[[34m2023-09-11T08:20:26.654+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-13T00:00:00+00:00, run_after=2023-07-14T00:00:00+00:00[0m
[[34m2023-09-11T08:20:26.700+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-11 00:00:00+00:00: scheduled__2023-07-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:22.679284+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:26.700+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-11 00:00:00+00:00, run_id=scheduled__2023-07-11T00:00:00+00:00, run_start_date=2023-09-11 08:20:22.695565+00:00, run_end_date=2023-09-11 08:20:26.700812+00:00, run_duration=4.005247, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-11 00:00:00+00:00, data_interval_end=2023-07-12 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:26.704+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-12T00:00:00+00:00, run_after=2023-07-13T00:00:00+00:00[0m
[[34m2023-09-11T08:20:26.720+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:26.721+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:26.721+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:26.723+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:26.724+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:26.725+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:26.727+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:28.553+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:28.682+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:28.846+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:28.879+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:29.336+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:29.337+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:29.419+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-12T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:29.483+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:30.151+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:30.161+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:29.570424+00:00, run_end_date=2023-09-11 08:20:29.762654+00:00, run_duration=0.19223, state=success, executor_state=success, try_number=1, max_tries=0, job_id=452, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:26.721999+00:00, queued_by_job_id=2, pid=51387[0m
[[34m2023-09-11T08:20:30.310+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-13T00:00:00+00:00, run_after=2023-07-14T00:00:00+00:00[0m
[[34m2023-09-11T08:20:30.333+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-12 00:00:00+00:00: scheduled__2023-07-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:26.647680+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:30.333+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-12 00:00:00+00:00, run_id=scheduled__2023-07-12T00:00:00+00:00, run_start_date=2023-09-11 08:20:26.674972+00:00, run_end_date=2023-09-11 08:20:30.333869+00:00, run_duration=3.658897, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-12 00:00:00+00:00, data_interval_end=2023-07-13 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:30.337+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-13T00:00:00+00:00, run_after=2023-07-14T00:00:00+00:00[0m
[[34m2023-09-11T08:20:31.637+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-14T00:00:00+00:00, run_after=2023-07-15T00:00:00+00:00[0m
[[34m2023-09-11T08:20:31.681+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:31.682+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:31.682+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:31.684+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:31.685+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:31.685+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:31.688+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:33.551+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:33.705+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:33.891+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:33.931+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:34.415+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:34.416+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:34.487+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-13T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:34.545+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:35.290+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:35.303+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:34.634155+00:00, run_end_date=2023-09-11 08:20:34.848740+00:00, run_duration=0.214585, state=success, executor_state=success, try_number=1, max_tries=0, job_id=453, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:31.683011+00:00, queued_by_job_id=2, pid=51396[0m
[[34m2023-09-11T08:20:35.568+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-15T00:00:00+00:00, run_after=2023-07-16T00:00:00+00:00[0m
[[34m2023-09-11T08:20:35.604+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-13 00:00:00+00:00: scheduled__2023-07-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:31.633064+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:35.604+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-13 00:00:00+00:00, run_id=scheduled__2023-07-13T00:00:00+00:00, run_start_date=2023-09-11 08:20:31.649955+00:00, run_end_date=2023-09-11 08:20:35.604724+00:00, run_duration=3.954769, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-13 00:00:00+00:00, data_interval_end=2023-07-14 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:35.608+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-14T00:00:00+00:00, run_after=2023-07-15T00:00:00+00:00[0m
[[34m2023-09-11T08:20:35.624+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:35.624+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:35.624+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:35.626+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:35.627+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:35.627+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:35.630+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:37.520+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:37.651+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:37.826+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:37.857+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:38.346+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:38.347+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:38.423+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-14T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:38.482+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:39.162+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:39.173+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:38.574799+00:00, run_end_date=2023-09-11 08:20:38.772270+00:00, run_duration=0.197471, state=success, executor_state=success, try_number=1, max_tries=0, job_id=454, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:35.625331+00:00, queued_by_job_id=2, pid=51404[0m
[[34m2023-09-11T08:20:39.469+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-15T00:00:00+00:00, run_after=2023-07-16T00:00:00+00:00[0m
[[34m2023-09-11T08:20:39.491+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-14 00:00:00+00:00: scheduled__2023-07-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:35.563004+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:39.491+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-14 00:00:00+00:00, run_id=scheduled__2023-07-14T00:00:00+00:00, run_start_date=2023-09-11 08:20:35.580888+00:00, run_end_date=2023-09-11 08:20:39.491416+00:00, run_duration=3.910528, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-14 00:00:00+00:00, data_interval_end=2023-07-15 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:39.494+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-15T00:00:00+00:00, run_after=2023-07-16T00:00:00+00:00[0m
[[34m2023-09-11T08:20:40.567+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-16T00:00:00+00:00, run_after=2023-07-17T00:00:00+00:00[0m
[[34m2023-09-11T08:20:40.614+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:40.615+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:40.615+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:40.617+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:40.618+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:40.618+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:40.644+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:42.560+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:42.690+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:42.867+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:42.900+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:43.366+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:43.367+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:43.450+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-15T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:43.508+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:44.219+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:44.232+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:43.608787+00:00, run_end_date=2023-09-11 08:20:43.811562+00:00, run_duration=0.202775, state=success, executor_state=success, try_number=1, max_tries=0, job_id=455, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:40.616173+00:00, queued_by_job_id=2, pid=51413[0m
[[34m2023-09-11T08:20:44.494+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-17T00:00:00+00:00, run_after=2023-07-18T00:00:00+00:00[0m
[[34m2023-09-11T08:20:44.546+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-15 00:00:00+00:00: scheduled__2023-07-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:40.563071+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:44.546+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-15 00:00:00+00:00, run_id=scheduled__2023-07-15T00:00:00+00:00, run_start_date=2023-09-11 08:20:40.580387+00:00, run_end_date=2023-09-11 08:20:44.546801+00:00, run_duration=3.966414, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-15 00:00:00+00:00, data_interval_end=2023-07-16 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:44.554+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-16T00:00:00+00:00, run_after=2023-07-17T00:00:00+00:00[0m
[[34m2023-09-11T08:20:44.583+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:44.584+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:44.585+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:44.588+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:44.589+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:44.590+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:44.594+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:46.587+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:46.731+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:46.915+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:46.951+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:47.424+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:47.424+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:47.493+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-16T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:47.552+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:48.317+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:48.328+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:47.641010+00:00, run_end_date=2023-09-11 08:20:47.875021+00:00, run_duration=0.234011, state=success, executor_state=success, try_number=1, max_tries=0, job_id=456, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:44.586118+00:00, queued_by_job_id=2, pid=51421[0m
[[34m2023-09-11T08:20:48.597+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-17T00:00:00+00:00, run_after=2023-07-18T00:00:00+00:00[0m
[[34m2023-09-11T08:20:48.620+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-16 00:00:00+00:00: scheduled__2023-07-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:44.489478+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:48.621+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-16 00:00:00+00:00, run_id=scheduled__2023-07-16T00:00:00+00:00, run_start_date=2023-09-11 08:20:44.507697+00:00, run_end_date=2023-09-11 08:20:48.620850+00:00, run_duration=4.113153, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-16 00:00:00+00:00, data_interval_end=2023-07-17 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:48.624+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-17T00:00:00+00:00, run_after=2023-07-18T00:00:00+00:00[0m
[[34m2023-09-11T08:20:49.431+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-18T00:00:00+00:00, run_after=2023-07-19T00:00:00+00:00[0m
[[34m2023-09-11T08:20:49.498+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:49.498+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:49.498+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:49.501+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:49.501+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:49.502+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:49.504+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:51.715+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:51.846+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:52.018+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:52.051+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:52.534+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:52.535+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:52.612+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-17T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:52.683+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:53.384+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:53.395+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:52.776811+00:00, run_end_date=2023-09-11 08:20:52.975182+00:00, run_duration=0.198371, state=success, executor_state=success, try_number=1, max_tries=0, job_id=457, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:49.499576+00:00, queued_by_job_id=2, pid=51430[0m
[[34m2023-09-11T08:20:53.666+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-19T00:00:00+00:00, run_after=2023-07-20T00:00:00+00:00[0m
[[34m2023-09-11T08:20:53.701+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-17 00:00:00+00:00: scheduled__2023-07-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:49.427121+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:53.701+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-17 00:00:00+00:00, run_id=scheduled__2023-07-17T00:00:00+00:00, run_start_date=2023-09-11 08:20:49.444433+00:00, run_end_date=2023-09-11 08:20:53.701893+00:00, run_duration=4.25746, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-17 00:00:00+00:00, data_interval_end=2023-07-18 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:53.705+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-18T00:00:00+00:00, run_after=2023-07-19T00:00:00+00:00[0m
[[34m2023-09-11T08:20:53.720+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:53.720+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:53.720+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:53.722+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:53.723+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:53.723+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:53.726+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:55.595+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:20:55.723+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:55.894+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:55.929+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:20:56.450+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:20:56.450+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:20:56.538+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-18T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:20:56.611+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:20:57.539+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:20:57.551+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:20:56.707535+00:00, run_end_date=2023-09-11 08:20:56.937292+00:00, run_duration=0.229757, state=success, executor_state=success, try_number=1, max_tries=0, job_id=458, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:53.721438+00:00, queued_by_job_id=2, pid=51436[0m
[[34m2023-09-11T08:20:57.789+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-19T00:00:00+00:00, run_after=2023-07-20T00:00:00+00:00[0m
[[34m2023-09-11T08:20:57.811+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-18 00:00:00+00:00: scheduled__2023-07-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:53.661634+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:20:57.812+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-18 00:00:00+00:00, run_id=scheduled__2023-07-18T00:00:00+00:00, run_start_date=2023-09-11 08:20:53.678756+00:00, run_end_date=2023-09-11 08:20:57.812043+00:00, run_duration=4.133287, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-18 00:00:00+00:00, data_interval_end=2023-07-19 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:20:57.815+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-19T00:00:00+00:00, run_after=2023-07-20T00:00:00+00:00[0m
[[34m2023-09-11T08:20:58.668+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-20T00:00:00+00:00, run_after=2023-07-21T00:00:00+00:00[0m
[[34m2023-09-11T08:20:58.718+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:58.718+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:20:58.718+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:20:58.720+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:20:58.721+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:20:58.722+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:20:58.725+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:01.426+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:01.580+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:01.813+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:01.879+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:02.458+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:02.459+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:02.542+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-19T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:02.608+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:03.346+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:03.357+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:02.715081+00:00, run_end_date=2023-09-11 08:21:02.957363+00:00, run_duration=0.242282, state=success, executor_state=success, try_number=1, max_tries=0, job_id=459, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:20:58.719295+00:00, queued_by_job_id=2, pid=51447[0m
[[34m2023-09-11T08:21:03.631+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-21T00:00:00+00:00, run_after=2023-07-22T00:00:00+00:00[0m
[[34m2023-09-11T08:21:03.666+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-19 00:00:00+00:00: scheduled__2023-07-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:20:58.663526+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:03.666+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-19 00:00:00+00:00, run_id=scheduled__2023-07-19T00:00:00+00:00, run_start_date=2023-09-11 08:20:58.681755+00:00, run_end_date=2023-09-11 08:21:03.666779+00:00, run_duration=4.985024, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-19 00:00:00+00:00, data_interval_end=2023-07-20 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:03.670+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-20T00:00:00+00:00, run_after=2023-07-21T00:00:00+00:00[0m
[[34m2023-09-11T08:21:03.688+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:03.689+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:03.689+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:03.691+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:03.692+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:03.692+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:03.695+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:05.603+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:05.732+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:05.910+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:05.944+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:06.532+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:06.533+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:06.616+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-20T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:06.686+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:07.829+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:07.845+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:06.773506+00:00, run_end_date=2023-09-11 08:21:07.280272+00:00, run_duration=0.506766, state=success, executor_state=success, try_number=1, max_tries=0, job_id=460, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:03.690288+00:00, queued_by_job_id=2, pid=51453[0m
[[34m2023-09-11T08:21:08.101+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-21T00:00:00+00:00, run_after=2023-07-22T00:00:00+00:00[0m
[[34m2023-09-11T08:21:08.146+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-20 00:00:00+00:00: scheduled__2023-07-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:03.626693+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:08.147+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-20 00:00:00+00:00, run_id=scheduled__2023-07-20T00:00:00+00:00, run_start_date=2023-09-11 08:21:03.644488+00:00, run_end_date=2023-09-11 08:21:08.147146+00:00, run_duration=4.502658, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-20 00:00:00+00:00, data_interval_end=2023-07-21 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:08.155+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-21T00:00:00+00:00, run_after=2023-07-22T00:00:00+00:00[0m
[[34m2023-09-11T08:21:08.766+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-22T00:00:00+00:00, run_after=2023-07-23T00:00:00+00:00[0m
[[34m2023-09-11T08:21:08.819+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:08.820+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:08.820+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:08.824+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:08.824+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:08.825+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:08.827+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:11.074+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:11.216+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:11.416+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:11.453+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:11.993+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:11.994+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:12.074+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-21T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:12.137+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:12.873+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:12.884+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:12.235921+00:00, run_end_date=2023-09-11 08:21:12.459882+00:00, run_duration=0.223961, state=success, executor_state=success, try_number=1, max_tries=0, job_id=461, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:08.822626+00:00, queued_by_job_id=2, pid=51462[0m
[[34m2023-09-11T08:21:13.146+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-23T00:00:00+00:00, run_after=2023-07-24T00:00:00+00:00[0m
[[34m2023-09-11T08:21:13.185+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-21 00:00:00+00:00: scheduled__2023-07-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:08.759915+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:13.186+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-21 00:00:00+00:00, run_id=scheduled__2023-07-21T00:00:00+00:00, run_start_date=2023-09-11 08:21:08.780440+00:00, run_end_date=2023-09-11 08:21:13.186289+00:00, run_duration=4.405849, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-21 00:00:00+00:00, data_interval_end=2023-07-22 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:13.190+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-22T00:00:00+00:00, run_after=2023-07-23T00:00:00+00:00[0m
[[34m2023-09-11T08:21:13.206+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:13.206+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:13.207+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:13.210+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:13.210+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:13.211+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:13.308+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:15.415+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:15.563+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:15.825+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:15.863+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:16.465+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:16.466+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:16.539+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-22T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:16.613+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:17.443+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:17.455+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:16.706813+00:00, run_end_date=2023-09-11 08:21:16.942333+00:00, run_duration=0.23552, state=success, executor_state=success, try_number=1, max_tries=0, job_id=462, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:13.208158+00:00, queued_by_job_id=2, pid=51470[0m
[[34m2023-09-11T08:21:17.729+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-23T00:00:00+00:00, run_after=2023-07-24T00:00:00+00:00[0m
[[34m2023-09-11T08:21:17.760+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-22 00:00:00+00:00: scheduled__2023-07-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:13.139753+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:17.761+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-22 00:00:00+00:00, run_id=scheduled__2023-07-22T00:00:00+00:00, run_start_date=2023-09-11 08:21:13.159486+00:00, run_end_date=2023-09-11 08:21:17.761280+00:00, run_duration=4.601794, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-22 00:00:00+00:00, data_interval_end=2023-07-23 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:17.766+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-23T00:00:00+00:00, run_after=2023-07-24T00:00:00+00:00[0m
[[34m2023-09-11T08:21:18.524+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-24T00:00:00+00:00, run_after=2023-07-25T00:00:00+00:00[0m
[[34m2023-09-11T08:21:18.604+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:18.605+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:18.605+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:18.610+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:18.610+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:18.611+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:18.614+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:20.675+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:20.831+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:21.028+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:21.062+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:21.610+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:21.611+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:21.695+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-23T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:21.756+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:22.525+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:22.542+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:21.849958+00:00, run_end_date=2023-09-11 08:21:22.065859+00:00, run_duration=0.215901, state=success, executor_state=success, try_number=1, max_tries=0, job_id=463, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:18.607117+00:00, queued_by_job_id=2, pid=51480[0m
[[34m2023-09-11T08:21:22.810+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-25T00:00:00+00:00, run_after=2023-07-26T00:00:00+00:00[0m
[[34m2023-09-11T08:21:22.879+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-23 00:00:00+00:00: scheduled__2023-07-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:18.519271+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:22.879+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-23 00:00:00+00:00, run_id=scheduled__2023-07-23T00:00:00+00:00, run_start_date=2023-09-11 08:21:18.563756+00:00, run_end_date=2023-09-11 08:21:22.879774+00:00, run_duration=4.316018, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-23 00:00:00+00:00, data_interval_end=2023-07-24 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:22.883+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-24T00:00:00+00:00, run_after=2023-07-25T00:00:00+00:00[0m
[[34m2023-09-11T08:21:22.899+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:22.900+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:22.900+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:22.903+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:22.904+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:22.904+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:22.908+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:25.083+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:25.233+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:25.431+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:25.466+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:25.998+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:25.999+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:26.084+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-24T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:26.147+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:26.909+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:26.921+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:26.241680+00:00, run_end_date=2023-09-11 08:21:26.453915+00:00, run_duration=0.212235, state=success, executor_state=success, try_number=1, max_tries=0, job_id=464, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:22.901558+00:00, queued_by_job_id=2, pid=51486[0m
[[34m2023-09-11T08:21:27.377+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-25T00:00:00+00:00, run_after=2023-07-26T00:00:00+00:00[0m
[[34m2023-09-11T08:21:27.406+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-24 00:00:00+00:00: scheduled__2023-07-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:22.802734+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:27.407+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-24 00:00:00+00:00, run_id=scheduled__2023-07-24T00:00:00+00:00, run_start_date=2023-09-11 08:21:22.827359+00:00, run_end_date=2023-09-11 08:21:27.407317+00:00, run_duration=4.579958, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-24 00:00:00+00:00, data_interval_end=2023-07-25 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:27.410+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-25T00:00:00+00:00, run_after=2023-07-26T00:00:00+00:00[0m
[[34m2023-09-11T08:21:27.827+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-26T00:00:00+00:00, run_after=2023-07-27T00:00:00+00:00[0m
[[34m2023-09-11T08:21:27.875+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:27.875+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:27.875+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:27.877+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:27.878+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:27.878+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:27.881+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:29.948+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:30.097+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:30.285+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:30.325+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:30.864+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:30.865+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:30.949+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-25T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:31.015+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:31.809+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:31.822+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:31.111322+00:00, run_end_date=2023-09-11 08:21:31.362466+00:00, run_duration=0.251144, state=success, executor_state=success, try_number=1, max_tries=0, job_id=465, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:27.876242+00:00, queued_by_job_id=2, pid=51495[0m
[[34m2023-09-11T08:21:32.153+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-27T00:00:00+00:00, run_after=2023-07-28T00:00:00+00:00[0m
[[34m2023-09-11T08:21:32.202+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-25 00:00:00+00:00: scheduled__2023-07-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:27.822908+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:32.203+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-25 00:00:00+00:00, run_id=scheduled__2023-07-25T00:00:00+00:00, run_start_date=2023-09-11 08:21:27.840750+00:00, run_end_date=2023-09-11 08:21:32.203494+00:00, run_duration=4.362744, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-25 00:00:00+00:00, data_interval_end=2023-07-26 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:32.210+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-26T00:00:00+00:00, run_after=2023-07-27T00:00:00+00:00[0m
[[34m2023-09-11T08:21:32.233+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:32.233+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:32.234+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:32.237+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:32.238+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:32.238+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:32.242+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:34.599+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:34.776+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:35.045+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:35.093+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:35.685+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:35.685+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:35.782+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-26T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:35.862+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:36.661+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:36.674+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:35.997690+00:00, run_end_date=2023-09-11 08:21:36.228429+00:00, run_duration=0.230739, state=success, executor_state=success, try_number=1, max_tries=0, job_id=466, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:32.235222+00:00, queued_by_job_id=2, pid=51503[0m
[[34m2023-09-11T08:21:36.939+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-27T00:00:00+00:00, run_after=2023-07-28T00:00:00+00:00[0m
[[34m2023-09-11T08:21:36.964+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-26 00:00:00+00:00: scheduled__2023-07-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:32.145682+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:36.965+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-26 00:00:00+00:00, run_id=scheduled__2023-07-26T00:00:00+00:00, run_start_date=2023-09-11 08:21:32.170510+00:00, run_end_date=2023-09-11 08:21:36.965261+00:00, run_duration=4.794751, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-26 00:00:00+00:00, data_interval_end=2023-07-27 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:36.968+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-27T00:00:00+00:00, run_after=2023-07-28T00:00:00+00:00[0m
[[34m2023-09-11T08:21:37.947+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-28T00:00:00+00:00, run_after=2023-07-29T00:00:00+00:00[0m
[[34m2023-09-11T08:21:37.999+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:38.000+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:38.001+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:38.006+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:38.007+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:38.007+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:38.010+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:40.408+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:40.567+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:40.791+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:40.826+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:41.365+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:41.366+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:41.460+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-27T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:41.527+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:42.294+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:42.307+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:41.626127+00:00, run_end_date=2023-09-11 08:21:41.857662+00:00, run_duration=0.231535, state=success, executor_state=success, try_number=1, max_tries=0, job_id=467, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:38.004811+00:00, queued_by_job_id=2, pid=51514[0m
[[34m2023-09-11T08:21:42.581+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-29T00:00:00+00:00, run_after=2023-07-30T00:00:00+00:00[0m
[[34m2023-09-11T08:21:42.624+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-27 00:00:00+00:00: scheduled__2023-07-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:37.941259+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:42.624+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-27 00:00:00+00:00, run_id=scheduled__2023-07-27T00:00:00+00:00, run_start_date=2023-09-11 08:21:37.964473+00:00, run_end_date=2023-09-11 08:21:42.624831+00:00, run_duration=4.660358, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-27 00:00:00+00:00, data_interval_end=2023-07-28 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:42.628+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-28T00:00:00+00:00, run_after=2023-07-29T00:00:00+00:00[0m
[[34m2023-09-11T08:21:42.645+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:42.645+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:42.646+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:42.648+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:42.649+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:42.649+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:42.651+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:44.831+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:45.005+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:45.210+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:45.245+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:45.781+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:45.782+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:45.858+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-28T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:45.924+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:46.750+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:46.765+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:46.020387+00:00, run_end_date=2023-09-11 08:21:46.263477+00:00, run_duration=0.24309, state=success, executor_state=success, try_number=1, max_tries=0, job_id=468, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:42.647161+00:00, queued_by_job_id=2, pid=51520[0m
[[34m2023-09-11T08:21:47.021+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-29T00:00:00+00:00, run_after=2023-07-30T00:00:00+00:00[0m
[[34m2023-09-11T08:21:47.058+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-28 00:00:00+00:00: scheduled__2023-07-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:42.576157+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:47.059+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-28 00:00:00+00:00, run_id=scheduled__2023-07-28T00:00:00+00:00, run_start_date=2023-09-11 08:21:42.596066+00:00, run_end_date=2023-09-11 08:21:47.059376+00:00, run_duration=4.46331, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-28 00:00:00+00:00, data_interval_end=2023-07-29 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:47.065+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-29T00:00:00+00:00, run_after=2023-07-30T00:00:00+00:00[0m
[[34m2023-09-11T08:21:47.586+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-30T00:00:00+00:00, run_after=2023-07-31T00:00:00+00:00[0m
[[34m2023-09-11T08:21:47.646+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:47.647+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:47.647+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:47.649+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:47.650+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:47.650+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:47.655+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:49.902+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:50.112+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:50.329+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:50.373+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:51.044+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:51.045+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:51.150+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-29T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:51.251+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:52.066+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:52.079+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:51.364927+00:00, run_end_date=2023-09-11 08:21:51.613641+00:00, run_duration=0.248714, state=success, executor_state=success, try_number=1, max_tries=0, job_id=469, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:47.647925+00:00, queued_by_job_id=2, pid=51531[0m
[[34m2023-09-11T08:21:52.443+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-31T00:00:00+00:00, run_after=2023-08-01T00:00:00+00:00[0m
[[34m2023-09-11T08:21:52.482+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-29 00:00:00+00:00: scheduled__2023-07-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:47.579555+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:52.482+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-29 00:00:00+00:00, run_id=scheduled__2023-07-29T00:00:00+00:00, run_start_date=2023-09-11 08:21:47.603934+00:00, run_end_date=2023-09-11 08:21:52.482762+00:00, run_duration=4.878828, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-29 00:00:00+00:00, data_interval_end=2023-07-30 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:52.487+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-30T00:00:00+00:00, run_after=2023-07-31T00:00:00+00:00[0m
[[34m2023-09-11T08:21:52.536+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:52.537+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:52.537+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:52.539+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:52.540+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:52.540+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:52.542+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:54.655+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:54.816+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:55.008+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:55.042+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:21:55.552+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:21:55.553+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:21:55.633+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-30T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:21:55.697+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:21:56.594+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:21:56.607+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:21:55.810628+00:00, run_end_date=2023-09-11 08:21:56.071852+00:00, run_duration=0.261224, state=success, executor_state=success, try_number=1, max_tries=0, job_id=470, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:52.537970+00:00, queued_by_job_id=2, pid=51537[0m
[[34m2023-09-11T08:21:56.917+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-31T00:00:00+00:00, run_after=2023-08-01T00:00:00+00:00[0m
[[34m2023-09-11T08:21:56.942+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-30 00:00:00+00:00: scheduled__2023-07-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:52.438210+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:21:56.943+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-30 00:00:00+00:00, run_id=scheduled__2023-07-30T00:00:00+00:00, run_start_date=2023-09-11 08:21:52.456064+00:00, run_end_date=2023-09-11 08:21:56.943297+00:00, run_duration=4.487233, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-30 00:00:00+00:00, data_interval_end=2023-07-31 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:21:56.947+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-07-31T00:00:00+00:00, run_after=2023-08-01T00:00:00+00:00[0m
[[34m2023-09-11T08:21:57.376+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-01T00:00:00+00:00, run_after=2023-08-02T00:00:00+00:00[0m
[[34m2023-09-11T08:21:57.440+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-07-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:57.441+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:21:57.441+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-07-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:21:57.444+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:21:57.445+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:21:57.445+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:57.451+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:21:59.758+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:21:59.921+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:00.125+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:00.163+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:00.793+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:00.793+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:00.880+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-07-31T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:00.950+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-07-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:01.738+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:01.749+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-07-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:01.060565+00:00, run_end_date=2023-09-11 08:22:01.281436+00:00, run_duration=0.220871, state=success, executor_state=success, try_number=1, max_tries=0, job_id=471, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:21:57.442731+00:00, queued_by_job_id=2, pid=51546[0m
[[34m2023-09-11T08:22:02.155+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-02T00:00:00+00:00, run_after=2023-08-03T00:00:00+00:00[0m
[[34m2023-09-11T08:22:02.194+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-07-31 00:00:00+00:00: scheduled__2023-07-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:21:57.370514+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:02.194+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-07-31 00:00:00+00:00, run_id=scheduled__2023-07-31T00:00:00+00:00, run_start_date=2023-09-11 08:21:57.398112+00:00, run_end_date=2023-09-11 08:22:02.194606+00:00, run_duration=4.796494, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-31 00:00:00+00:00, data_interval_end=2023-08-01 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:02.198+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-01T00:00:00+00:00, run_after=2023-08-02T00:00:00+00:00[0m
[[34m2023-09-11T08:22:02.227+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:02.227+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:02.227+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:02.230+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:02.231+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:02.231+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:02.233+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:04.430+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:04.588+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:04.804+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:04.844+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:05.398+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:05.399+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:05.490+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-01T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:05.558+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:06.384+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:06.399+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:05.700350+00:00, run_end_date=2023-09-11 08:22:05.931287+00:00, run_duration=0.230937, state=success, executor_state=success, try_number=1, max_tries=0, job_id=472, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:02.228791+00:00, queued_by_job_id=2, pid=51553[0m
[[34m2023-09-11T08:22:06.789+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-02T00:00:00+00:00, run_after=2023-08-03T00:00:00+00:00[0m
[[34m2023-09-11T08:22:06.816+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-01 00:00:00+00:00: scheduled__2023-08-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:02.148752+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:06.817+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-01 00:00:00+00:00, run_id=scheduled__2023-08-01T00:00:00+00:00, run_start_date=2023-09-11 08:22:02.169368+00:00, run_end_date=2023-09-11 08:22:06.817091+00:00, run_duration=4.647723, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-01 00:00:00+00:00, data_interval_end=2023-08-02 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:06.822+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-02T00:00:00+00:00, run_after=2023-08-03T00:00:00+00:00[0m
[[34m2023-09-11T08:22:07.173+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-03T00:00:00+00:00, run_after=2023-08-04T00:00:00+00:00[0m
[[34m2023-09-11T08:22:07.222+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:07.223+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:07.223+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:07.225+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:07.226+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:07.226+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:07.229+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:10.228+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:10.416+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:10.676+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:10.724+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:11.236+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:11.237+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:11.368+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-02T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:11.445+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:12.385+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:12.396+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:11.541350+00:00, run_end_date=2023-09-11 08:22:11.933268+00:00, run_duration=0.391918, state=success, executor_state=success, try_number=1, max_tries=0, job_id=473, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:07.224128+00:00, queued_by_job_id=2, pid=51564[0m
[[34m2023-09-11T08:22:12.665+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-04T00:00:00+00:00, run_after=2023-08-05T00:00:00+00:00[0m
[[34m2023-09-11T08:22:12.702+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-02 00:00:00+00:00: scheduled__2023-08-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:07.165866+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:12.702+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-02 00:00:00+00:00, run_id=scheduled__2023-08-02T00:00:00+00:00, run_start_date=2023-09-11 08:22:07.187385+00:00, run_end_date=2023-09-11 08:22:12.702548+00:00, run_duration=5.515163, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-02 00:00:00+00:00, data_interval_end=2023-08-03 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:12.705+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-03T00:00:00+00:00, run_after=2023-08-04T00:00:00+00:00[0m
[[34m2023-09-11T08:22:12.741+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:12.741+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:12.742+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:12.744+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:12.745+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:12.746+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:12.752+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:14.553+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:14.682+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:14.860+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:14.892+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:15.348+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:15.349+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:15.420+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-03T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:15.475+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:16.115+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:16.127+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:15.557384+00:00, run_end_date=2023-09-11 08:22:15.746769+00:00, run_duration=0.189385, state=success, executor_state=success, try_number=1, max_tries=0, job_id=474, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:12.742792+00:00, queued_by_job_id=2, pid=51570[0m
[[34m2023-09-11T08:22:16.369+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-04T00:00:00+00:00, run_after=2023-08-05T00:00:00+00:00[0m
[[34m2023-09-11T08:22:16.390+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-03 00:00:00+00:00: scheduled__2023-08-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:12.660925+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:16.391+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-03 00:00:00+00:00, run_id=scheduled__2023-08-03T00:00:00+00:00, run_start_date=2023-09-11 08:22:12.678460+00:00, run_end_date=2023-09-11 08:22:16.391193+00:00, run_duration=3.712733, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-03 00:00:00+00:00, data_interval_end=2023-08-04 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:16.394+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-04T00:00:00+00:00, run_after=2023-08-05T00:00:00+00:00[0m
[[34m2023-09-11T08:22:16.992+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-05T00:00:00+00:00, run_after=2023-08-06T00:00:00+00:00[0m
[[34m2023-09-11T08:22:17.035+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:17.035+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:17.036+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:17.038+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:17.038+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:17.039+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:17.041+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:18.871+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:19.010+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:19.182+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:19.214+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:19.687+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:19.688+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:19.762+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-04T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:19.820+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:20.527+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:20.538+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:19.909282+00:00, run_end_date=2023-09-11 08:22:20.100839+00:00, run_duration=0.191557, state=success, executor_state=success, try_number=1, max_tries=0, job_id=475, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:17.036943+00:00, queued_by_job_id=2, pid=51578[0m
[[34m2023-09-11T08:22:20.857+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-06T00:00:00+00:00, run_after=2023-08-07T00:00:00+00:00[0m
[[34m2023-09-11T08:22:20.893+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-04 00:00:00+00:00: scheduled__2023-08-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:16.988127+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:20.893+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-04 00:00:00+00:00, run_id=scheduled__2023-08-04T00:00:00+00:00, run_start_date=2023-09-11 08:22:17.004968+00:00, run_end_date=2023-09-11 08:22:20.893800+00:00, run_duration=3.888832, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-04 00:00:00+00:00, data_interval_end=2023-08-05 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:20.906+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-05T00:00:00+00:00, run_after=2023-08-06T00:00:00+00:00[0m
[[34m2023-09-11T08:22:20.924+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:20.924+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:20.924+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:20.926+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:20.927+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:20.927+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:20.930+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:22.970+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:23.106+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:23.278+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:23.310+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:23.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:23.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:23.842+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-05T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:23.909+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:24.622+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:24.633+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:23.996713+00:00, run_end_date=2023-09-11 08:22:24.193558+00:00, run_duration=0.196845, state=success, executor_state=success, try_number=1, max_tries=0, job_id=476, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:20.925501+00:00, queued_by_job_id=2, pid=51586[0m
[[34m2023-09-11T08:22:24.771+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-06T00:00:00+00:00, run_after=2023-08-07T00:00:00+00:00[0m
[[34m2023-09-11T08:22:24.793+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-05 00:00:00+00:00: scheduled__2023-08-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:20.852922+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:24.794+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-05 00:00:00+00:00, run_id=scheduled__2023-08-05T00:00:00+00:00, run_start_date=2023-09-11 08:22:20.870526+00:00, run_end_date=2023-09-11 08:22:24.794402+00:00, run_duration=3.923876, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-05 00:00:00+00:00, data_interval_end=2023-08-06 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:24.797+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-06T00:00:00+00:00, run_after=2023-08-07T00:00:00+00:00[0m
[[34m2023-09-11T08:22:25.770+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-07T00:00:00+00:00, run_after=2023-08-08T00:00:00+00:00[0m
[[34m2023-09-11T08:22:25.893+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:25.893+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:25.894+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:25.896+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:25.897+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:25.897+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:25.900+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:27.952+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:28.164+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:28.362+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:28.397+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:28.893+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:28.894+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:28.966+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-06T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:29.022+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:29.730+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:29.741+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:29.104439+00:00, run_end_date=2023-09-11 08:22:29.298208+00:00, run_duration=0.193769, state=success, executor_state=success, try_number=1, max_tries=0, job_id=477, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:25.894998+00:00, queued_by_job_id=2, pid=51595[0m
[[34m2023-09-11T08:22:29.917+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-08T00:00:00+00:00, run_after=2023-08-09T00:00:00+00:00[0m
[[34m2023-09-11T08:22:29.969+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-06 00:00:00+00:00: scheduled__2023-08-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:25.762552+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:29.970+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-06 00:00:00+00:00, run_id=scheduled__2023-08-06T00:00:00+00:00, run_start_date=2023-09-11 08:22:25.803749+00:00, run_end_date=2023-09-11 08:22:29.970039+00:00, run_duration=4.16629, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-06 00:00:00+00:00, data_interval_end=2023-08-07 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:29.973+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-07T00:00:00+00:00, run_after=2023-08-08T00:00:00+00:00[0m
[[34m2023-09-11T08:22:29.989+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:29.989+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:29.990+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:29.992+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:29.992+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:29.993+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:29.995+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:32.406+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:32.546+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:32.741+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:32.775+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:33.790+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:33.791+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:33.911+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-07T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:33.985+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:34.918+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:34.939+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:34.102943+00:00, run_end_date=2023-09-11 08:22:34.386200+00:00, run_duration=0.283257, state=success, executor_state=success, try_number=1, max_tries=0, job_id=478, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:29.990831+00:00, queued_by_job_id=2, pid=51603[0m
[[34m2023-09-11T08:22:35.147+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-08T00:00:00+00:00, run_after=2023-08-09T00:00:00+00:00[0m
[[34m2023-09-11T08:22:35.181+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-07 00:00:00+00:00: scheduled__2023-08-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:29.912189+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:35.182+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-07 00:00:00+00:00, run_id=scheduled__2023-08-07T00:00:00+00:00, run_start_date=2023-09-11 08:22:29.930614+00:00, run_end_date=2023-09-11 08:22:35.182440+00:00, run_duration=5.251826, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-07 00:00:00+00:00, data_interval_end=2023-08-08 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:35.187+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-08T00:00:00+00:00, run_after=2023-08-09T00:00:00+00:00[0m
[[34m2023-09-11T08:22:35.904+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-09T00:00:00+00:00, run_after=2023-08-10T00:00:00+00:00[0m
[[34m2023-09-11T08:22:35.967+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:35.968+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:35.968+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:35.972+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:35.973+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:35.973+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:35.976+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:39.365+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:39.499+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:39.686+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:39.735+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:40.379+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:40.385+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:40.489+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-08T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:40.554+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:41.497+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:41.517+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:40.661581+00:00, run_end_date=2023-09-11 08:22:40.957124+00:00, run_duration=0.295543, state=success, executor_state=success, try_number=1, max_tries=0, job_id=479, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:35.969981+00:00, queued_by_job_id=2, pid=51612[0m
[[34m2023-09-11T08:22:42.340+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-10T00:00:00+00:00, run_after=2023-08-11T00:00:00+00:00[0m
[[34m2023-09-11T08:22:42.380+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-08 00:00:00+00:00: scheduled__2023-08-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:35.897361+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:42.380+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-08 00:00:00+00:00, run_id=scheduled__2023-08-08T00:00:00+00:00, run_start_date=2023-09-11 08:22:35.921141+00:00, run_end_date=2023-09-11 08:22:42.380737+00:00, run_duration=6.459596, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-08 00:00:00+00:00, data_interval_end=2023-08-09 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:42.384+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-09T00:00:00+00:00, run_after=2023-08-10T00:00:00+00:00[0m
[[34m2023-09-11T08:22:42.412+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:42.412+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:42.413+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:42.416+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:42.417+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:42.417+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:42.423+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:45.409+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:45.553+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:45.775+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:45.809+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:46.352+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:46.353+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:46.427+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-09T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:46.484+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:47.182+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:47.193+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:46.567228+00:00, run_end_date=2023-09-11 08:22:46.763605+00:00, run_duration=0.196377, state=success, executor_state=success, try_number=1, max_tries=0, job_id=480, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:42.414066+00:00, queued_by_job_id=2, pid=51622[0m
[[34m2023-09-11T08:22:47.503+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-10T00:00:00+00:00, run_after=2023-08-11T00:00:00+00:00[0m
[[34m2023-09-11T08:22:47.579+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-09 00:00:00+00:00: scheduled__2023-08-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:42.333295+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:47.580+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-09 00:00:00+00:00, run_id=scheduled__2023-08-09T00:00:00+00:00, run_start_date=2023-09-11 08:22:42.354729+00:00, run_end_date=2023-09-11 08:22:47.580530+00:00, run_duration=5.225801, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-09 00:00:00+00:00, data_interval_end=2023-08-10 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:47.587+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-10T00:00:00+00:00, run_after=2023-08-11T00:00:00+00:00[0m
[[34m2023-09-11T08:22:48.854+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-11T00:00:00+00:00, run_after=2023-08-12T00:00:00+00:00[0m
[[34m2023-09-11T08:22:48.902+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:48.902+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:48.903+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:48.906+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:48.907+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:48.908+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:48.910+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:50.930+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:51.073+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:51.253+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:51.286+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:51.780+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:51.780+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:51.853+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-10T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:51.923+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:52.824+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:52.835+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:52.043306+00:00, run_end_date=2023-09-11 08:22:52.251863+00:00, run_duration=0.208557, state=success, executor_state=success, try_number=1, max_tries=0, job_id=481, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:48.903806+00:00, queued_by_job_id=2, pid=51631[0m
[[34m2023-09-11T08:22:53.303+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-12T00:00:00+00:00, run_after=2023-08-13T00:00:00+00:00[0m
[[34m2023-09-11T08:22:53.342+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-10 00:00:00+00:00: scheduled__2023-08-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:48.848902+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:53.343+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-10 00:00:00+00:00, run_id=scheduled__2023-08-10T00:00:00+00:00, run_start_date=2023-09-11 08:22:48.866208+00:00, run_end_date=2023-09-11 08:22:53.343351+00:00, run_duration=4.477143, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-10 00:00:00+00:00, data_interval_end=2023-08-11 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:53.346+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-11T00:00:00+00:00, run_after=2023-08-12T00:00:00+00:00[0m
[[34m2023-09-11T08:22:53.371+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:53.373+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:53.373+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:53.376+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:53.377+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:53.377+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:53.379+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:55.789+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:22:55.966+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:56.177+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:56.209+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:22:56.696+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:22:56.697+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:22:56.764+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-11T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:22:56.820+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:22:57.543+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:22:57.555+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:22:56.904241+00:00, run_end_date=2023-09-11 08:22:57.102560+00:00, run_duration=0.198319, state=success, executor_state=success, try_number=1, max_tries=0, job_id=482, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:53.374823+00:00, queued_by_job_id=2, pid=51639[0m
[[34m2023-09-11T08:22:57.804+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-12T00:00:00+00:00, run_after=2023-08-13T00:00:00+00:00[0m
[[34m2023-09-11T08:22:57.826+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-11 00:00:00+00:00: scheduled__2023-08-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:53.297557+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:22:57.827+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-11 00:00:00+00:00, run_id=scheduled__2023-08-11T00:00:00+00:00, run_start_date=2023-09-11 08:22:53.319169+00:00, run_end_date=2023-09-11 08:22:57.826958+00:00, run_duration=4.507789, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-11 00:00:00+00:00, data_interval_end=2023-08-12 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:22:57.830+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-12T00:00:00+00:00, run_after=2023-08-13T00:00:00+00:00[0m
[[34m2023-09-11T08:22:59.141+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-13T00:00:00+00:00, run_after=2023-08-14T00:00:00+00:00[0m
[[34m2023-09-11T08:22:59.188+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:59.189+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:22:59.189+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:22:59.191+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:22:59.192+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:22:59.192+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:22:59.195+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:01.970+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:02.200+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:02.539+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:02.595+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:03.367+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:03.368+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:03.522+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-12T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:03.643+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:04.766+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:04.783+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:03.814704+00:00, run_end_date=2023-09-11 08:23:04.144808+00:00, run_duration=0.330104, state=success, executor_state=success, try_number=1, max_tries=0, job_id=483, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:22:59.190311+00:00, queued_by_job_id=2, pid=51650[0m
[[34m2023-09-11T08:23:05.054+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-14T00:00:00+00:00, run_after=2023-08-15T00:00:00+00:00[0m
[[34m2023-09-11T08:23:05.101+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-12 00:00:00+00:00: scheduled__2023-08-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:22:59.135816+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:05.102+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-12 00:00:00+00:00, run_id=scheduled__2023-08-12T00:00:00+00:00, run_start_date=2023-09-11 08:22:59.154724+00:00, run_end_date=2023-09-11 08:23:05.102291+00:00, run_duration=5.947567, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-12 00:00:00+00:00, data_interval_end=2023-08-13 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:05.106+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-13T00:00:00+00:00, run_after=2023-08-14T00:00:00+00:00[0m
[[34m2023-09-11T08:23:05.131+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:05.132+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:05.132+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:05.136+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:05.137+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:05.138+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:05.141+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:07.913+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:08.062+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:08.314+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:08.361+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:08.969+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:08.970+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:09.041+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-13T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:09.116+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:09.950+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:09.962+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:09.208278+00:00, run_end_date=2023-09-11 08:23:09.460052+00:00, run_duration=0.251774, state=success, executor_state=success, try_number=1, max_tries=0, job_id=484, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:05.134120+00:00, queued_by_job_id=2, pid=51658[0m
[[34m2023-09-11T08:23:10.227+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-14T00:00:00+00:00, run_after=2023-08-15T00:00:00+00:00[0m
[[34m2023-09-11T08:23:10.253+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-13 00:00:00+00:00: scheduled__2023-08-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:05.047365+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:10.254+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-13 00:00:00+00:00, run_id=scheduled__2023-08-13T00:00:00+00:00, run_start_date=2023-09-11 08:23:05.068105+00:00, run_end_date=2023-09-11 08:23:10.254116+00:00, run_duration=5.186011, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-13 00:00:00+00:00, data_interval_end=2023-08-14 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:10.257+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-14T00:00:00+00:00, run_after=2023-08-15T00:00:00+00:00[0m
[[34m2023-09-11T08:23:11.546+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-15T00:00:00+00:00, run_after=2023-08-16T00:00:00+00:00[0m
[[34m2023-09-11T08:23:11.589+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:11.589+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:11.590+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:11.592+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:11.592+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:11.593+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:11.595+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:13.507+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:13.632+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:13.813+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:13.845+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:14.313+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:14.313+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:14.384+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-14T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:14.444+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:15.154+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:15.165+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:14.529462+00:00, run_end_date=2023-09-11 08:23:14.728730+00:00, run_duration=0.199268, state=success, executor_state=success, try_number=1, max_tries=0, job_id=485, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:11.590780+00:00, queued_by_job_id=2, pid=51666[0m
[[34m2023-09-11T08:23:15.433+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-16T00:00:00+00:00, run_after=2023-08-17T00:00:00+00:00[0m
[[34m2023-09-11T08:23:15.469+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-14 00:00:00+00:00: scheduled__2023-08-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:11.541814+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:15.470+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-14 00:00:00+00:00, run_id=scheduled__2023-08-14T00:00:00+00:00, run_start_date=2023-09-11 08:23:11.558511+00:00, run_end_date=2023-09-11 08:23:15.470041+00:00, run_duration=3.91153, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-14 00:00:00+00:00, data_interval_end=2023-08-15 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:15.473+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-15T00:00:00+00:00, run_after=2023-08-16T00:00:00+00:00[0m
[[34m2023-09-11T08:23:15.488+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:15.488+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:15.489+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:15.491+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:15.491+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:15.491+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:15.513+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:17.318+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:17.452+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:17.616+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:17.646+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:18.123+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:18.123+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:18.193+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-15T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:18.248+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:18.984+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:18.995+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:18.337741+00:00, run_end_date=2023-09-11 08:23:18.534575+00:00, run_duration=0.196834, state=success, executor_state=success, try_number=1, max_tries=0, job_id=486, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:15.489718+00:00, queued_by_job_id=2, pid=51674[0m
[[34m2023-09-11T08:23:19.241+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-16T00:00:00+00:00, run_after=2023-08-17T00:00:00+00:00[0m
[[34m2023-09-11T08:23:19.265+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-15 00:00:00+00:00: scheduled__2023-08-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:15.428706+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:19.265+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-15 00:00:00+00:00, run_id=scheduled__2023-08-15T00:00:00+00:00, run_start_date=2023-09-11 08:23:15.446479+00:00, run_end_date=2023-09-11 08:23:19.265714+00:00, run_duration=3.819235, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-15 00:00:00+00:00, data_interval_end=2023-08-16 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:19.269+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-16T00:00:00+00:00, run_after=2023-08-17T00:00:00+00:00[0m
[[34m2023-09-11T08:23:20.264+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-17T00:00:00+00:00, run_after=2023-08-18T00:00:00+00:00[0m
[[34m2023-09-11T08:23:20.307+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:20.308+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:20.308+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:20.310+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:20.311+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:20.311+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:20.314+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:22.122+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:22.246+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:22.418+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:22.450+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:22.908+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:22.909+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:22.979+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-16T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:23.037+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:23.720+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:23.730+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:23.120990+00:00, run_end_date=2023-09-11 08:23:23.315762+00:00, run_duration=0.194772, state=success, executor_state=success, try_number=1, max_tries=0, job_id=487, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:20.309000+00:00, queued_by_job_id=2, pid=51683[0m
[[34m2023-09-11T08:23:24.036+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-18T00:00:00+00:00, run_after=2023-08-19T00:00:00+00:00[0m
[[34m2023-09-11T08:23:24.072+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-16 00:00:00+00:00: scheduled__2023-08-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:20.259986+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:24.073+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-16 00:00:00+00:00, run_id=scheduled__2023-08-16T00:00:00+00:00, run_start_date=2023-09-11 08:23:20.277249+00:00, run_end_date=2023-09-11 08:23:24.072939+00:00, run_duration=3.79569, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-16 00:00:00+00:00, data_interval_end=2023-08-17 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:24.076+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-17T00:00:00+00:00, run_after=2023-08-18T00:00:00+00:00[0m
[[34m2023-09-11T08:23:24.092+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:24.092+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:24.092+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:24.094+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:24.095+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:24.095+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:24.098+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:25.912+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:26.038+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:26.210+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:26.241+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:26.705+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:26.705+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:26.776+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-17T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:26.834+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:27.492+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:27.503+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:26.927733+00:00, run_end_date=2023-09-11 08:23:27.118398+00:00, run_duration=0.190665, state=success, executor_state=success, try_number=1, max_tries=0, job_id=488, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:24.093386+00:00, queued_by_job_id=2, pid=51689[0m
[[34m2023-09-11T08:23:27.741+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-18T00:00:00+00:00, run_after=2023-08-19T00:00:00+00:00[0m
[[34m2023-09-11T08:23:27.765+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-17 00:00:00+00:00: scheduled__2023-08-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:24.031112+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:27.765+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-17 00:00:00+00:00, run_id=scheduled__2023-08-17T00:00:00+00:00, run_start_date=2023-09-11 08:23:24.049718+00:00, run_end_date=2023-09-11 08:23:27.765675+00:00, run_duration=3.715957, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-17 00:00:00+00:00, data_interval_end=2023-08-18 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:27.769+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-18T00:00:00+00:00, run_after=2023-08-19T00:00:00+00:00[0m
[[34m2023-09-11T08:23:29.035+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-19T00:00:00+00:00, run_after=2023-08-20T00:00:00+00:00[0m
[[34m2023-09-11T08:23:29.094+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:29.094+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:29.095+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:29.098+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:29.099+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:29.099+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:29.103+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:31.175+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:31.304+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:31.485+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:31.518+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:31.973+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:31.973+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:32.046+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-18T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:32.104+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:32.805+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:32.816+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:32.191822+00:00, run_end_date=2023-09-11 08:23:32.407287+00:00, run_duration=0.215465, state=success, executor_state=success, try_number=1, max_tries=0, job_id=489, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:29.095960+00:00, queued_by_job_id=2, pid=51698[0m
[[34m2023-09-11T08:23:33.081+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-20T00:00:00+00:00, run_after=2023-08-21T00:00:00+00:00[0m
[[34m2023-09-11T08:23:33.116+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-18 00:00:00+00:00: scheduled__2023-08-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:29.029126+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:33.116+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-18 00:00:00+00:00, run_id=scheduled__2023-08-18T00:00:00+00:00, run_start_date=2023-09-11 08:23:29.050627+00:00, run_end_date=2023-09-11 08:23:33.116895+00:00, run_duration=4.066268, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-18 00:00:00+00:00, data_interval_end=2023-08-19 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:33.120+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-19T00:00:00+00:00, run_after=2023-08-20T00:00:00+00:00[0m
[[34m2023-09-11T08:23:33.135+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:33.136+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:33.136+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:33.138+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:33.139+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:33.139+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:33.141+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:34.993+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:35.120+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:35.288+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:35.321+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:35.785+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:35.786+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:35.856+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-19T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:35.918+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:36.618+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:36.629+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:36.005250+00:00, run_end_date=2023-09-11 08:23:36.200680+00:00, run_duration=0.19543, state=success, executor_state=success, try_number=1, max_tries=0, job_id=490, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:33.137193+00:00, queued_by_job_id=2, pid=51704[0m
[[34m2023-09-11T08:23:36.985+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-20T00:00:00+00:00, run_after=2023-08-21T00:00:00+00:00[0m
[[34m2023-09-11T08:23:37.008+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-19 00:00:00+00:00: scheduled__2023-08-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:33.076376+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:37.008+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-19 00:00:00+00:00, run_id=scheduled__2023-08-19T00:00:00+00:00, run_start_date=2023-09-11 08:23:33.094457+00:00, run_end_date=2023-09-11 08:23:37.008795+00:00, run_duration=3.914338, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-19 00:00:00+00:00, data_interval_end=2023-08-20 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:37.012+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-20T00:00:00+00:00, run_after=2023-08-21T00:00:00+00:00[0m
[[34m2023-09-11T08:23:38.183+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-21T00:00:00+00:00, run_after=2023-08-22T00:00:00+00:00[0m
[[34m2023-09-11T08:23:38.228+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:38.229+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:38.229+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:38.231+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:38.232+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:38.232+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:38.235+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:40.057+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:40.188+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:40.355+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:40.385+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:40.842+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:40.842+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:40.915+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-20T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:40.973+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:42.217+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:42.227+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:41.060441+00:00, run_end_date=2023-09-11 08:23:41.800536+00:00, run_duration=0.740095, state=success, executor_state=success, try_number=1, max_tries=0, job_id=491, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:38.229836+00:00, queued_by_job_id=2, pid=51713[0m
[[34m2023-09-11T08:23:43.220+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-22T00:00:00+00:00, run_after=2023-08-23T00:00:00+00:00[0m
[[34m2023-09-11T08:23:43.255+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-20 00:00:00+00:00: scheduled__2023-08-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:38.177981+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:43.255+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-20 00:00:00+00:00, run_id=scheduled__2023-08-20T00:00:00+00:00, run_start_date=2023-09-11 08:23:38.196457+00:00, run_end_date=2023-09-11 08:23:43.255575+00:00, run_duration=5.059118, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-20 00:00:00+00:00, data_interval_end=2023-08-21 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:43.259+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-21T00:00:00+00:00, run_after=2023-08-22T00:00:00+00:00[0m
[[34m2023-09-11T08:23:43.273+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:43.274+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:43.274+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:43.276+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:43.277+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:43.277+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:43.280+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:45.147+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:45.278+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:45.445+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:45.476+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:45.943+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:45.943+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:46.017+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-21T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:46.073+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:46.721+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:46.731+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:46.157049+00:00, run_end_date=2023-09-11 08:23:46.348803+00:00, run_duration=0.191754, state=success, executor_state=success, try_number=1, max_tries=0, job_id=492, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:43.275045+00:00, queued_by_job_id=2, pid=51721[0m
[[34m2023-09-11T08:23:46.986+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-22T00:00:00+00:00, run_after=2023-08-23T00:00:00+00:00[0m
[[34m2023-09-11T08:23:47.009+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-21 00:00:00+00:00: scheduled__2023-08-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:43.215195+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:47.009+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-21 00:00:00+00:00, run_id=scheduled__2023-08-21T00:00:00+00:00, run_start_date=2023-09-11 08:23:43.232891+00:00, run_end_date=2023-09-11 08:23:47.009531+00:00, run_duration=3.77664, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-21 00:00:00+00:00, data_interval_end=2023-08-22 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:47.013+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-22T00:00:00+00:00, run_after=2023-08-23T00:00:00+00:00[0m
[[34m2023-09-11T08:23:47.486+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-23T00:00:00+00:00, run_after=2023-08-24T00:00:00+00:00[0m
[[34m2023-09-11T08:23:47.529+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:47.529+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:47.530+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:47.532+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:47.533+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:47.533+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:47.559+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:49.619+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:49.743+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:49.905+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:49.936+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:50.384+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:50.384+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:50.452+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-22T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:50.508+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:51.163+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:51.174+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:50.593393+00:00, run_end_date=2023-09-11 08:23:50.783885+00:00, run_duration=0.190492, state=success, executor_state=success, try_number=1, max_tries=0, job_id=493, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:47.530984+00:00, queued_by_job_id=2, pid=51730[0m
[[34m2023-09-11T08:23:51.566+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-24T00:00:00+00:00, run_after=2023-08-25T00:00:00+00:00[0m
[[34m2023-09-11T08:23:51.603+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-22 00:00:00+00:00: scheduled__2023-08-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:47.482037+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:51.603+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-22 00:00:00+00:00, run_id=scheduled__2023-08-22T00:00:00+00:00, run_start_date=2023-09-11 08:23:47.498362+00:00, run_end_date=2023-09-11 08:23:51.603385+00:00, run_duration=4.105023, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-22 00:00:00+00:00, data_interval_end=2023-08-23 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:51.607+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-23T00:00:00+00:00, run_after=2023-08-24T00:00:00+00:00[0m
[[34m2023-09-11T08:23:51.622+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:51.622+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:51.622+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:51.625+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:51.625+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:51.625+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:51.628+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:53.488+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:53.612+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:53.788+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:53.821+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:54.310+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:54.311+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:54.384+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-23T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:54.447+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:23:55.170+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:23:55.182+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:54.531442+00:00, run_end_date=2023-09-11 08:23:54.724127+00:00, run_duration=0.192685, state=success, executor_state=success, try_number=1, max_tries=0, job_id=494, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:51.623613+00:00, queued_by_job_id=2, pid=51738[0m
[[34m2023-09-11T08:23:55.427+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-24T00:00:00+00:00, run_after=2023-08-25T00:00:00+00:00[0m
[[34m2023-09-11T08:23:55.456+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-23 00:00:00+00:00: scheduled__2023-08-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:51.561945+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:23:55.456+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-23 00:00:00+00:00, run_id=scheduled__2023-08-23T00:00:00+00:00, run_start_date=2023-09-11 08:23:51.579716+00:00, run_end_date=2023-09-11 08:23:55.456512+00:00, run_duration=3.876796, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-23 00:00:00+00:00, data_interval_end=2023-08-24 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:23:55.460+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-24T00:00:00+00:00, run_after=2023-08-25T00:00:00+00:00[0m
[[34m2023-09-11T08:23:56.457+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-25T00:00:00+00:00, run_after=2023-08-26T00:00:00+00:00[0m
[[34m2023-09-11T08:23:56.502+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:56.503+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:23:56.503+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:23:56.505+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:23:56.506+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:23:56.506+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:56.509+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:23:58.496+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:23:58.632+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:58.812+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:58.849+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:23:59.319+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:23:59.320+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:23:59.389+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-24T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:23:59.448+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:00.158+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:00.169+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:23:59.536587+00:00, run_end_date=2023-09-11 08:23:59.737278+00:00, run_duration=0.200691, state=success, executor_state=success, try_number=1, max_tries=0, job_id=495, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:23:56.504027+00:00, queued_by_job_id=2, pid=51747[0m
[[34m2023-09-11T08:24:00.428+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-26T00:00:00+00:00, run_after=2023-08-27T00:00:00+00:00[0m
[[34m2023-09-11T08:24:00.464+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-24 00:00:00+00:00: scheduled__2023-08-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:23:56.452679+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:00.464+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-24 00:00:00+00:00, run_id=scheduled__2023-08-24T00:00:00+00:00, run_start_date=2023-09-11 08:23:56.469311+00:00, run_end_date=2023-09-11 08:24:00.464741+00:00, run_duration=3.99543, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-24 00:00:00+00:00, data_interval_end=2023-08-25 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:00.468+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-25T00:00:00+00:00, run_after=2023-08-26T00:00:00+00:00[0m
[[34m2023-09-11T08:24:00.483+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:00.483+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:00.483+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:00.486+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:00.486+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:00.486+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:00.489+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:02.315+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:02.445+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:02.611+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:02.643+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:03.090+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:03.090+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:03.165+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-25T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:03.224+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:04.003+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:04.020+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:03.310294+00:00, run_end_date=2023-09-11 08:24:03.519287+00:00, run_duration=0.208993, state=success, executor_state=success, try_number=1, max_tries=0, job_id=496, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:00.484472+00:00, queued_by_job_id=2, pid=51755[0m
[[34m2023-09-11T08:24:04.185+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-26T00:00:00+00:00, run_after=2023-08-27T00:00:00+00:00[0m
[[34m2023-09-11T08:24:04.207+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-25 00:00:00+00:00: scheduled__2023-08-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:00.423394+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:04.207+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-25 00:00:00+00:00, run_id=scheduled__2023-08-25T00:00:00+00:00, run_start_date=2023-09-11 08:24:00.441538+00:00, run_end_date=2023-09-11 08:24:04.207655+00:00, run_duration=3.766117, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-25 00:00:00+00:00, data_interval_end=2023-08-26 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:04.211+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-26T00:00:00+00:00, run_after=2023-08-27T00:00:00+00:00[0m
[[34m2023-09-11T08:24:05.465+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-27T00:00:00+00:00, run_after=2023-08-28T00:00:00+00:00[0m
[[34m2023-09-11T08:24:05.509+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:05.509+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:05.509+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:05.511+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:05.512+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:05.512+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:05.515+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:07.526+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:07.653+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:07.822+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:07.852+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:08.317+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:08.318+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:08.386+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-26T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:08.453+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:09.197+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:09.208+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:08.554638+00:00, run_end_date=2023-09-11 08:24:08.760472+00:00, run_duration=0.205834, state=success, executor_state=success, try_number=1, max_tries=0, job_id=497, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:05.510437+00:00, queued_by_job_id=2, pid=51764[0m
[[34m2023-09-11T08:24:09.533+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-28T00:00:00+00:00, run_after=2023-08-29T00:00:00+00:00[0m
[[34m2023-09-11T08:24:09.568+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-26 00:00:00+00:00: scheduled__2023-08-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:05.460897+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:09.568+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-26 00:00:00+00:00, run_id=scheduled__2023-08-26T00:00:00+00:00, run_start_date=2023-09-11 08:24:05.477947+00:00, run_end_date=2023-09-11 08:24:09.568852+00:00, run_duration=4.090905, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-26 00:00:00+00:00, data_interval_end=2023-08-27 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:09.572+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-27T00:00:00+00:00, run_after=2023-08-28T00:00:00+00:00[0m
[[34m2023-09-11T08:24:09.587+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:09.588+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:09.588+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:09.590+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:09.591+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:09.591+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:09.593+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:11.482+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:11.611+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:11.784+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:11.816+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:12.285+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:12.285+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:12.353+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-27T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:12.411+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:13.181+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:13.192+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:12.500607+00:00, run_end_date=2023-09-11 08:24:12.716269+00:00, run_duration=0.215662, state=success, executor_state=success, try_number=1, max_tries=0, job_id=498, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:09.589053+00:00, queued_by_job_id=2, pid=51770[0m
[[34m2023-09-11T08:24:13.494+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-28T00:00:00+00:00, run_after=2023-08-29T00:00:00+00:00[0m
[[34m2023-09-11T08:24:13.517+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-27 00:00:00+00:00: scheduled__2023-08-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:09.527882+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:13.518+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-27 00:00:00+00:00, run_id=scheduled__2023-08-27T00:00:00+00:00, run_start_date=2023-09-11 08:24:09.546421+00:00, run_end_date=2023-09-11 08:24:13.518170+00:00, run_duration=3.971749, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-27 00:00:00+00:00, data_interval_end=2023-08-28 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:13.521+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-28T00:00:00+00:00, run_after=2023-08-29T00:00:00+00:00[0m
[[34m2023-09-11T08:24:14.520+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-29T00:00:00+00:00, run_after=2023-08-30T00:00:00+00:00[0m
[[34m2023-09-11T08:24:14.570+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:14.570+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:14.571+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:14.573+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:14.573+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:14.574+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:14.577+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:16.511+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:16.637+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:16.804+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:16.836+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:17.320+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:17.321+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:17.388+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-28T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:17.454+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:18.169+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:18.180+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:17.539276+00:00, run_end_date=2023-09-11 08:24:17.735621+00:00, run_duration=0.196345, state=success, executor_state=success, try_number=1, max_tries=0, job_id=499, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:14.571783+00:00, queued_by_job_id=2, pid=51780[0m
[[34m2023-09-11T08:24:18.505+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-30T00:00:00+00:00, run_after=2023-08-31T00:00:00+00:00[0m
[[34m2023-09-11T08:24:18.559+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-28 00:00:00+00:00: scheduled__2023-08-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:14.515568+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:18.559+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-28 00:00:00+00:00, run_id=scheduled__2023-08-28T00:00:00+00:00, run_start_date=2023-09-11 08:24:14.533069+00:00, run_end_date=2023-09-11 08:24:18.559605+00:00, run_duration=4.026536, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-28 00:00:00+00:00, data_interval_end=2023-08-29 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:18.563+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-29T00:00:00+00:00, run_after=2023-08-30T00:00:00+00:00[0m
[[34m2023-09-11T08:24:18.578+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:18.578+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:18.578+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:18.581+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:18.581+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:18.581+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:18.584+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:20.436+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:20.575+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:20.749+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:20.789+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:21.279+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:21.279+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:21.347+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-29T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:21.407+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:22.045+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:22.056+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:21.488759+00:00, run_end_date=2023-09-11 08:24:21.679117+00:00, run_duration=0.190358, state=success, executor_state=success, try_number=1, max_tries=0, job_id=500, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:18.579497+00:00, queued_by_job_id=2, pid=51786[0m
[[34m2023-09-11T08:24:22.833+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-30T00:00:00+00:00, run_after=2023-08-31T00:00:00+00:00[0m
[[34m2023-09-11T08:24:22.856+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-29 00:00:00+00:00: scheduled__2023-08-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:18.500565+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:22.856+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-29 00:00:00+00:00, run_id=scheduled__2023-08-29T00:00:00+00:00, run_start_date=2023-09-11 08:24:18.518412+00:00, run_end_date=2023-09-11 08:24:22.856871+00:00, run_duration=4.338459, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-29 00:00:00+00:00, data_interval_end=2023-08-30 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:22.860+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-30T00:00:00+00:00, run_after=2023-08-31T00:00:00+00:00[0m
[[34m2023-09-11T08:24:23.392+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-31T00:00:00+00:00, run_after=2023-09-01T00:00:00+00:00[0m
[[34m2023-09-11T08:24:23.436+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:23.436+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:23.436+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:23.438+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:23.439+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:23.439+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:23.442+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:25.264+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:25.389+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:25.554+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:25.584+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:26.039+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:26.039+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:26.110+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-30T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:26.173+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:26.880+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:26.890+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:26.257015+00:00, run_end_date=2023-09-11 08:24:26.493797+00:00, run_duration=0.236782, state=success, executor_state=success, try_number=1, max_tries=0, job_id=501, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:23.437504+00:00, queued_by_job_id=2, pid=51795[0m
[[34m2023-09-11T08:24:27.293+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-01T00:00:00+00:00, run_after=2023-09-02T00:00:00+00:00[0m
[[34m2023-09-11T08:24:27.344+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-30 00:00:00+00:00: scheduled__2023-08-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:23.388019+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:27.345+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-30 00:00:00+00:00, run_id=scheduled__2023-08-30T00:00:00+00:00, run_start_date=2023-09-11 08:24:23.404613+00:00, run_end_date=2023-09-11 08:24:27.345032+00:00, run_duration=3.940419, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-30 00:00:00+00:00, data_interval_end=2023-08-31 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:27.348+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-08-31T00:00:00+00:00, run_after=2023-09-01T00:00:00+00:00[0m
[[34m2023-09-11T08:24:27.368+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-08-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:27.368+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:27.368+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-08-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:27.370+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:27.371+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:27.371+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:27.374+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:29.223+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:29.348+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:29.519+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:29.551+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:29.998+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:29.999+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:30.065+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-08-31T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:30.121+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-08-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:31.169+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:31.180+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-08-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:30.206002+00:00, run_end_date=2023-09-11 08:24:30.401507+00:00, run_duration=0.195505, state=success, executor_state=success, try_number=1, max_tries=0, job_id=502, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:27.369420+00:00, queued_by_job_id=2, pid=51803[0m
[[34m2023-09-11T08:24:31.355+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-01T00:00:00+00:00, run_after=2023-09-02T00:00:00+00:00[0m
[[34m2023-09-11T08:24:31.378+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-08-31 00:00:00+00:00: scheduled__2023-08-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:27.286791+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:31.378+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-08-31 00:00:00+00:00, run_id=scheduled__2023-08-31T00:00:00+00:00, run_start_date=2023-09-11 08:24:27.321737+00:00, run_end_date=2023-09-11 08:24:31.378490+00:00, run_duration=4.056753, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-31 00:00:00+00:00, data_interval_end=2023-09-01 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:31.382+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-01T00:00:00+00:00, run_after=2023-09-02T00:00:00+00:00[0m
[[34m2023-09-11T08:24:32.292+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00[0m
[[34m2023-09-11T08:24:32.344+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:32.344+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:32.344+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:32.346+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:32.347+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:32.347+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:32.350+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:34.204+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:34.340+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:34.504+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:34.535+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:34.998+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:34.999+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:35.070+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-01T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:35.127+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:35.827+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:35.838+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:35.209100+00:00, run_end_date=2023-09-11 08:24:35.439096+00:00, run_duration=0.229996, state=success, executor_state=success, try_number=1, max_tries=0, job_id=503, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:32.345527+00:00, queued_by_job_id=2, pid=51812[0m
[[34m2023-09-11T08:24:36.098+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-03T00:00:00+00:00, run_after=2023-09-04T00:00:00+00:00[0m
[[34m2023-09-11T08:24:36.135+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-01 00:00:00+00:00: scheduled__2023-09-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:32.287848+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:36.135+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-01 00:00:00+00:00, run_id=scheduled__2023-09-01T00:00:00+00:00, run_start_date=2023-09-11 08:24:32.308325+00:00, run_end_date=2023-09-11 08:24:36.135554+00:00, run_duration=3.827229, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-01 00:00:00+00:00, data_interval_end=2023-09-02 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:36.139+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00[0m
[[34m2023-09-11T08:24:36.154+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:36.154+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:36.154+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:36.156+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:36.157+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:36.157+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:36.160+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:38.080+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:38.213+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:38.389+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:38.420+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:38.887+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:38.888+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:38.981+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-02T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:39.049+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:39.764+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:39.775+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:39.184643+00:00, run_end_date=2023-09-11 08:24:39.370991+00:00, run_duration=0.186348, state=success, executor_state=success, try_number=1, max_tries=0, job_id=504, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:36.155413+00:00, queued_by_job_id=2, pid=51820[0m
[[34m2023-09-11T08:24:39.980+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-03T00:00:00+00:00, run_after=2023-09-04T00:00:00+00:00[0m
[[34m2023-09-11T08:24:40.002+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-02 00:00:00+00:00: scheduled__2023-09-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:36.093822+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:40.003+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-02 00:00:00+00:00, run_id=scheduled__2023-09-02T00:00:00+00:00, run_start_date=2023-09-11 08:24:36.111626+00:00, run_end_date=2023-09-11 08:24:40.003015+00:00, run_duration=3.891389, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-02 00:00:00+00:00, data_interval_end=2023-09-03 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:40.006+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-03T00:00:00+00:00, run_after=2023-09-04T00:00:00+00:00[0m
[[34m2023-09-11T08:24:41.100+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-04T00:00:00+00:00, run_after=2023-09-05T00:00:00+00:00[0m
[[34m2023-09-11T08:24:41.144+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:41.144+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:41.145+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:41.147+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:41.147+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:41.148+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:41.161+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:42.950+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:43.072+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:43.242+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:43.273+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:43.776+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:43.777+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:43.844+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-03T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:43.905+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:44.875+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:44.886+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:43.991008+00:00, run_end_date=2023-09-11 08:24:44.199064+00:00, run_duration=0.208056, state=success, executor_state=success, try_number=1, max_tries=0, job_id=505, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:41.145914+00:00, queued_by_job_id=2, pid=51829[0m
[[34m2023-09-11T08:24:45.369+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-05T00:00:00+00:00, run_after=2023-09-06T00:00:00+00:00[0m
[[34m2023-09-11T08:24:45.426+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-03 00:00:00+00:00: scheduled__2023-09-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:41.095400+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:45.427+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-03 00:00:00+00:00, run_id=scheduled__2023-09-03T00:00:00+00:00, run_start_date=2023-09-11 08:24:41.113247+00:00, run_end_date=2023-09-11 08:24:45.427078+00:00, run_duration=4.313831, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-03 00:00:00+00:00, data_interval_end=2023-09-04 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:45.430+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-04T00:00:00+00:00, run_after=2023-09-05T00:00:00+00:00[0m
[[34m2023-09-11T08:24:45.445+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:45.445+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:45.445+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:45.447+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:45.448+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:45.448+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:45.459+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:47.258+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:47.386+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:47.557+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:47.589+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:48.038+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:48.039+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:48.106+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-04T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:48.167+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:48.893+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:48.903+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:48.256885+00:00, run_end_date=2023-09-11 08:24:48.453391+00:00, run_duration=0.196506, state=success, executor_state=success, try_number=1, max_tries=0, job_id=506, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:45.446493+00:00, queued_by_job_id=2, pid=51835[0m
[[34m2023-09-11T08:24:49.156+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-05T00:00:00+00:00, run_after=2023-09-06T00:00:00+00:00[0m
[[34m2023-09-11T08:24:49.181+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-04 00:00:00+00:00: scheduled__2023-09-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:45.364978+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:49.181+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-04 00:00:00+00:00, run_id=scheduled__2023-09-04T00:00:00+00:00, run_start_date=2023-09-11 08:24:45.404467+00:00, run_end_date=2023-09-11 08:24:49.181832+00:00, run_duration=3.777365, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-04 00:00:00+00:00, data_interval_end=2023-09-05 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:49.185+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-05T00:00:00+00:00, run_after=2023-09-06T00:00:00+00:00[0m
[[34m2023-09-11T08:24:50.063+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-06T00:00:00+00:00, run_after=2023-09-07T00:00:00+00:00[0m
[[34m2023-09-11T08:24:50.110+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:50.111+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:50.111+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:50.113+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:50.114+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:50.114+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:50.117+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:51.928+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:52.049+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:52.218+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:52.249+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:52.703+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:52.704+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:52.772+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-05T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:52.827+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:53.500+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:53.511+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:52.914104+00:00, run_end_date=2023-09-11 08:24:53.105147+00:00, run_duration=0.191043, state=success, executor_state=success, try_number=1, max_tries=0, job_id=507, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:50.112030+00:00, queued_by_job_id=2, pid=51844[0m
[[34m2023-09-11T08:24:53.831+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00[0m
[[34m2023-09-11T08:24:53.868+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-05 00:00:00+00:00: scheduled__2023-09-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:50.058975+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:53.868+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-05 00:00:00+00:00, run_id=scheduled__2023-09-05T00:00:00+00:00, run_start_date=2023-09-11 08:24:50.079029+00:00, run_end_date=2023-09-11 08:24:53.868390+00:00, run_duration=3.789361, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-05 00:00:00+00:00, data_interval_end=2023-09-06 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:53.872+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-06T00:00:00+00:00, run_after=2023-09-07T00:00:00+00:00[0m
[[34m2023-09-11T08:24:53.886+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:53.886+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:53.887+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:53.889+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:53.889+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:53.890+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:53.892+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:55.704+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:24:55.831+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:55.995+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:56.026+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:24:56.482+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:24:56.482+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:24:56.552+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-06T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:24:56.610+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:24:57.322+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:24:57.333+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:24:56.699506+00:00, run_end_date=2023-09-11 08:24:56.890715+00:00, run_duration=0.191209, state=success, executor_state=success, try_number=1, max_tries=0, job_id=508, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:53.887808+00:00, queued_by_job_id=2, pid=51850[0m
[[34m2023-09-11T08:24:57.480+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00[0m
[[34m2023-09-11T08:24:57.502+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-06 00:00:00+00:00: scheduled__2023-09-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:53.825854+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:24:57.503+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-06 00:00:00+00:00, run_id=scheduled__2023-09-06T00:00:00+00:00, run_start_date=2023-09-11 08:24:53.844963+00:00, run_end_date=2023-09-11 08:24:57.503108+00:00, run_duration=3.658145, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-06 00:00:00+00:00, data_interval_end=2023-09-07 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:24:57.506+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00[0m
[[34m2023-09-11T08:24:58.668+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00[0m
[[34m2023-09-11T08:24:58.727+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:58.727+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:24:58.728+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:24:58.730+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:24:58.731+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:24:58.731+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:24:58.733+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:25:00.578+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:25:00.708+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:25:00.882+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:25:00.914+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:25:01.359+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:25:01.360+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:25:01.432+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-07T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:25:01.488+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:25:02.142+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:25:02.152+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:25:01.570330+00:00, run_end_date=2023-09-11 08:25:01.760977+00:00, run_duration=0.190647, state=success, executor_state=success, try_number=1, max_tries=0, job_id=509, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:24:58.728912+00:00, queued_by_job_id=2, pid=51859[0m
[[34m2023-09-11T08:25:02.874+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-09T00:00:00+00:00, run_after=2023-09-10T00:00:00+00:00[0m
[[34m2023-09-11T08:25:02.920+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-07 00:00:00+00:00: scheduled__2023-09-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:24:58.663600+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:25:02.921+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-07 00:00:00+00:00, run_id=scheduled__2023-09-07T00:00:00+00:00, run_start_date=2023-09-11 08:24:58.696439+00:00, run_end_date=2023-09-11 08:25:02.921176+00:00, run_duration=4.224737, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-07 00:00:00+00:00, data_interval_end=2023-09-08 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:25:02.924+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00[0m
[[34m2023-09-11T08:25:02.939+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:25:02.940+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:25:02.940+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:25:02.942+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:25:02.943+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:25:02.943+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:25:02.946+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:25:04.756+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:25:04.880+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:25:05.047+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:25:05.079+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:25:05.537+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:25:05.538+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:25:05.605+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-08T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:25:05.661+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:25:06.583+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:25:06.594+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:25:05.747200+00:00, run_end_date=2023-09-11 08:25:05.934445+00:00, run_duration=0.187245, state=success, executor_state=success, try_number=1, max_tries=0, job_id=510, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:25:02.941240+00:00, queued_by_job_id=2, pid=51867[0m
[[34m2023-09-11T08:25:06.888+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-09T00:00:00+00:00, run_after=2023-09-10T00:00:00+00:00[0m
[[34m2023-09-11T08:25:06.918+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-08 00:00:00+00:00: scheduled__2023-09-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:25:02.869023+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:25:06.919+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-08 00:00:00+00:00, run_id=scheduled__2023-09-08T00:00:00+00:00, run_start_date=2023-09-11 08:25:02.886500+00:00, run_end_date=2023-09-11 08:25:06.918979+00:00, run_duration=4.032479, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-08 00:00:00+00:00, data_interval_end=2023-09-09 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:25:06.922+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-09T00:00:00+00:00, run_after=2023-09-10T00:00:00+00:00[0m
[[34m2023-09-11T08:25:07.408+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00[0m
[[34m2023-09-11T08:25:07.452+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:25:07.452+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:25:07.453+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:25:07.455+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:25:07.455+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:25:07.455+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:25:07.458+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:25:09.259+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:25:09.389+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:25:09.556+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:25:09.588+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:25:10.099+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:25:10.099+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:25:10.180+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-09T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:25:10.237+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:25:10.939+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:25:10.951+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:25:10.319263+00:00, run_end_date=2023-09-11 08:25:10.526068+00:00, run_duration=0.206805, state=success, executor_state=success, try_number=1, max_tries=0, job_id=511, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:25:07.453747+00:00, queued_by_job_id=2, pid=51876[0m
[[34m2023-09-11T08:25:10.973+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:25:11.221+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-11T00:00:00+00:00, run_after=2023-09-12T00:00:00+00:00[0m
[[34m2023-09-11T08:25:11.256+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-09 00:00:00+00:00: scheduled__2023-09-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:25:07.404022+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:25:11.256+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-09 00:00:00+00:00, run_id=scheduled__2023-09-09T00:00:00+00:00, run_start_date=2023-09-11 08:25:07.421122+00:00, run_end_date=2023-09-11 08:25:11.256866+00:00, run_duration=3.835744, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-09 00:00:00+00:00, data_interval_end=2023-09-10 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:25:11.260+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00[0m
[[34m2023-09-11T08:25:11.275+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: python_operation.print_python scheduled__2023-09-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:25:11.276+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-11T08:25:11.276+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: python_operation.print_python scheduled__2023-09-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T08:25:11.278+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-11T08:25:11.279+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T08:25:11.279+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:25:11.281+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-11T08:25:13.151+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-11T08:25:13.286+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:25:13.486+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T08:25:13.523+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T08:25:14.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T08:25:14.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T08:25:14.079+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-10T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-11T08:25:14.134+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T08:25:14.917+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T08:25:14.928+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 08:25:14.222895+00:00, run_end_date=2023-09-11 08:25:14.471851+00:00, run_duration=0.248956, state=success, executor_state=success, try_number=1, max_tries=0, job_id=512, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-11 08:25:11.277091+00:00, queued_by_job_id=2, pid=51884[0m
[[34m2023-09-11T08:25:15.319+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-11T00:00:00+00:00, run_after=2023-09-12T00:00:00+00:00[0m
[[34m2023-09-11T08:25:15.343+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-10 00:00:00+00:00: scheduled__2023-09-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 08:25:11.216734+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T08:25:15.343+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-10 00:00:00+00:00, run_id=scheduled__2023-09-10T00:00:00+00:00, run_start_date=2023-09-11 08:25:11.234370+00:00, run_end_date=2023-09-11 08:25:15.343858+00:00, run_duration=4.109488, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-10 00:00:00+00:00, data_interval_end=2023-09-11 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-11T08:25:15.347+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-11T00:00:00+00:00, run_after=2023-09-12T00:00:00+00:00[0m
[[34m2023-09-11T08:30:11.260+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:35:11.944+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:40:12.076+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:45:12.408+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:50:12.685+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T08:55:13.068+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:00:13.381+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:05:13.797+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:10:14.828+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:15:15.310+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:20:15.715+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:25:16.038+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:30:16.072+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:35:16.359+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:40:16.408+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:45:16.821+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:50:17.122+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T09:55:17.272+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:00:17.286+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:05:17.479+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:10:17.871+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:15:18.144+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:20:18.435+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:25:18.721+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:30:19.112+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:35:19.524+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:40:19.814+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:45:20.018+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:50:20.300+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T10:55:20.324+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:00:20.488+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:05:20.927+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:10:21.206+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:15:21.384+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:20:21.744+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:25:22.028+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:30:22.193+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:35:22.373+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:40:22.875+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:45:23.288+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:50:23.554+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T11:55:23.636+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:00:23.950+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:05:24.122+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:10:24.429+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:15:24.645+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:20:24.980+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:25:25.529+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:30:25.863+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:35:26.165+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:40:26.504+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:45:26.702+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:50:26.966+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T12:55:27.343+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:00:27.650+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:05:27.971+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:10:28.239+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:15:28.548+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:20:28.973+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:25:29.301+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:30:29.589+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:35:29.856+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:40:30.306+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:45:30.644+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:50:30.992+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T13:55:31.179+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T14:00:31.440+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T14:05:31.798+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T14:10:32.073+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T14:15:32.411+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T14:20:32.855+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T14:25:32.899+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-11 15:43:58 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:38657)
[2023-09-11 15:43:58 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:38661)
[2023-09-11 15:43:58 +0000] [38657] [INFO] Worker exiting (pid: 38657)
[2023-09-11 15:43:58 +0000] [38661] [INFO] Worker exiting (pid: 38661)
[2023-09-11 15:43:58 +0000] [10437] [ERROR] Worker (pid:38657) exited with code 1
[2023-09-11 15:43:58 +0000] [10437] [ERROR] Worker (pid:38657) exited with code 1.
[2023-09-11 15:43:58 +0000] [10437] [ERROR] Worker (pid:38661) exited with code 1
[2023-09-11 15:43:58 +0000] [10437] [ERROR] Worker (pid:38661) exited with code 1.
[2023-09-11 15:43:58 +0000] [87963] [INFO] Booting worker with pid: 87963
[2023-09-11 15:43:58 +0000] [87964] [INFO] Booting worker with pid: 87964
[[34m2023-09-11T15:48:26.483+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T15:53:26.868+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T15:58:27.387+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:03:27.696+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:08:27.999+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:13:28.372+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:18:28.638+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:23:29.163+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:28:29.547+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:33:29.931+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:38:30.101+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:43:30.682+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:48:31.119+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:53:31.660+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T16:58:31.932+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:03:32.075+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:08:32.310+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:13:32.586+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:18:32.689+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:23:32.875+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:28:33.057+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:33:33.333+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:38:33.620+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:43:34.007+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:48:34.078+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:53:34.454+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T17:58:34.557+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:03:34.867+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:08:35.139+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:13:35.251+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:18:35.523+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:23:35.659+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:28:36.559+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:33:36.753+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:38:37.018+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:43:37.287+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:48:37.562+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:53:37.711+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T18:58:38.005+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T19:03:38.342+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T19:08:38.508+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T19:13:38.861+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T19:18:39.130+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T19:23:39.421+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T19:28:39.791+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-12 04:30:09 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:87963)
[2023-09-12 04:30:09 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:87964)
[2023-09-12 04:30:09 +0000] [87963] [INFO] Worker exiting (pid: 87963)
[2023-09-12 04:30:09 +0000] [87964] [INFO] Worker exiting (pid: 87964)
[2023-09-12 04:30:09 +0000] [10437] [ERROR] Worker (pid:87964) exited with code 1
[2023-09-12 04:30:09 +0000] [10437] [ERROR] Worker (pid:87964) exited with code 1.
[2023-09-12 04:30:09 +0000] [10437] [ERROR] Worker (pid:87963) exited with code 1
[2023-09-12 04:30:09 +0000] [10437] [ERROR] Worker (pid:87963) exited with code 1.
[2023-09-12 04:30:09 +0000] [111354] [INFO] Booting worker with pid: 111354
[2023-09-12 04:30:09 +0000] [111364] [INFO] Booting worker with pid: 111364
[[34m2023-09-12T04:30:10.387+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-12T00:00:00+00:00, run_after=2023-09-13T00:00:00+00:00[0m
[[34m2023-09-12T04:30:10.396+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-12T00:00:00+00:00, run_after=2023-09-13T00:00:00+00:00[0m
[[34m2023-09-12T04:30:10.467+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 2 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: python_operation.print_python scheduled__2023-09-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-12T04:30:10.467+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-12T04:30:10.468+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-12T04:30:10.468+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: python_operation.print_python scheduled__2023-09-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-12T04:30:10.473+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-12T04:30:10.474+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-12T04:30:10.474+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-12T04:30:10.475+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-12T04:30:10.475+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-12T04:30:10.476+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-12T04:30:10.479+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-12T04:30:12.420+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-12T04:30:12.550+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-12T04:30:12.725+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-12T04:30:12.758+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-12T04:30:13.272+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-12T04:30:13.272+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-12T04:30:13.347+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-12T04:30:13.406+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-12T04:30:14.146+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-12T04:30:15.971+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-12T04:30:16.110+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-12T04:30:16.282+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-12T04:30:16.314+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-12T04:30:16.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-12T04:30:16.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-12T04:30:16.850+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-11T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-12T04:30:16.907+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-12T04:30:17.553+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-12T04:30:17.554+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-12T04:30:17.572+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-12 04:30:13.496415+00:00, run_end_date=2023-09-12 04:30:13.740560+00:00, run_duration=0.244145, state=success, executor_state=success, try_number=1, max_tries=0, job_id=513, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-12 04:30:10.469843+00:00, queued_by_job_id=2, pid=111462[0m
[[34m2023-09-12T04:30:17.572+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-12 04:30:16.991966+00:00, run_end_date=2023-09-12 04:30:17.186731+00:00, run_duration=0.194765, state=success, executor_state=success, try_number=1, max_tries=0, job_id=514, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-12 04:30:10.469843+00:00, queued_by_job_id=2, pid=111468[0m
[[34m2023-09-12T04:30:18.085+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-11 00:00:00+00:00: scheduled__2023-09-11T00:00:00+00:00, state:running, queued_at: 2023-09-12 04:30:10.381288+00:00. externally triggered: False> successful[0m
[[34m2023-09-12T04:30:18.086+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-11 00:00:00+00:00, run_id=scheduled__2023-09-11T00:00:00+00:00, run_start_date=2023-09-12 04:30:10.416466+00:00, run_end_date=2023-09-12 04:30:18.086253+00:00, run_duration=7.669787, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-11 00:00:00+00:00, data_interval_end=2023-09-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-12T04:30:18.089+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-12T00:00:00+00:00, run_after=2023-09-13T00:00:00+00:00[0m
[[34m2023-09-12T04:30:18.095+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-11 00:00:00+00:00: scheduled__2023-09-11T00:00:00+00:00, state:running, queued_at: 2023-09-12 04:30:10.389866+00:00. externally triggered: False> successful[0m
[[34m2023-09-12T04:30:18.095+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-11 00:00:00+00:00, run_id=scheduled__2023-09-11T00:00:00+00:00, run_start_date=2023-09-12 04:30:10.418005+00:00, run_end_date=2023-09-12 04:30:18.095901+00:00, run_duration=7.677896, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-11 00:00:00+00:00, data_interval_end=2023-09-12 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-12T04:30:18.099+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-12T00:00:00+00:00, run_after=2023-09-13T00:00:00+00:00[0m
[[34m2023-09-12T04:30:53.407+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T04:35:53.673+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T04:40:53.836+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T04:45:54.138+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T04:50:54.355+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T04:55:54.643+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:00:54.966+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:05:55.332+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:10:55.724+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:15:55.869+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:20:56.295+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:25:56.452+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:30:56.638+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:35:56.827+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:40:57.098+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:45:57.518+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:50:57.558+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T05:55:58.131+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:00:58.429+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:05:58.775+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:10:59.081+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:15:59.453+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:20:59.743+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:26:00.094+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:31:00.367+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:36:00.579+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:41:00.976+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:46:01.053+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:51:01.494+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T06:56:01.800+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:01:02.083+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:06:02.453+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:11:02.867+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:16:03.136+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:21:03.635+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:26:04.473+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:31:04.757+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:36:05.046+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:41:05.217+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:46:05.453+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:51:05.758+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T07:56:05.958+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:01:06.330+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:06:06.599+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:11:06.950+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:16:07.214+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:21:08.028+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:26:08.231+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:31:08.538+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:36:08.944+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:41:09.242+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T08:46:09.522+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-12 10:48:32 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:111354)
[2023-09-12 10:48:32 +0000] [111354] [INFO] Worker exiting (pid: 111354)
[2023-09-12 10:48:32 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:111364)
[2023-09-12 10:48:32 +0000] [111364] [INFO] Worker exiting (pid: 111364)
[2023-09-12 10:48:32 +0000] [10437] [ERROR] Worker (pid:111354) exited with code 1
[2023-09-12 10:48:32 +0000] [10437] [ERROR] Worker (pid:111354) exited with code 1.
[2023-09-12 10:48:32 +0000] [10437] [ERROR] Worker (pid:111364) exited with code 1
[2023-09-12 10:48:32 +0000] [10437] [ERROR] Worker (pid:111364) exited with code 1.
[2023-09-12 10:48:32 +0000] [138201] [INFO] Booting worker with pid: 138201
[2023-09-12 10:48:32 +0000] [138202] [INFO] Booting worker with pid: 138202
[[34m2023-09-12T10:49:08.821+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T10:54:08.880+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T10:59:09.199+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:04:09.475+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:09:09.659+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:14:09.696+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:19:09.930+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:24:10.303+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:29:10.675+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:34:10.868+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:39:11.062+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:44:11.083+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T11:49:11.408+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-12 12:40:39 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:138201)
[2023-09-12 12:40:39 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:138202)
[2023-09-12 12:40:39 +0000] [138202] [INFO] Worker exiting (pid: 138202)
[2023-09-12 12:40:39 +0000] [138201] [INFO] Worker exiting (pid: 138201)
[2023-09-12 12:40:39 +0000] [10437] [ERROR] Worker (pid:138201) exited with code 1
[2023-09-12 12:40:39 +0000] [10437] [ERROR] Worker (pid:138201) exited with code 1.
[2023-09-12 12:40:39 +0000] [10437] [ERROR] Worker (pid:138202) exited with code 1
[2023-09-12 12:40:39 +0000] [10437] [ERROR] Worker (pid:138202) exited with code 1.
[2023-09-12 12:40:39 +0000] [144562] [INFO] Booting worker with pid: 144562
[2023-09-12 12:40:39 +0000] [144563] [INFO] Booting worker with pid: 144563
[[34m2023-09-12T12:45:02.341+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T12:50:02.628+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T12:55:02.796+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:00:03.040+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:05:03.350+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:10:03.633+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:15:04.223+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:20:04.424+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:25:04.593+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:30:04.631+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:35:04.808+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:40:05.088+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:45:05.371+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:50:05.559+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T13:55:05.740+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:00:06.043+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:05:06.523+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:10:06.785+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:15:07.155+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:20:07.440+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:25:07.607+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:30:07.872+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:35:08.180+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:40:08.504+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:45:08.808+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:50:09.077+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T14:55:09.283+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T15:00:09.319+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T15:05:09.696+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T15:10:09.800+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T15:15:10.120+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T15:20:10.483+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T15:25:10.751+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T15:30:11.035+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T16:34:44.239+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T16:39:44.265+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T16:44:44.648+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T16:49:44.833+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T16:54:45.014+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T16:59:45.288+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-12 18:44:04 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:144562)
[2023-09-12 18:44:04 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:144563)
[2023-09-12 18:44:04 +0000] [144562] [INFO] Worker exiting (pid: 144562)
[2023-09-12 18:44:04 +0000] [144563] [INFO] Worker exiting (pid: 144563)
[2023-09-12 18:44:04 +0000] [10437] [ERROR] Worker (pid:144562) exited with code 1
[2023-09-12 18:44:04 +0000] [10437] [ERROR] Worker (pid:144562) exited with code 1.
[2023-09-12 18:44:04 +0000] [10437] [ERROR] Worker (pid:144563) exited with code 1
[2023-09-12 18:44:04 +0000] [10437] [ERROR] Worker (pid:144563) exited with code 1.
[2023-09-12 18:44:04 +0000] [166547] [INFO] Booting worker with pid: 166547
[2023-09-12 18:44:04 +0000] [166548] [INFO] Booting worker with pid: 166548
[[34m2023-09-12T18:45:56.579+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T18:50:57.643+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T18:55:57.966+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T19:00:58.306+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T19:05:59.043+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-12T19:10:59.214+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-13 05:23:13 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:166547)
[2023-09-13 05:23:13 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:166548)
[2023-09-13 05:23:13 +0000] [166548] [INFO] Worker exiting (pid: 166548)
[2023-09-13 05:23:13 +0000] [166547] [INFO] Worker exiting (pid: 166547)
[2023-09-13 05:23:13 +0000] [10437] [ERROR] Worker (pid:166548) exited with code 1
[2023-09-13 05:23:13 +0000] [10437] [ERROR] Worker (pid:166548) exited with code 1.
[2023-09-13 05:23:13 +0000] [10437] [ERROR] Worker (pid:166547) exited with code 1
[2023-09-13 05:23:13 +0000] [10437] [ERROR] Worker (pid:166547) exited with code 1.
[2023-09-13 05:23:13 +0000] [169652] [INFO] Booting worker with pid: 169652
[2023-09-13 05:23:13 +0000] [169653] [INFO] Booting worker with pid: 169653
[[34m2023-09-13T05:23:14.483+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-13T00:00:00+00:00, run_after=2023-09-14T00:00:00+00:00[0m
[[34m2023-09-13T05:23:14.493+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-13T00:00:00+00:00, run_after=2023-09-14T00:00:00+00:00[0m
[[34m2023-09-13T05:23:14.611+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 2 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: python_operation.print_python scheduled__2023-09-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-13T05:23:14.612+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-13T05:23:14.612+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG python_operation has 0/16 running and queued tasks[0m
[[34m2023-09-13T05:23:14.612+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: python_operation.print_python scheduled__2023-09-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-13T05:23:14.619+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-13T05:23:14.620+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task print_python because previous state change time has not been saved[0m
[[34m2023-09-13T05:23:14.621+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-13T05:23:14.622+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-13T05:23:14.622+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-13T05:23:14.623+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-13T05:23:14.631+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-13T05:23:17.396+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-13T05:23:17.576+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-13T05:23:17.772+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-13T05:23:17.809+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-13T05:23:18.397+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-13T05:23:18.402+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-13T05:23:18.605+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-13T05:23:18.762+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-13T05:23:20.318+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'python_operation', 'print_python', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/1-tasks_dag.py'][0m
[[34m2023-09-13T05:23:23.207+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/1-tasks_dag.py[0m
[[34m2023-09-13T05:23:23.350+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-13T05:23:23.536+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-13T05:23:23.573+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-13T05:23:24.073+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-13T05:23:24.073+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-13T05:23:24.159+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=python_operation/run_id=scheduled__2023-09-12T00:00:00+00:00/task_id=print_python permission to 509
[[34m2023-09-13T05:23:24.218+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: python_operation.print_python scheduled__2023-09-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-13T05:23:29.653+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-13T05:23:29.654+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operation', task_id='print_python', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-13T05:23:29.666+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-13 05:23:19.530589+00:00, run_end_date=2023-09-13 05:23:19.821514+00:00, run_duration=0.290925, state=success, executor_state=success, try_number=1, max_tries=0, job_id=515, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-13 05:23:14.615628+00:00, queued_by_job_id=2, pid=169712[0m
[[34m2023-09-13T05:23:29.667+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=python_operation, task_id=print_python, run_id=scheduled__2023-09-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-13 05:23:24.626816+00:00, run_end_date=2023-09-13 05:23:27.511732+00:00, run_duration=2.884916, state=success, executor_state=success, try_number=1, max_tries=0, job_id=516, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2023-09-13 05:23:14.615628+00:00, queued_by_job_id=2, pid=170133[0m
[[34m2023-09-13T05:23:29.958+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-12 00:00:00+00:00: scheduled__2023-09-12T00:00:00+00:00, state:running, queued_at: 2023-09-13 05:23:14.453432+00:00. externally triggered: False> successful[0m
[[34m2023-09-13T05:23:29.958+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-12 00:00:00+00:00, run_id=scheduled__2023-09-12T00:00:00+00:00, run_start_date=2023-09-13 05:23:14.522322+00:00, run_end_date=2023-09-13 05:23:29.958433+00:00, run_duration=15.436111, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-12 00:00:00+00:00, data_interval_end=2023-09-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-13T05:23:29.962+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-13T00:00:00+00:00, run_after=2023-09-14T00:00:00+00:00[0m
[[34m2023-09-13T05:23:29.969+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun python_operation @ 2023-09-12 00:00:00+00:00: scheduled__2023-09-12T00:00:00+00:00, state:running, queued_at: 2023-09-13 05:23:14.487087+00:00. externally triggered: False> successful[0m
[[34m2023-09-13T05:23:29.970+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=python_operation, execution_date=2023-09-12 00:00:00+00:00, run_id=scheduled__2023-09-12T00:00:00+00:00, run_start_date=2023-09-13 05:23:14.523728+00:00, run_end_date=2023-09-13 05:23:29.970103+00:00, run_duration=15.446375, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-12 00:00:00+00:00, data_interval_end=2023-09-13 00:00:00+00:00, dag_hash=4e02e8d3b6cc77bc2bc87a03cb7e4176[0m
[[34m2023-09-13T05:23:29.974+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for python_operation to 2023-09-13T00:00:00+00:00, run_after=2023-09-14T00:00:00+00:00[0m
[[34m2023-09-13T05:23:32.799+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-13T05:23:32.804+0000[0m] {[34mscheduler_job_runner.py:[0m1609} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2023-09-13T05:28:33.181+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-13T05:33:33.747+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
