nohup: ignoring input
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2023-09-10T12:32:59.874+0000[0m] {[34mexecutor_loader.py:[0m117} INFO[0m - Loaded executor: SequentialExecutor[0m
[2023-09-10 12:33:00 +0000] [10437] [INFO] Starting gunicorn 21.2.0
[2023-09-10 12:33:00 +0000] [10437] [INFO] Listening at: http://[::]:8793 (10437)
[2023-09-10 12:33:00 +0000] [10437] [INFO] Using worker: sync
[2023-09-10 12:33:00 +0000] [10438] [INFO] Booting worker with pid: 10438
[[34m2023-09-10T12:33:00.100+0000[0m] {[34mscheduler_job_runner.py:[0m798} INFO[0m - Starting the scheduler[0m
[[34m2023-09-10T12:33:00.103+0000[0m] {[34mscheduler_job_runner.py:[0m805} INFO[0m - Processing each file at most -1 times[0m
[2023-09-10 12:33:00 +0000] [10441] [INFO] Booting worker with pid: 10441
[[34m2023-09-10T12:33:00.112+0000[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 10442[0m
[[34m2023-09-10T12:33:00.116+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:33:00.124+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-09-10T12:33:00.195+0000] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-09-10T12:38:00.471+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:43:00.744+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:48:00.873+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:53:01.231+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T12:58:01.503+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:03:01.787+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:08:02.054+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:13:02.326+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:18:02.510+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:23:02.628+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:28:02.997+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:33:03.292+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:38:03.458+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:43:03.726+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:48:03.995+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:53:04.288+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:58:04.597+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T13:58:30.989+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 13:51:53+00:00: manual__2023-09-10T13:51:53+00:00, state:running, queued_at: 2023-09-10 13:51:53.445715+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T13:58:30.991+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 13:51:53+00:00, run_id=manual__2023-09-10T13:51:53+00:00, run_start_date=2023-09-10 13:58:30.936354+00:00, run_end_date=2023-09-10 13:58:30.990910+00:00, run_duration=0.054556, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 13:51:53+00:00, data_interval_end=2023-09-10 13:51:53+00:00, dag_hash=293888ec3e25412de58e13b922f598df[0m
[[34m2023-09-10T13:58:30.995+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T13:51:53+00:00, run_after=2023-09-11T13:51:53+00:00[0m
[[34m2023-09-10T13:58:31.007+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 13:53:18+00:00: manual__2023-09-10T13:53:18+00:00, state:running, queued_at: 2023-09-10 13:53:18.806127+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T13:58:31.007+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 13:53:18+00:00, run_id=manual__2023-09-10T13:53:18+00:00, run_start_date=2023-09-10 13:58:30.936878+00:00, run_end_date=2023-09-10 13:58:31.007671+00:00, run_duration=0.070793, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 13:53:18+00:00, data_interval_end=2023-09-10 13:53:18+00:00, dag_hash=293888ec3e25412de58e13b922f598df[0m
[[34m2023-09-10T13:58:31.010+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T13:53:18+00:00, run_after=2023-09-11T13:53:18+00:00[0m
[[34m2023-09-10T14:00:51.810+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 14:00:50.536342+00:00: manual__2023-09-10T14:00:50.536342+00:00, state:running, queued_at: 2023-09-10 14:00:50.595656+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T14:00:51.810+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 14:00:50.536342+00:00, run_id=manual__2023-09-10T14:00:50.536342+00:00, run_start_date=2023-09-10 14:00:51.795208+00:00, run_end_date=2023-09-10 14:00:51.810792+00:00, run_duration=0.015584, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 14:00:50.536342+00:00, data_interval_end=2023-09-10 14:00:50.536342+00:00, dag_hash=293888ec3e25412de58e13b922f598df[0m
[[34m2023-09-10T14:00:51.813+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T14:00:50.536342+00:00, run_after=2023-09-11T14:00:50.536342+00:00[0m
[[34m2023-09-10T14:01:14.394+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 14:01:13.650498+00:00: manual__2023-09-10T14:01:13.650498+00:00, state:running, queued_at: 2023-09-10 14:01:13.681643+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T14:01:14.395+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 14:01:13.650498+00:00, run_id=manual__2023-09-10T14:01:13.650498+00:00, run_start_date=2023-09-10 14:01:14.344663+00:00, run_end_date=2023-09-10 14:01:14.394954+00:00, run_duration=0.050291, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 14:01:13.650498+00:00, data_interval_end=2023-09-10 14:01:13.650498+00:00, dag_hash=293888ec3e25412de58e13b922f598df[0m
[[34m2023-09-10T14:01:14.398+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T14:01:13.650498+00:00, run_after=2023-09-11T14:01:13.650498+00:00[0m
[[34m2023-09-10T14:03:04.875+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:08:05.174+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:13:05.281+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-10T14:17:59.579+0000] {manager.py:543} INFO - DAG etl_workflow is missing and will be deactivated.
[2023-09-10T14:17:59.587+0000] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2023-09-10T14:17:59.591+0000] {manager.py:557} INFO - Deleted DAG etl_workflow in serialized_dag table
[[34m2023-09-10T14:18:05.616+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:23:05.779+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:28:05.951+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:33:06.155+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T14:36:46.212+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 14:36:45.940783+00:00: manual__2023-09-10T14:36:45.940783+00:00, state:running, queued_at: 2023-09-10 14:36:45.989405+00:00. externally triggered: True> successful[0m
[[34m2023-09-10T14:36:46.213+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 14:36:45.940783+00:00, run_id=manual__2023-09-10T14:36:45.940783+00:00, run_start_date=2023-09-10 14:36:46.196161+00:00, run_end_date=2023-09-10 14:36:46.213120+00:00, run_duration=0.016959, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-09 14:36:45.940783+00:00, data_interval_end=2023-09-10 14:36:45.940783+00:00, dag_hash=79b760373eec0d82222a80c0bd784dc3[0m
[[34m2023-09-10T14:36:46.217+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00[0m
[[34m2023-09-10T14:38:06.446+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-10 16:10:17 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:10438)
[2023-09-10 16:10:17 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:10441)
[2023-09-10 16:10:17 +0000] [10438] [INFO] Worker exiting (pid: 10438)
[2023-09-10 16:10:17 +0000] [10441] [INFO] Worker exiting (pid: 10441)
[2023-09-10 16:10:17 +0000] [10437] [ERROR] Worker (pid:10438) exited with code 1
[2023-09-10 16:10:17 +0000] [10437] [ERROR] Worker (pid:10438) exited with code 1.
[2023-09-10 16:10:17 +0000] [10437] [ERROR] Worker (pid:10441) exited with code 1
[2023-09-10 16:10:17 +0000] [10437] [ERROR] Worker (pid:10441) exited with code 1.
[2023-09-10 16:10:17 +0000] [23714] [INFO] Booting worker with pid: 23714
[2023-09-10 16:10:17 +0000] [23716] [INFO] Booting worker with pid: 23716
[[34m2023-09-10T16:10:53.960+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:15:54.100+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:20:54.256+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:25:54.541+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:30:54.810+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:35:55.087+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:40:55.135+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:45:55.449+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:50:55.631+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T16:55:55.922+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:00:56.230+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:05:56.631+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:10:56.977+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:15:57.406+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:20:57.826+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:25:57.994+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:30:58.432+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:35:58.724+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:40:59.084+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:45:59.351+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:50:59.418+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T17:56:00.116+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:01:00.264+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:06:00.461+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:11:01.049+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:16:01.200+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:21:02.100+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:26:02.386+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:31:02.558+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-10T18:36:02.881+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2023-09-11 06:26:29 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:23714)
[2023-09-11 06:26:29 +0000] [10437] [CRITICAL] WORKER TIMEOUT (pid:23716)
[2023-09-11 06:26:29 +0000] [23714] [INFO] Worker exiting (pid: 23714)
[2023-09-11 06:26:29 +0000] [23716] [INFO] Worker exiting (pid: 23716)
[2023-09-11 06:26:29 +0000] [10437] [ERROR] Worker (pid:23716) exited with code 1
[2023-09-11 06:26:29 +0000] [10437] [ERROR] Worker (pid:23716) exited with code 1.
[2023-09-11 06:26:29 +0000] [10437] [ERROR] Worker (pid:23714) exited with code 1
[2023-09-11 06:26:29 +0000] [10437] [ERROR] Worker (pid:23714) exited with code 1.
[2023-09-11 06:26:29 +0000] [38657] [INFO] Booting worker with pid: 38657
[2023-09-11 06:26:29 +0000] [38661] [INFO] Booting worker with pid: 38661
[2023-09-11T06:26:30.274+0000] {manager.py:543} INFO - DAG example_external_task_marker_parent is missing and will be deactivated.
[2023-09-11T06:26:30.275+0000] {manager.py:543} INFO - DAG example_external_task_marker_child is missing and will be deactivated.
[2023-09-11T06:26:30.329+0000] {manager.py:553} INFO - Deactivated 2 DAGs which are no longer present in file.
[2023-09-11T06:26:30.354+0000] {manager.py:557} INFO - Deleted DAG example_external_task_marker_parent in serialized_dag table
[2023-09-11T06:26:30.452+0000] {manager.py:557} INFO - Deleted DAG example_external_task_marker_child in serialized_dag table
[[34m2023-09-11T06:29:55.500+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:34:55.769+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:39:40.383+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-02T00:00:00+00:00, run_after=2023-01-03T00:00:00+00:00[0m
[[34m2023-09-11T06:39:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:40.441+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:40.442+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:40.442+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:40.445+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:42.360+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:39:42.501+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:42.680+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:42.713+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:43.190+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:39:43.190+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:43.259+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:39:43.314+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:39:44.106+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:39:44.123+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:39:43.470130+00:00, run_end_date=2023-09-11 06:39:43.695420+00:00, run_duration=0.22529, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:40.439366+00:00, queued_by_job_id=2, pid=40739[0m
[[34m2023-09-11T06:39:44.477+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00[0m
[[34m2023-09-11T06:39:44.510+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-01 00:00:00+00:00: scheduled__2023-01-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:40.375379+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:39:44.511+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-01 00:00:00+00:00, run_id=scheduled__2023-01-01T00:00:00+00:00, run_start_date=2023-09-11 06:39:40.406312+00:00, run_end_date=2023-09-11 06:39:44.511186+00:00, run_duration=4.104874, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-01 00:00:00+00:00, data_interval_end=2023-01-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:39:44.515+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-02T00:00:00+00:00, run_after=2023-01-03T00:00:00+00:00[0m
[[34m2023-09-11T06:39:44.529+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:44.530+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:44.530+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:44.532+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:44.533+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:44.533+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:44.536+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:46.492+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:39:46.627+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:46.798+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:46.832+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:47.299+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:39:47.300+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:47.373+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:39:47.435+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:39:48.235+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:39:48.246+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:39:47.531289+00:00, run_end_date=2023-09-11 06:39:47.764989+00:00, run_duration=0.2337, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:44.531224+00:00, queued_by_job_id=2, pid=40749[0m
[[34m2023-09-11T06:39:48.491+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00[0m
[[34m2023-09-11T06:39:48.513+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-02 00:00:00+00:00: scheduled__2023-01-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:44.472358+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:39:48.513+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-02 00:00:00+00:00, run_id=scheduled__2023-01-02T00:00:00+00:00, run_start_date=2023-09-11 06:39:44.489014+00:00, run_end_date=2023-09-11 06:39:48.513855+00:00, run_duration=4.024841, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-02 00:00:00+00:00, data_interval_end=2023-01-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:39:48.517+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-03T00:00:00+00:00, run_after=2023-01-04T00:00:00+00:00[0m
[[34m2023-09-11T06:39:49.551+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00[0m
[[34m2023-09-11T06:39:49.594+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:49.594+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:49.594+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:49.596+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:49.597+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:49.597+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:49.600+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:51.528+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:39:51.675+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:51.860+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:51.898+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:52.445+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:39:52.446+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:52.524+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:39:52.590+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:39:53.402+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:39:53.413+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:39:52.685003+00:00, run_end_date=2023-09-11 06:39:52.931325+00:00, run_duration=0.246322, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:49.595321+00:00, queued_by_job_id=2, pid=40762[0m
[[34m2023-09-11T06:39:53.575+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-05T00:00:00+00:00, run_after=2023-01-06T00:00:00+00:00[0m
[[34m2023-09-11T06:39:53.613+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-03 00:00:00+00:00: scheduled__2023-01-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:49.546924+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:39:53.614+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-03 00:00:00+00:00, run_id=scheduled__2023-01-03T00:00:00+00:00, run_start_date=2023-09-11 06:39:49.562931+00:00, run_end_date=2023-09-11 06:39:53.614463+00:00, run_duration=4.051532, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-03 00:00:00+00:00, data_interval_end=2023-01-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:39:53.619+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-04T00:00:00+00:00, run_after=2023-01-05T00:00:00+00:00[0m
[[34m2023-09-11T06:39:53.635+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:53.635+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:53.635+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:53.638+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:53.638+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:53.639+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:53.641+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:55.788+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:39:55.946+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:56.150+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:56.187+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:39:56.684+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:39:56.685+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:39:56.759+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:39:56.819+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:39:57.593+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:39:57.605+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:39:56.904209+00:00, run_end_date=2023-09-11 06:39:57.158652+00:00, run_duration=0.254443, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:53.636711+00:00, queued_by_job_id=2, pid=40771[0m
[[34m2023-09-11T06:39:57.626+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:39:57.876+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-05T00:00:00+00:00, run_after=2023-01-06T00:00:00+00:00[0m
[[34m2023-09-11T06:39:57.899+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-04 00:00:00+00:00: scheduled__2023-01-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:53.570301+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:39:57.899+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-04 00:00:00+00:00, run_id=scheduled__2023-01-04T00:00:00+00:00, run_start_date=2023-09-11 06:39:53.588110+00:00, run_end_date=2023-09-11 06:39:57.899457+00:00, run_duration=4.311347, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-04 00:00:00+00:00, data_interval_end=2023-01-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:39:57.903+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-05T00:00:00+00:00, run_after=2023-01-06T00:00:00+00:00[0m
[[34m2023-09-11T06:39:58.587+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-06T00:00:00+00:00, run_after=2023-01-07T00:00:00+00:00[0m
[[34m2023-09-11T06:39:58.661+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:58.661+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:39:58.662+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:39:58.663+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:39:58.664+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:39:58.664+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:39:58.667+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:00.630+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:00.774+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:00.969+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:01.006+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:01.528+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:01.529+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:01.609+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:01.672+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:02.482+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:02.495+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:01.765217+00:00, run_end_date=2023-09-11 06:40:02.029368+00:00, run_duration=0.264151, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:39:58.662661+00:00, queued_by_job_id=2, pid=40784[0m
[[34m2023-09-11T06:40:02.662+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-07T00:00:00+00:00, run_after=2023-01-08T00:00:00+00:00[0m
[[34m2023-09-11T06:40:02.705+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-05 00:00:00+00:00: scheduled__2023-01-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:39:58.582250+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:02.705+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-05 00:00:00+00:00, run_id=scheduled__2023-01-05T00:00:00+00:00, run_start_date=2023-09-11 06:39:58.627868+00:00, run_end_date=2023-09-11 06:40:02.705422+00:00, run_duration=4.077554, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-05 00:00:00+00:00, data_interval_end=2023-01-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:02.709+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-06T00:00:00+00:00, run_after=2023-01-07T00:00:00+00:00[0m
[[34m2023-09-11T06:40:02.727+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:02.728+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:02.728+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:02.731+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:02.731+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:02.731+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:02.734+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:04.758+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:04.894+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:05.058+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:05.090+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:05.552+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:05.553+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:05.623+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:05.680+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:06.446+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:06.457+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:05.766569+00:00, run_end_date=2023-09-11 06:40:06.013298+00:00, run_duration=0.246729, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:02.729258+00:00, queued_by_job_id=2, pid=40791[0m
[[34m2023-09-11T06:40:06.659+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-07T00:00:00+00:00, run_after=2023-01-08T00:00:00+00:00[0m
[[34m2023-09-11T06:40:06.695+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-06 00:00:00+00:00: scheduled__2023-01-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:02.656912+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:06.696+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-06 00:00:00+00:00, run_id=scheduled__2023-01-06T00:00:00+00:00, run_start_date=2023-09-11 06:40:02.679593+00:00, run_end_date=2023-09-11 06:40:06.696035+00:00, run_duration=4.016442, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-06 00:00:00+00:00, data_interval_end=2023-01-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:06.699+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-07T00:00:00+00:00, run_after=2023-01-08T00:00:00+00:00[0m
[[34m2023-09-11T06:40:06.714+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number manual__2023-09-11T06:40:03+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:06.714+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:06.714+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number manual__2023-09-11T06:40:03+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:06.716+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:06.717+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='manual__2023-09-11T06:40:03+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:06.717+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'manual__2023-09-11T06:40:03+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:06.720+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'manual__2023-09-11T06:40:03+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:08.554+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:08.681+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:08.851+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:08.883+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:09.372+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:09.373+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:09.447+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=manual__2023-09-11T06:40:03+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:09.506+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number manual__2023-09-11T06:40:03+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:10.227+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='manual__2023-09-11T06:40:03+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:10.238+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=manual__2023-09-11T06:40:03+00:00, map_index=-1, run_start_date=2023-09-11 06:40:09.590795+00:00, run_end_date=2023-09-11 06:40:09.809267+00:00, run_duration=0.218472, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:06.715432+00:00, queued_by_job_id=2, pid=40798[0m
[[34m2023-09-11T06:40:10.452+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-08T00:00:00+00:00, run_after=2023-01-09T00:00:00+00:00[0m
[[34m2023-09-11T06:40:10.486+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-11 06:40:03+00:00: manual__2023-09-11T06:40:03+00:00, state:running, queued_at: 2023-09-11 06:40:03.754595+00:00. externally triggered: True> successful[0m
[[34m2023-09-11T06:40:10.487+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-11 06:40:03+00:00, run_id=manual__2023-09-11T06:40:03+00:00, run_start_date=2023-09-11 06:40:06.672701+00:00, run_end_date=2023-09-11 06:40:10.487222+00:00, run_duration=3.814521, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-09-10 06:40:03+00:00, data_interval_end=2023-09-11 06:40:03+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:10.490+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-11T06:40:03+00:00, run_after=2023-09-12T06:40:03+00:00[0m
[[34m2023-09-11T06:40:10.506+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:10.507+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:10.507+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:10.509+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:10.510+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:10.510+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:10.512+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:12.345+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:12.480+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:12.655+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:12.688+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:13.152+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:13.153+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:13.224+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:13.282+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:14.046+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:14.057+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:13.369343+00:00, run_end_date=2023-09-11 06:40:13.607271+00:00, run_duration=0.237928, state=success, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:10.508093+00:00, queued_by_job_id=2, pid=40807[0m
[[34m2023-09-11T06:40:14.483+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-07 00:00:00+00:00: scheduled__2023-01-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:10.447471+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:14.483+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-07 00:00:00+00:00, run_id=scheduled__2023-01-07T00:00:00+00:00, run_start_date=2023-09-11 06:40:10.465538+00:00, run_end_date=2023-09-11 06:40:14.483586+00:00, run_duration=4.018048, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-07 00:00:00+00:00, data_interval_end=2023-01-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:14.487+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-08T00:00:00+00:00, run_after=2023-01-09T00:00:00+00:00[0m
[[34m2023-09-11T06:40:15.632+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-09T00:00:00+00:00, run_after=2023-01-10T00:00:00+00:00[0m
[[34m2023-09-11T06:40:15.709+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:15.709+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:15.710+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:15.713+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:15.713+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:15.714+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:15.721+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:17.748+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:17.875+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:18.046+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:18.078+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:18.548+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:18.549+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:18.621+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:18.681+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:19.418+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:19.428+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:18.766916+00:00, run_end_date=2023-09-11 06:40:18.982970+00:00, run_duration=0.216054, state=success, executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:15.711168+00:00, queued_by_job_id=2, pid=40817[0m
[[34m2023-09-11T06:40:19.710+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-10T00:00:00+00:00, run_after=2023-01-11T00:00:00+00:00[0m
[[34m2023-09-11T06:40:19.746+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-08 00:00:00+00:00: scheduled__2023-01-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:15.625747+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:19.747+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-08 00:00:00+00:00, run_id=scheduled__2023-01-08T00:00:00+00:00, run_start_date=2023-09-11 06:40:15.655804+00:00, run_end_date=2023-09-11 06:40:19.746987+00:00, run_duration=4.091183, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-08 00:00:00+00:00, data_interval_end=2023-01-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:19.750+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-09T00:00:00+00:00, run_after=2023-01-10T00:00:00+00:00[0m
[[34m2023-09-11T06:40:19.766+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:19.766+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:19.766+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:19.768+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:19.769+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:19.769+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:19.772+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:21.708+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:21.867+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:22.099+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:22.141+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:22.619+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:22.619+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:22.687+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:22.743+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:23.488+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:23.498+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:22.845132+00:00, run_end_date=2023-09-11 06:40:23.071824+00:00, run_duration=0.226692, state=success, executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:19.767329+00:00, queued_by_job_id=2, pid=40826[0m
[[34m2023-09-11T06:40:23.743+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-10T00:00:00+00:00, run_after=2023-01-11T00:00:00+00:00[0m
[[34m2023-09-11T06:40:23.765+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-09 00:00:00+00:00: scheduled__2023-01-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:19.703175+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:23.766+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-09 00:00:00+00:00, run_id=scheduled__2023-01-09T00:00:00+00:00, run_start_date=2023-09-11 06:40:19.723873+00:00, run_end_date=2023-09-11 06:40:23.766280+00:00, run_duration=4.042407, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-09 00:00:00+00:00, data_interval_end=2023-01-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:23.770+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-10T00:00:00+00:00, run_after=2023-01-11T00:00:00+00:00[0m
[[34m2023-09-11T06:40:24.723+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-11T00:00:00+00:00, run_after=2023-01-12T00:00:00+00:00[0m
[[34m2023-09-11T06:40:24.773+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:24.773+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:24.773+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:24.775+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:24.776+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:24.776+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:24.779+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:26.710+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:26.864+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:27.049+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:27.082+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:27.560+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:27.561+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:27.630+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:27.687+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:28.414+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:28.425+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:27.777781+00:00, run_end_date=2023-09-11 06:40:28.002600+00:00, run_duration=0.224819, state=success, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:24.774386+00:00, queued_by_job_id=2, pid=40836[0m
[[34m2023-09-11T06:40:28.688+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-12T00:00:00+00:00, run_after=2023-01-13T00:00:00+00:00[0m
[[34m2023-09-11T06:40:28.742+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-10 00:00:00+00:00: scheduled__2023-01-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:24.716530+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:28.743+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-10 00:00:00+00:00, run_id=scheduled__2023-01-10T00:00:00+00:00, run_start_date=2023-09-11 06:40:24.739065+00:00, run_end_date=2023-09-11 06:40:28.742946+00:00, run_duration=4.003881, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-10 00:00:00+00:00, data_interval_end=2023-01-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:28.746+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-11T00:00:00+00:00, run_after=2023-01-12T00:00:00+00:00[0m
[[34m2023-09-11T06:40:28.762+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:28.762+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:28.762+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:28.764+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:28.765+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:28.765+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:28.768+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:30.683+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:30.811+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:30.985+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:31.018+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:31.507+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:31.508+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:31.581+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:31.645+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:32.641+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:32.652+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:31.732479+00:00, run_end_date=2023-09-11 06:40:32.058965+00:00, run_duration=0.326486, state=success, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:28.763398+00:00, queued_by_job_id=2, pid=40845[0m
[[34m2023-09-11T06:40:32.908+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-12T00:00:00+00:00, run_after=2023-01-13T00:00:00+00:00[0m
[[34m2023-09-11T06:40:32.931+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-11 00:00:00+00:00: scheduled__2023-01-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:28.683710+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:32.932+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-11 00:00:00+00:00, run_id=scheduled__2023-01-11T00:00:00+00:00, run_start_date=2023-09-11 06:40:28.703038+00:00, run_end_date=2023-09-11 06:40:32.931946+00:00, run_duration=4.228908, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-11 00:00:00+00:00, data_interval_end=2023-01-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:32.935+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-12T00:00:00+00:00, run_after=2023-01-13T00:00:00+00:00[0m
[[34m2023-09-11T06:40:33.771+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-13T00:00:00+00:00, run_after=2023-01-14T00:00:00+00:00[0m
[[34m2023-09-11T06:40:33.821+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:33.821+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:33.822+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:33.824+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:33.824+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:33.825+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:33.828+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:35.727+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:35.853+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:36.030+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:36.063+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:36.590+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:36.591+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:36.674+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:36.735+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:37.585+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:37.596+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:36.837129+00:00, run_end_date=2023-09-11 06:40:37.080420+00:00, run_duration=0.243291, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:33.822666+00:00, queued_by_job_id=2, pid=40855[0m
[[34m2023-09-11T06:40:37.891+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-14T00:00:00+00:00, run_after=2023-01-15T00:00:00+00:00[0m
[[34m2023-09-11T06:40:37.957+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-12 00:00:00+00:00: scheduled__2023-01-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:33.764241+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:37.958+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-12 00:00:00+00:00, run_id=scheduled__2023-01-12T00:00:00+00:00, run_start_date=2023-09-11 06:40:33.786292+00:00, run_end_date=2023-09-11 06:40:37.957898+00:00, run_duration=4.171606, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-12 00:00:00+00:00, data_interval_end=2023-01-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:37.962+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-13T00:00:00+00:00, run_after=2023-01-14T00:00:00+00:00[0m
[[34m2023-09-11T06:40:37.982+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:37.983+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:37.983+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:37.986+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:37.987+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:37.987+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:37.990+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:40.219+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:40.353+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:40.541+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:40.574+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:41.077+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:41.078+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:41.161+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:41.228+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:42.071+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:42.082+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:41.363874+00:00, run_end_date=2023-09-11 06:40:41.614110+00:00, run_duration=0.250236, state=success, executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:37.985040+00:00, queued_by_job_id=2, pid=40862[0m
[[34m2023-09-11T06:40:42.327+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-14T00:00:00+00:00, run_after=2023-01-15T00:00:00+00:00[0m
[[34m2023-09-11T06:40:42.349+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-13 00:00:00+00:00: scheduled__2023-01-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:37.882307+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:42.350+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-13 00:00:00+00:00, run_id=scheduled__2023-01-13T00:00:00+00:00, run_start_date=2023-09-11 06:40:37.921144+00:00, run_end_date=2023-09-11 06:40:42.350165+00:00, run_duration=4.429021, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-13 00:00:00+00:00, data_interval_end=2023-01-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:42.353+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-14T00:00:00+00:00, run_after=2023-01-15T00:00:00+00:00[0m
[[34m2023-09-11T06:40:42.859+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-15T00:00:00+00:00, run_after=2023-01-16T00:00:00+00:00[0m
[[34m2023-09-11T06:40:42.904+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:42.904+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:42.904+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:42.906+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:42.907+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:42.907+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:42.910+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:44.764+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:44.938+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:45.183+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:45.231+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:45.769+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:45.770+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:45.875+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:45.937+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:46.762+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:46.773+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:46.028528+00:00, run_end_date=2023-09-11 06:40:46.265201+00:00, run_duration=0.236673, state=success, executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:42.905336+00:00, queued_by_job_id=2, pid=40872[0m
[[34m2023-09-11T06:40:47.034+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-16T00:00:00+00:00, run_after=2023-01-17T00:00:00+00:00[0m
[[34m2023-09-11T06:40:47.071+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-14 00:00:00+00:00: scheduled__2023-01-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:42.854457+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:47.072+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-14 00:00:00+00:00, run_id=scheduled__2023-01-14T00:00:00+00:00, run_start_date=2023-09-11 06:40:42.872587+00:00, run_end_date=2023-09-11 06:40:47.071990+00:00, run_duration=4.199403, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-14 00:00:00+00:00, data_interval_end=2023-01-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:47.075+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-15T00:00:00+00:00, run_after=2023-01-16T00:00:00+00:00[0m
[[34m2023-09-11T06:40:47.090+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:47.090+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:47.091+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:47.093+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:47.093+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:47.094+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:47.097+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:49.027+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:49.168+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:49.351+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:49.383+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:49.859+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:49.860+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:49.937+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:49.996+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:50.727+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:50.737+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:50.085209+00:00, run_end_date=2023-09-11 06:40:50.303685+00:00, run_duration=0.218476, state=success, executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:47.091668+00:00, queued_by_job_id=2, pid=40879[0m
[[34m2023-09-11T06:40:51.092+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-16T00:00:00+00:00, run_after=2023-01-17T00:00:00+00:00[0m
[[34m2023-09-11T06:40:51.116+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-15 00:00:00+00:00: scheduled__2023-01-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:47.029557+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:51.117+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-15 00:00:00+00:00, run_id=scheduled__2023-01-15T00:00:00+00:00, run_start_date=2023-09-11 06:40:47.047694+00:00, run_end_date=2023-09-11 06:40:51.117133+00:00, run_duration=4.069439, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-15 00:00:00+00:00, data_interval_end=2023-01-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:51.120+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-16T00:00:00+00:00, run_after=2023-01-17T00:00:00+00:00[0m
[[34m2023-09-11T06:40:52.117+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-17T00:00:00+00:00, run_after=2023-01-18T00:00:00+00:00[0m
[[34m2023-09-11T06:40:52.160+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:52.161+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:52.161+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:52.163+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:52.164+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:52.164+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:52.167+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:54.380+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:54.531+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:54.729+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:54.763+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:55.308+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:55.309+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:55.383+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:55.444+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:40:56.244+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:40:56.255+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:40:55.532361+00:00, run_end_date=2023-09-11 06:40:55.773782+00:00, run_duration=0.241421, state=success, executor_state=success, try_number=1, max_tries=0, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:52.162310+00:00, queued_by_job_id=2, pid=40889[0m
[[34m2023-09-11T06:40:56.594+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-18T00:00:00+00:00, run_after=2023-01-19T00:00:00+00:00[0m
[[34m2023-09-11T06:40:56.637+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-16 00:00:00+00:00: scheduled__2023-01-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:52.112800+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:40:56.638+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-16 00:00:00+00:00, run_id=scheduled__2023-01-16T00:00:00+00:00, run_start_date=2023-09-11 06:40:52.129610+00:00, run_end_date=2023-09-11 06:40:56.638185+00:00, run_duration=4.508575, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-16 00:00:00+00:00, data_interval_end=2023-01-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:40:56.643+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-17T00:00:00+00:00, run_after=2023-01-18T00:00:00+00:00[0m
[[34m2023-09-11T06:40:56.659+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:56.660+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:40:56.660+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:40:56.662+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:40:56.663+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:40:56.663+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:56.666+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:40:58.875+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:40:59.037+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:59.221+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:59.258+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:40:59.806+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:40:59.807+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:40:59.923+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:40:59.990+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:00.805+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:00.816+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:00.086493+00:00, run_end_date=2023-09-11 06:41:00.327629+00:00, run_duration=0.241136, state=success, executor_state=success, try_number=1, max_tries=0, job_id=21, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:40:56.661219+00:00, queued_by_job_id=2, pid=40898[0m
[[34m2023-09-11T06:41:00.970+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-18T00:00:00+00:00, run_after=2023-01-19T00:00:00+00:00[0m
[[34m2023-09-11T06:41:00.998+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-17 00:00:00+00:00: scheduled__2023-01-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:40:56.587699+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:00.999+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-17 00:00:00+00:00, run_id=scheduled__2023-01-17T00:00:00+00:00, run_start_date=2023-09-11 06:40:56.609923+00:00, run_end_date=2023-09-11 06:41:00.999415+00:00, run_duration=4.389492, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-17 00:00:00+00:00, data_interval_end=2023-01-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:01.002+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-18T00:00:00+00:00, run_after=2023-01-19T00:00:00+00:00[0m
[[34m2023-09-11T06:41:01.600+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-19T00:00:00+00:00, run_after=2023-01-20T00:00:00+00:00[0m
[[34m2023-09-11T06:41:01.652+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:01.653+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:01.653+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:01.655+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:01.655+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:01.656+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:01.658+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:03.554+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:03.688+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:03.861+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:03.899+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:04.375+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:04.376+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:04.458+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:04.556+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:05.376+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:05.387+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:04.677219+00:00, run_end_date=2023-09-11 06:41:04.959552+00:00, run_duration=0.282333, state=success, executor_state=success, try_number=1, max_tries=0, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:01.654040+00:00, queued_by_job_id=2, pid=40908[0m
[[34m2023-09-11T06:41:05.663+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-20T00:00:00+00:00, run_after=2023-01-21T00:00:00+00:00[0m
[[34m2023-09-11T06:41:05.710+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-18 00:00:00+00:00: scheduled__2023-01-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:01.595195+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:05.711+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-18 00:00:00+00:00, run_id=scheduled__2023-01-18T00:00:00+00:00, run_start_date=2023-09-11 06:41:01.614044+00:00, run_end_date=2023-09-11 06:41:05.711324+00:00, run_duration=4.09728, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-18 00:00:00+00:00, data_interval_end=2023-01-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:05.718+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-19T00:00:00+00:00, run_after=2023-01-20T00:00:00+00:00[0m
[[34m2023-09-11T06:41:05.734+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:05.734+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:05.734+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:05.736+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:05.737+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:05.737+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:05.740+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:07.787+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:07.924+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:08.102+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:08.139+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:08.627+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:08.627+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:08.702+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:08.761+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:09.607+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:09.622+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:08.847537+00:00, run_end_date=2023-09-11 06:41:09.103358+00:00, run_duration=0.255821, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:05.735212+00:00, queued_by_job_id=2, pid=40917[0m
[[34m2023-09-11T06:41:09.778+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-20T00:00:00+00:00, run_after=2023-01-21T00:00:00+00:00[0m
[[34m2023-09-11T06:41:09.801+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-19 00:00:00+00:00: scheduled__2023-01-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:05.658376+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:09.801+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-19 00:00:00+00:00, run_id=scheduled__2023-01-19T00:00:00+00:00, run_start_date=2023-09-11 06:41:05.679056+00:00, run_end_date=2023-09-11 06:41:09.801776+00:00, run_duration=4.12272, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-19 00:00:00+00:00, data_interval_end=2023-01-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:09.805+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-20T00:00:00+00:00, run_after=2023-01-21T00:00:00+00:00[0m
[[34m2023-09-11T06:41:10.627+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-21T00:00:00+00:00, run_after=2023-01-22T00:00:00+00:00[0m
[[34m2023-09-11T06:41:10.686+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:10.687+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:10.687+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:10.690+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:10.691+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:10.691+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:10.694+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:13.896+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:14.151+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:14.624+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:14.813+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:15.748+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:15.750+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:15.879+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:15.996+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:16.930+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:16.941+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:16.096727+00:00, run_end_date=2023-09-11 06:41:16.471293+00:00, run_duration=0.374566, state=success, executor_state=success, try_number=1, max_tries=0, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:10.688292+00:00, queued_by_job_id=2, pid=40929[0m
[[34m2023-09-11T06:41:17.183+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-22T00:00:00+00:00, run_after=2023-01-23T00:00:00+00:00[0m
[[34m2023-09-11T06:41:17.224+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-20 00:00:00+00:00: scheduled__2023-01-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:10.623020+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:17.225+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-20 00:00:00+00:00, run_id=scheduled__2023-01-20T00:00:00+00:00, run_start_date=2023-09-11 06:41:10.643249+00:00, run_end_date=2023-09-11 06:41:17.224922+00:00, run_duration=6.581673, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-20 00:00:00+00:00, data_interval_end=2023-01-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:17.228+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-21T00:00:00+00:00, run_after=2023-01-22T00:00:00+00:00[0m
[[34m2023-09-11T06:41:17.244+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:17.244+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:17.244+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:17.249+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:17.250+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:17.250+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:17.252+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:19.885+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:20.062+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:20.278+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:20.321+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:21.530+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:21.532+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:21.633+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:21.698+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:22.751+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:22.762+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:21.799723+00:00, run_end_date=2023-09-11 06:41:22.050770+00:00, run_duration=0.251047, state=success, executor_state=success, try_number=1, max_tries=0, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:17.245608+00:00, queued_by_job_id=2, pid=40940[0m
[[34m2023-09-11T06:41:23.046+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-22T00:00:00+00:00, run_after=2023-01-23T00:00:00+00:00[0m
[[34m2023-09-11T06:41:23.075+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-21 00:00:00+00:00: scheduled__2023-01-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:17.178757+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:23.076+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-21 00:00:00+00:00, run_id=scheduled__2023-01-21T00:00:00+00:00, run_start_date=2023-09-11 06:41:17.197039+00:00, run_end_date=2023-09-11 06:41:23.076388+00:00, run_duration=5.879349, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-21 00:00:00+00:00, data_interval_end=2023-01-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:23.083+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-22T00:00:00+00:00, run_after=2023-01-23T00:00:00+00:00[0m
[[34m2023-09-11T06:41:24.359+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-23T00:00:00+00:00, run_after=2023-01-24T00:00:00+00:00[0m
[[34m2023-09-11T06:41:24.407+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:24.408+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:24.408+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:24.410+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:24.411+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:24.411+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:24.414+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:27.511+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:27.701+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:27.918+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:27.981+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:28.570+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:28.571+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:28.669+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:28.778+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:29.611+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:29.627+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:28.886801+00:00, run_end_date=2023-09-11 06:41:29.144110+00:00, run_duration=0.257309, state=success, executor_state=success, try_number=1, max_tries=0, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:24.409225+00:00, queued_by_job_id=2, pid=40950[0m
[[34m2023-09-11T06:41:29.907+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-24T00:00:00+00:00, run_after=2023-01-25T00:00:00+00:00[0m
[[34m2023-09-11T06:41:29.945+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-22 00:00:00+00:00: scheduled__2023-01-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:24.355111+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:29.945+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-22 00:00:00+00:00, run_id=scheduled__2023-01-22T00:00:00+00:00, run_start_date=2023-09-11 06:41:24.373918+00:00, run_end_date=2023-09-11 06:41:29.945858+00:00, run_duration=5.57194, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-22 00:00:00+00:00, data_interval_end=2023-01-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:29.949+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-23T00:00:00+00:00, run_after=2023-01-24T00:00:00+00:00[0m
[[34m2023-09-11T06:41:29.964+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:29.965+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:29.965+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:29.967+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:29.967+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:29.968+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:29.970+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:32.080+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:32.214+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:32.390+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:32.442+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:32.927+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:32.928+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:32.999+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:33.057+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:33.909+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:33.923+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:33.179532+00:00, run_end_date=2023-09-11 06:41:33.424112+00:00, run_duration=0.24458, state=success, executor_state=success, try_number=1, max_tries=0, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:29.966056+00:00, queued_by_job_id=2, pid=40959[0m
[[34m2023-09-11T06:41:34.217+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-24T00:00:00+00:00, run_after=2023-01-25T00:00:00+00:00[0m
[[34m2023-09-11T06:41:34.239+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-23 00:00:00+00:00: scheduled__2023-01-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:29.902803+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:34.240+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-23 00:00:00+00:00, run_id=scheduled__2023-01-23T00:00:00+00:00, run_start_date=2023-09-11 06:41:29.920133+00:00, run_end_date=2023-09-11 06:41:34.240345+00:00, run_duration=4.320212, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-23 00:00:00+00:00, data_interval_end=2023-01-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:34.243+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-24T00:00:00+00:00, run_after=2023-01-25T00:00:00+00:00[0m
[[34m2023-09-11T06:41:35.015+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-25T00:00:00+00:00, run_after=2023-01-26T00:00:00+00:00[0m
[[34m2023-09-11T06:41:35.064+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:35.065+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:35.065+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:35.069+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:35.070+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:35.071+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:35.074+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:38.099+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:38.360+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:38.737+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:38.809+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:39.851+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:39.852+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:39.945+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:40.145+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:42.077+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:42.089+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:40.512111+00:00, run_end_date=2023-09-11 06:41:41.228737+00:00, run_duration=0.716626, state=success, executor_state=success, try_number=1, max_tries=0, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:35.067432+00:00, queued_by_job_id=2, pid=40970[0m
[[34m2023-09-11T06:41:42.382+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-26T00:00:00+00:00, run_after=2023-01-27T00:00:00+00:00[0m
[[34m2023-09-11T06:41:42.416+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-24 00:00:00+00:00: scheduled__2023-01-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:34.997933+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:42.417+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-24 00:00:00+00:00, run_id=scheduled__2023-01-24T00:00:00+00:00, run_start_date=2023-09-11 06:41:35.027137+00:00, run_end_date=2023-09-11 06:41:42.417244+00:00, run_duration=7.390107, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-24 00:00:00+00:00, data_interval_end=2023-01-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:42.420+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-25T00:00:00+00:00, run_after=2023-01-26T00:00:00+00:00[0m
[[34m2023-09-11T06:41:42.442+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:42.442+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:42.442+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:42.445+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:42.445+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:42.445+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:42.448+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:44.706+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:44.993+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:45.341+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:45.386+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:46.188+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:46.189+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:46.385+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:46.458+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:47.424+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:47.437+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:46.557435+00:00, run_end_date=2023-09-11 06:41:46.929410+00:00, run_duration=0.371975, state=success, executor_state=success, try_number=1, max_tries=0, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:42.443606+00:00, queued_by_job_id=2, pid=40979[0m
[[34m2023-09-11T06:41:47.703+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-26T00:00:00+00:00, run_after=2023-01-27T00:00:00+00:00[0m
[[34m2023-09-11T06:41:47.742+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-25 00:00:00+00:00: scheduled__2023-01-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:42.377250+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:47.742+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-25 00:00:00+00:00, run_id=scheduled__2023-01-25T00:00:00+00:00, run_start_date=2023-09-11 06:41:42.394345+00:00, run_end_date=2023-09-11 06:41:47.742731+00:00, run_duration=5.348386, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-25 00:00:00+00:00, data_interval_end=2023-01-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:47.749+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-26T00:00:00+00:00, run_after=2023-01-27T00:00:00+00:00[0m
[[34m2023-09-11T06:41:49.017+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-27T00:00:00+00:00, run_after=2023-01-28T00:00:00+00:00[0m
[[34m2023-09-11T06:41:49.062+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:49.062+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:49.063+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:49.065+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:49.066+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:49.066+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:49.069+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:51.604+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:51.771+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:52.025+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:52.061+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:52.905+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:52.906+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:53.021+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:53.154+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:54.701+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:54.722+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:53.358728+00:00, run_end_date=2023-09-11 06:41:53.934015+00:00, run_duration=0.575287, state=success, executor_state=success, try_number=1, max_tries=0, job_id=30, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:49.063971+00:00, queued_by_job_id=2, pid=40991[0m
[[34m2023-09-11T06:41:55.073+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-28T00:00:00+00:00, run_after=2023-01-29T00:00:00+00:00[0m
[[34m2023-09-11T06:41:55.128+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-26 00:00:00+00:00: scheduled__2023-01-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:49.010833+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:55.129+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-26 00:00:00+00:00, run_id=scheduled__2023-01-26T00:00:00+00:00, run_start_date=2023-09-11 06:41:49.029645+00:00, run_end_date=2023-09-11 06:41:55.129060+00:00, run_duration=6.099415, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-26 00:00:00+00:00, data_interval_end=2023-01-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:55.135+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-27T00:00:00+00:00, run_after=2023-01-28T00:00:00+00:00[0m
[[34m2023-09-11T06:41:55.161+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:55.161+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:41:55.162+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:41:55.166+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:41:55.167+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:41:55.167+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:55.183+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:41:57.373+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:41:57.500+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:57.676+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:57.708+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:41:58.226+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:41:58.227+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:41:58.295+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:41:58.350+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:41:59.122+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:41:59.134+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:41:58.446104+00:00, run_end_date=2023-09-11 06:41:58.664908+00:00, run_duration=0.218804, state=success, executor_state=success, try_number=1, max_tries=0, job_id=31, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:41:55.163634+00:00, queued_by_job_id=2, pid=40999[0m
[[34m2023-09-11T06:41:59.382+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-28T00:00:00+00:00, run_after=2023-01-29T00:00:00+00:00[0m
[[34m2023-09-11T06:41:59.405+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-27 00:00:00+00:00: scheduled__2023-01-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:55.067059+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:41:59.406+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-27 00:00:00+00:00, run_id=scheduled__2023-01-27T00:00:00+00:00, run_start_date=2023-09-11 06:41:55.089089+00:00, run_end_date=2023-09-11 06:41:59.406421+00:00, run_duration=4.317332, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-27 00:00:00+00:00, data_interval_end=2023-01-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:41:59.409+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-28T00:00:00+00:00, run_after=2023-01-29T00:00:00+00:00[0m
[[34m2023-09-11T06:42:00.000+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-29T00:00:00+00:00, run_after=2023-01-30T00:00:00+00:00[0m
[[34m2023-09-11T06:42:00.051+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:00.051+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:00.051+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:00.053+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:00.054+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:00.054+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:00.057+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:01.884+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:02.015+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:02.223+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:02.255+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:02.714+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:02.715+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:02.783+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:02.839+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:03.587+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:03.598+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:02.928341+00:00, run_end_date=2023-09-11 06:42:03.142399+00:00, run_duration=0.214058, state=success, executor_state=success, try_number=1, max_tries=0, job_id=32, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:00.052514+00:00, queued_by_job_id=2, pid=41009[0m
[[34m2023-09-11T06:42:03.882+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-30T00:00:00+00:00, run_after=2023-01-31T00:00:00+00:00[0m
[[34m2023-09-11T06:42:03.919+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-28 00:00:00+00:00: scheduled__2023-01-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:41:59.994997+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:03.919+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-28 00:00:00+00:00, run_id=scheduled__2023-01-28T00:00:00+00:00, run_start_date=2023-09-11 06:42:00.018150+00:00, run_end_date=2023-09-11 06:42:03.919884+00:00, run_duration=3.901734, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-28 00:00:00+00:00, data_interval_end=2023-01-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:03.923+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-29T00:00:00+00:00, run_after=2023-01-30T00:00:00+00:00[0m
[[34m2023-09-11T06:42:03.939+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:03.939+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:03.940+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:03.942+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:03.942+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:03.943+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:03.946+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:05.878+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:06.020+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:06.197+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:06.230+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:06.706+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:06.707+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:06.780+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:06.841+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:07.551+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:07.562+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:06.925034+00:00, run_end_date=2023-09-11 06:42:07.144433+00:00, run_duration=0.219399, state=success, executor_state=success, try_number=1, max_tries=0, job_id=33, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:03.940814+00:00, queued_by_job_id=2, pid=41018[0m
[[34m2023-09-11T06:42:07.905+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-30T00:00:00+00:00, run_after=2023-01-31T00:00:00+00:00[0m
[[34m2023-09-11T06:42:07.930+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-29 00:00:00+00:00: scheduled__2023-01-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:03.877201+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:07.931+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-29 00:00:00+00:00, run_id=scheduled__2023-01-29T00:00:00+00:00, run_start_date=2023-09-11 06:42:03.895457+00:00, run_end_date=2023-09-11 06:42:07.931315+00:00, run_duration=4.035858, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-29 00:00:00+00:00, data_interval_end=2023-01-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:07.935+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-30T00:00:00+00:00, run_after=2023-01-31T00:00:00+00:00[0m
[[34m2023-09-11T06:42:08.956+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-31T00:00:00+00:00, run_after=2023-02-01T00:00:00+00:00[0m
[[34m2023-09-11T06:42:09.002+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:09.003+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:09.003+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:09.005+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:09.006+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:09.006+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:09.009+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:10.923+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:11.051+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:11.223+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:11.254+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:11.701+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:11.701+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:11.770+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:11.827+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:12.605+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:12.616+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:11.913963+00:00, run_end_date=2023-09-11 06:42:12.149309+00:00, run_duration=0.235346, state=success, executor_state=success, try_number=1, max_tries=0, job_id=34, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:09.004025+00:00, queued_by_job_id=2, pid=41028[0m
[[34m2023-09-11T06:42:12.953+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-01T00:00:00+00:00, run_after=2023-02-02T00:00:00+00:00[0m
[[34m2023-09-11T06:42:12.991+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-30 00:00:00+00:00: scheduled__2023-01-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:08.952142+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:12.991+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-30 00:00:00+00:00, run_id=scheduled__2023-01-30T00:00:00+00:00, run_start_date=2023-09-11 06:42:08.970005+00:00, run_end_date=2023-09-11 06:42:12.991710+00:00, run_duration=4.021705, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-30 00:00:00+00:00, data_interval_end=2023-01-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:12.995+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-01-31T00:00:00+00:00, run_after=2023-02-01T00:00:00+00:00[0m
[[34m2023-09-11T06:42:13.010+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:13.010+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:13.011+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-01-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:13.013+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:13.014+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:13.014+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:13.017+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-01-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:14.976+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:15.116+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:15.297+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:15.329+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:15.818+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:15.819+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:15.890+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-01-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:15.950+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-01-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:16.725+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-01-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:16.736+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-01-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:16.053204+00:00, run_end_date=2023-09-11 06:42:16.316770+00:00, run_duration=0.263566, state=success, executor_state=success, try_number=1, max_tries=0, job_id=35, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:13.011872+00:00, queued_by_job_id=2, pid=41035[0m
[[34m2023-09-11T06:42:16.941+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-01T00:00:00+00:00, run_after=2023-02-02T00:00:00+00:00[0m
[[34m2023-09-11T06:42:16.965+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-01-31 00:00:00+00:00: scheduled__2023-01-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:12.948874+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:16.965+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-01-31 00:00:00+00:00, run_id=scheduled__2023-01-31T00:00:00+00:00, run_start_date=2023-09-11 06:42:12.968102+00:00, run_end_date=2023-09-11 06:42:16.965847+00:00, run_duration=3.997745, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-01-31 00:00:00+00:00, data_interval_end=2023-02-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:16.969+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-01T00:00:00+00:00, run_after=2023-02-02T00:00:00+00:00[0m
[[34m2023-09-11T06:42:18.006+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-02T00:00:00+00:00, run_after=2023-02-03T00:00:00+00:00[0m
[[34m2023-09-11T06:42:18.048+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:18.049+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:18.049+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:18.051+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:18.052+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:18.052+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:18.055+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:20.003+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:20.134+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:20.316+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:20.349+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:20.890+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:20.890+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:20.961+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:21.020+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:21.867+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:21.878+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:21.105362+00:00, run_end_date=2023-09-11 06:42:21.361849+00:00, run_duration=0.256487, state=success, executor_state=success, try_number=1, max_tries=0, job_id=36, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:18.050281+00:00, queued_by_job_id=2, pid=41045[0m
[[34m2023-09-11T06:42:22.146+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-03T00:00:00+00:00, run_after=2023-02-04T00:00:00+00:00[0m
[[34m2023-09-11T06:42:22.192+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-01 00:00:00+00:00: scheduled__2023-02-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:18.002053+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:22.192+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-01 00:00:00+00:00, run_id=scheduled__2023-02-01T00:00:00+00:00, run_start_date=2023-09-11 06:42:18.018116+00:00, run_end_date=2023-09-11 06:42:22.192408+00:00, run_duration=4.174292, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-01 00:00:00+00:00, data_interval_end=2023-02-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:22.196+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-02T00:00:00+00:00, run_after=2023-02-03T00:00:00+00:00[0m
[[34m2023-09-11T06:42:22.211+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:22.211+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:22.212+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:22.214+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:22.215+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:22.215+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:22.218+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:24.219+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:24.349+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:24.518+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:24.553+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:25.029+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:25.029+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:25.105+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:25.172+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:25.901+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:25.912+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:25.263161+00:00, run_end_date=2023-09-11 06:42:25.489605+00:00, run_duration=0.226444, state=success, executor_state=success, try_number=1, max_tries=0, job_id=37, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:22.212924+00:00, queued_by_job_id=2, pid=41052[0m
[[34m2023-09-11T06:42:26.154+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-03T00:00:00+00:00, run_after=2023-02-04T00:00:00+00:00[0m
[[34m2023-09-11T06:42:26.177+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-02 00:00:00+00:00: scheduled__2023-02-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:22.141646+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:26.177+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-02 00:00:00+00:00, run_id=scheduled__2023-02-02T00:00:00+00:00, run_start_date=2023-09-11 06:42:22.167776+00:00, run_end_date=2023-09-11 06:42:26.177786+00:00, run_duration=4.01001, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-02 00:00:00+00:00, data_interval_end=2023-02-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:26.181+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-03T00:00:00+00:00, run_after=2023-02-04T00:00:00+00:00[0m
[[34m2023-09-11T06:42:27.145+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-04T00:00:00+00:00, run_after=2023-02-05T00:00:00+00:00[0m
[[34m2023-09-11T06:42:27.191+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:27.191+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:27.192+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:27.194+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:27.194+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:27.195+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:27.212+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:29.210+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:29.339+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:29.507+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:29.539+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:30.009+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:30.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:30.080+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:30.139+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:31.003+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:31.014+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:30.224526+00:00, run_end_date=2023-09-11 06:42:30.461543+00:00, run_duration=0.237017, state=success, executor_state=success, try_number=1, max_tries=0, job_id=38, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:27.192837+00:00, queued_by_job_id=2, pid=41064[0m
[[34m2023-09-11T06:42:31.273+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-05T00:00:00+00:00, run_after=2023-02-06T00:00:00+00:00[0m
[[34m2023-09-11T06:42:31.309+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-03 00:00:00+00:00: scheduled__2023-02-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:27.140335+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:31.309+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-03 00:00:00+00:00, run_id=scheduled__2023-02-03T00:00:00+00:00, run_start_date=2023-09-11 06:42:27.158627+00:00, run_end_date=2023-09-11 06:42:31.309853+00:00, run_duration=4.151226, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-03 00:00:00+00:00, data_interval_end=2023-02-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:31.313+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-04T00:00:00+00:00, run_after=2023-02-05T00:00:00+00:00[0m
[[34m2023-09-11T06:42:31.328+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:31.328+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:31.328+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:31.331+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:31.331+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:31.331+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:31.334+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:33.197+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:33.330+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:33.527+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:33.593+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:34.085+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:34.086+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:34.157+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:34.215+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:34.921+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:34.931+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:34.298812+00:00, run_end_date=2023-09-11 06:42:34.522586+00:00, run_duration=0.223774, state=success, executor_state=success, try_number=1, max_tries=0, job_id=39, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:31.329664+00:00, queued_by_job_id=2, pid=41073[0m
[[34m2023-09-11T06:42:35.180+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-05T00:00:00+00:00, run_after=2023-02-06T00:00:00+00:00[0m
[[34m2023-09-11T06:42:35.218+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-04 00:00:00+00:00: scheduled__2023-02-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:31.267921+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:35.218+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-04 00:00:00+00:00, run_id=scheduled__2023-02-04T00:00:00+00:00, run_start_date=2023-09-11 06:42:31.285914+00:00, run_end_date=2023-09-11 06:42:35.218707+00:00, run_duration=3.932793, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-04 00:00:00+00:00, data_interval_end=2023-02-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:35.222+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-05T00:00:00+00:00, run_after=2023-02-06T00:00:00+00:00[0m
[[34m2023-09-11T06:42:36.372+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-06T00:00:00+00:00, run_after=2023-02-07T00:00:00+00:00[0m
[[34m2023-09-11T06:42:36.417+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:36.417+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:36.417+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:36.419+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:36.420+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:36.420+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:36.422+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:38.341+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:38.473+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:38.657+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:38.690+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:39.154+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:39.155+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:39.259+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:39.357+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:40.102+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:40.113+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:39.450772+00:00, run_end_date=2023-09-11 06:42:39.691135+00:00, run_duration=0.240363, state=success, executor_state=success, try_number=1, max_tries=0, job_id=40, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:36.418466+00:00, queued_by_job_id=2, pid=41083[0m
[[34m2023-09-11T06:42:40.382+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-07T00:00:00+00:00, run_after=2023-02-08T00:00:00+00:00[0m
[[34m2023-09-11T06:42:40.418+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-05 00:00:00+00:00: scheduled__2023-02-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:36.368113+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:40.419+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-05 00:00:00+00:00, run_id=scheduled__2023-02-05T00:00:00+00:00, run_start_date=2023-09-11 06:42:36.386302+00:00, run_end_date=2023-09-11 06:42:40.419167+00:00, run_duration=4.032865, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-05 00:00:00+00:00, data_interval_end=2023-02-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:40.422+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-06T00:00:00+00:00, run_after=2023-02-07T00:00:00+00:00[0m
[[34m2023-09-11T06:42:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:40.438+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:40.439+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:40.441+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:40.441+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:40.442+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:40.444+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:42.274+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:42.409+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:42.580+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:42.613+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:43.072+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:43.073+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:43.143+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:43.202+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:43.957+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:43.968+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:43.287128+00:00, run_end_date=2023-09-11 06:42:43.527439+00:00, run_duration=0.240311, state=success, executor_state=success, try_number=1, max_tries=0, job_id=41, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:40.439818+00:00, queued_by_job_id=2, pid=41092[0m
[[34m2023-09-11T06:42:44.218+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-07T00:00:00+00:00, run_after=2023-02-08T00:00:00+00:00[0m
[[34m2023-09-11T06:42:44.241+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-06 00:00:00+00:00: scheduled__2023-02-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:40.377685+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:44.242+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-06 00:00:00+00:00, run_id=scheduled__2023-02-06T00:00:00+00:00, run_start_date=2023-09-11 06:42:40.395668+00:00, run_end_date=2023-09-11 06:42:44.242005+00:00, run_duration=3.846337, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-06 00:00:00+00:00, data_interval_end=2023-02-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:44.245+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-07T00:00:00+00:00, run_after=2023-02-08T00:00:00+00:00[0m
[[34m2023-09-11T06:42:45.186+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-08T00:00:00+00:00, run_after=2023-02-09T00:00:00+00:00[0m
[[34m2023-09-11T06:42:45.229+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:45.230+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:45.230+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:45.232+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:45.232+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:45.233+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:45.236+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:47.196+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:47.333+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:47.514+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:47.549+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:48.025+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:48.026+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:48.103+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:48.164+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:48.906+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:48.917+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:48.248876+00:00, run_end_date=2023-09-11 06:42:48.479508+00:00, run_duration=0.230632, state=success, executor_state=success, try_number=1, max_tries=0, job_id=42, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:45.230955+00:00, queued_by_job_id=2, pid=41102[0m
[[34m2023-09-11T06:42:49.090+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-09T00:00:00+00:00, run_after=2023-02-10T00:00:00+00:00[0m
[[34m2023-09-11T06:42:49.138+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-07 00:00:00+00:00: scheduled__2023-02-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:45.181935+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:49.138+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-07 00:00:00+00:00, run_id=scheduled__2023-02-07T00:00:00+00:00, run_start_date=2023-09-11 06:42:45.198417+00:00, run_end_date=2023-09-11 06:42:49.138814+00:00, run_duration=3.940397, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-07 00:00:00+00:00, data_interval_end=2023-02-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:49.142+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-08T00:00:00+00:00, run_after=2023-02-09T00:00:00+00:00[0m
[[34m2023-09-11T06:42:49.159+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:49.159+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:49.160+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:49.163+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:49.163+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:49.163+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:49.166+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:51.148+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:51.277+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:51.462+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:51.494+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:51.989+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:51.990+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:52.071+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:52.133+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:52.933+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:52.944+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:52.247340+00:00, run_end_date=2023-09-11 06:42:52.495153+00:00, run_duration=0.247813, state=success, executor_state=success, try_number=1, max_tries=0, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:49.161628+00:00, queued_by_job_id=2, pid=41111[0m
[[34m2023-09-11T06:42:53.301+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-09T00:00:00+00:00, run_after=2023-02-10T00:00:00+00:00[0m
[[34m2023-09-11T06:42:53.324+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-08 00:00:00+00:00: scheduled__2023-02-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:49.083771+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:53.325+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-08 00:00:00+00:00, run_id=scheduled__2023-02-08T00:00:00+00:00, run_start_date=2023-09-11 06:42:49.106755+00:00, run_end_date=2023-09-11 06:42:53.324931+00:00, run_duration=4.218176, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-08 00:00:00+00:00, data_interval_end=2023-02-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:53.328+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-09T00:00:00+00:00, run_after=2023-02-10T00:00:00+00:00[0m
[[34m2023-09-11T06:42:54.183+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-10T00:00:00+00:00, run_after=2023-02-11T00:00:00+00:00[0m
[[34m2023-09-11T06:42:54.226+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:54.226+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:54.227+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:54.229+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:54.230+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:54.230+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:54.233+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:56.193+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:42:56.323+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:56.497+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:56.530+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:42:57.020+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:42:57.021+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:42:57.092+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:42:57.165+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:42:57.891+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:42:57.902+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:42:57.255303+00:00, run_end_date=2023-09-11 06:42:57.473040+00:00, run_duration=0.217737, state=success, executor_state=success, try_number=1, max_tries=0, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:54.227750+00:00, queued_by_job_id=2, pid=41121[0m
[[34m2023-09-11T06:42:58.159+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-11T00:00:00+00:00, run_after=2023-02-12T00:00:00+00:00[0m
[[34m2023-09-11T06:42:58.198+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-09 00:00:00+00:00: scheduled__2023-02-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:54.178331+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:42:58.198+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-09 00:00:00+00:00, run_id=scheduled__2023-02-09T00:00:00+00:00, run_start_date=2023-09-11 06:42:54.194943+00:00, run_end_date=2023-09-11 06:42:58.198707+00:00, run_duration=4.003764, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-09 00:00:00+00:00, data_interval_end=2023-02-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:42:58.202+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-10T00:00:00+00:00, run_after=2023-02-11T00:00:00+00:00[0m
[[34m2023-09-11T06:42:58.219+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:58.219+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:42:58.220+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:42:58.222+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:42:58.222+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:42:58.223+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:42:58.225+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:00.149+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:00.287+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:00.474+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:00.508+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:00.989+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:00.989+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:01.059+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:01.122+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:01.830+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:01.840+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:01.208874+00:00, run_end_date=2023-09-11 06:43:01.435342+00:00, run_duration=0.226468, state=success, executor_state=success, try_number=1, max_tries=0, job_id=45, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:42:58.220747+00:00, queued_by_job_id=2, pid=41128[0m
[[34m2023-09-11T06:43:02.082+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-11T00:00:00+00:00, run_after=2023-02-12T00:00:00+00:00[0m
[[34m2023-09-11T06:43:02.105+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-10 00:00:00+00:00: scheduled__2023-02-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:42:58.154390+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:02.106+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-10 00:00:00+00:00, run_id=scheduled__2023-02-10T00:00:00+00:00, run_start_date=2023-09-11 06:42:58.171830+00:00, run_end_date=2023-09-11 06:43:02.106104+00:00, run_duration=3.934274, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-10 00:00:00+00:00, data_interval_end=2023-02-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:02.110+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-11T00:00:00+00:00, run_after=2023-02-12T00:00:00+00:00[0m
[[34m2023-09-11T06:43:03.165+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-12T00:00:00+00:00, run_after=2023-02-13T00:00:00+00:00[0m
[[34m2023-09-11T06:43:03.210+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:03.211+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:03.211+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:03.213+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:03.214+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:03.214+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:03.216+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:05.054+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:05.184+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:05.355+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:05.388+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:05.855+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:05.856+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:05.940+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:06.002+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:06.753+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:06.764+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:06.090282+00:00, run_end_date=2023-09-11 06:43:06.318182+00:00, run_duration=0.2279, state=success, executor_state=success, try_number=1, max_tries=0, job_id=46, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:03.212004+00:00, queued_by_job_id=2, pid=41138[0m
[[34m2023-09-11T06:43:06.919+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-13T00:00:00+00:00, run_after=2023-02-14T00:00:00+00:00[0m
[[34m2023-09-11T06:43:06.969+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-11 00:00:00+00:00: scheduled__2023-02-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:03.158711+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:06.969+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-11 00:00:00+00:00, run_id=scheduled__2023-02-11T00:00:00+00:00, run_start_date=2023-09-11 06:43:03.180002+00:00, run_end_date=2023-09-11 06:43:06.969797+00:00, run_duration=3.789795, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-11 00:00:00+00:00, data_interval_end=2023-02-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:06.973+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-12T00:00:00+00:00, run_after=2023-02-13T00:00:00+00:00[0m
[[34m2023-09-11T06:43:06.988+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:06.988+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:06.988+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:06.990+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:06.991+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:06.991+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:06.994+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:08.853+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:08.992+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:09.165+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:09.196+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:09.678+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:09.679+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:09.753+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:09.811+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:10.514+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:10.527+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:09.903075+00:00, run_end_date=2023-09-11 06:43:10.126387+00:00, run_duration=0.223312, state=success, executor_state=success, try_number=1, max_tries=0, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:06.989483+00:00, queued_by_job_id=2, pid=41146[0m
[[34m2023-09-11T06:43:10.772+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-13T00:00:00+00:00, run_after=2023-02-14T00:00:00+00:00[0m
[[34m2023-09-11T06:43:10.794+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-12 00:00:00+00:00: scheduled__2023-02-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:06.914723+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:10.795+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-12 00:00:00+00:00, run_id=scheduled__2023-02-12T00:00:00+00:00, run_start_date=2023-09-11 06:43:06.933504+00:00, run_end_date=2023-09-11 06:43:10.794887+00:00, run_duration=3.861383, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-12 00:00:00+00:00, data_interval_end=2023-02-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:10.798+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-13T00:00:00+00:00, run_after=2023-02-14T00:00:00+00:00[0m
[[34m2023-09-11T06:43:11.936+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-14T00:00:00+00:00, run_after=2023-02-15T00:00:00+00:00[0m
[[34m2023-09-11T06:43:11.980+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:11.980+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:11.981+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:11.983+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:11.983+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:11.984+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:11.986+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:14.101+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:14.276+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:14.467+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:14.504+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:15.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:15.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:15.087+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:15.149+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:15.893+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:15.903+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:15.234046+00:00, run_end_date=2023-09-11 06:43:15.473106+00:00, run_duration=0.23906, state=success, executor_state=success, try_number=1, max_tries=0, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:11.981895+00:00, queued_by_job_id=2, pid=41156[0m
[[34m2023-09-11T06:43:16.087+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-15T00:00:00+00:00, run_after=2023-02-16T00:00:00+00:00[0m
[[34m2023-09-11T06:43:16.125+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-13 00:00:00+00:00: scheduled__2023-02-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:11.931891+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:16.125+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-13 00:00:00+00:00, run_id=scheduled__2023-02-13T00:00:00+00:00, run_start_date=2023-09-11 06:43:11.949056+00:00, run_end_date=2023-09-11 06:43:16.125807+00:00, run_duration=4.176751, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-13 00:00:00+00:00, data_interval_end=2023-02-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:16.129+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-14T00:00:00+00:00, run_after=2023-02-15T00:00:00+00:00[0m
[[34m2023-09-11T06:43:16.144+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:16.145+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:16.145+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:16.147+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:16.147+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:16.147+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:16.150+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:18.285+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:18.455+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:18.674+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:18.706+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:19.170+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:19.171+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:19.243+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:19.307+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:20.027+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:20.037+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:19.400549+00:00, run_end_date=2023-09-11 06:43:19.619740+00:00, run_duration=0.219191, state=success, executor_state=success, try_number=1, max_tries=0, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:16.145901+00:00, queued_by_job_id=2, pid=41165[0m
[[34m2023-09-11T06:43:20.174+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-15T00:00:00+00:00, run_after=2023-02-16T00:00:00+00:00[0m
[[34m2023-09-11T06:43:20.197+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-14 00:00:00+00:00: scheduled__2023-02-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:16.083069+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:20.198+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-14 00:00:00+00:00, run_id=scheduled__2023-02-14T00:00:00+00:00, run_start_date=2023-09-11 06:43:16.100863+00:00, run_end_date=2023-09-11 06:43:20.198225+00:00, run_duration=4.097362, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-14 00:00:00+00:00, data_interval_end=2023-02-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:20.201+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-15T00:00:00+00:00, run_after=2023-02-16T00:00:00+00:00[0m
[[34m2023-09-11T06:43:21.083+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-16T00:00:00+00:00, run_after=2023-02-17T00:00:00+00:00[0m
[[34m2023-09-11T06:43:21.129+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:21.130+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:21.130+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:21.132+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:21.133+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:21.133+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:21.136+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:23.026+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:23.157+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:23.324+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:23.358+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:23.828+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:23.829+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:23.908+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:23.967+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:24.856+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:24.867+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:24.052237+00:00, run_end_date=2023-09-11 06:43:24.421215+00:00, run_duration=0.368978, state=success, executor_state=success, try_number=1, max_tries=0, job_id=50, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:21.131100+00:00, queued_by_job_id=2, pid=41175[0m
[[34m2023-09-11T06:43:25.125+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-17T00:00:00+00:00, run_after=2023-02-18T00:00:00+00:00[0m
[[34m2023-09-11T06:43:25.179+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-15 00:00:00+00:00: scheduled__2023-02-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:21.078556+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:25.180+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-15 00:00:00+00:00, run_id=scheduled__2023-02-15T00:00:00+00:00, run_start_date=2023-09-11 06:43:21.095678+00:00, run_end_date=2023-09-11 06:43:25.180247+00:00, run_duration=4.084569, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-15 00:00:00+00:00, data_interval_end=2023-02-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:25.183+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-16T00:00:00+00:00, run_after=2023-02-17T00:00:00+00:00[0m
[[34m2023-09-11T06:43:25.198+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:25.198+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:25.198+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:25.201+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:25.201+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:25.201+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:25.204+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:27.169+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:27.311+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:27.531+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:27.567+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:28.040+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:28.041+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:28.116+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:28.185+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:28.933+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:28.944+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:28.270249+00:00, run_end_date=2023-09-11 06:43:28.513418+00:00, run_duration=0.243169, state=success, executor_state=success, try_number=1, max_tries=0, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:25.199573+00:00, queued_by_job_id=2, pid=41184[0m
[[34m2023-09-11T06:43:29.197+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-17T00:00:00+00:00, run_after=2023-02-18T00:00:00+00:00[0m
[[34m2023-09-11T06:43:29.218+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-16 00:00:00+00:00: scheduled__2023-02-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:25.120323+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:29.219+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-16 00:00:00+00:00, run_id=scheduled__2023-02-16T00:00:00+00:00, run_start_date=2023-09-11 06:43:25.157344+00:00, run_end_date=2023-09-11 06:43:29.219219+00:00, run_duration=4.061875, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-16 00:00:00+00:00, data_interval_end=2023-02-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:29.222+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-17T00:00:00+00:00, run_after=2023-02-18T00:00:00+00:00[0m
[[34m2023-09-11T06:43:30.125+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-18T00:00:00+00:00, run_after=2023-02-19T00:00:00+00:00[0m
[[34m2023-09-11T06:43:30.169+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:30.170+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:30.170+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:30.172+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:30.173+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:30.173+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:30.177+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:32.258+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:32.430+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:32.659+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:32.701+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:33.452+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:33.453+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:33.530+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:33.592+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:34.333+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:34.345+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:33.705298+00:00, run_end_date=2023-09-11 06:43:33.936476+00:00, run_duration=0.231178, state=success, executor_state=success, try_number=1, max_tries=0, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:30.171053+00:00, queued_by_job_id=2, pid=41195[0m
[[34m2023-09-11T06:43:34.721+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-19T00:00:00+00:00, run_after=2023-02-20T00:00:00+00:00[0m
[[34m2023-09-11T06:43:34.757+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-17 00:00:00+00:00: scheduled__2023-02-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:30.120756+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:34.757+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-17 00:00:00+00:00, run_id=scheduled__2023-02-17T00:00:00+00:00, run_start_date=2023-09-11 06:43:30.136956+00:00, run_end_date=2023-09-11 06:43:34.757646+00:00, run_duration=4.62069, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-17 00:00:00+00:00, data_interval_end=2023-02-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:34.761+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-18T00:00:00+00:00, run_after=2023-02-19T00:00:00+00:00[0m
[[34m2023-09-11T06:43:34.778+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:34.778+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:34.778+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:34.781+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:34.782+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:34.782+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:34.784+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:37.270+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:37.416+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:37.624+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:37.706+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:38.282+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:38.283+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:38.357+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:38.420+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:39.200+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:39.212+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:38.509890+00:00, run_end_date=2023-09-11 06:43:38.750638+00:00, run_duration=0.240748, state=success, executor_state=success, try_number=1, max_tries=0, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:34.779800+00:00, queued_by_job_id=2, pid=41204[0m
[[34m2023-09-11T06:43:39.489+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-19T00:00:00+00:00, run_after=2023-02-20T00:00:00+00:00[0m
[[34m2023-09-11T06:43:39.515+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-18 00:00:00+00:00: scheduled__2023-02-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:34.717087+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:39.515+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-18 00:00:00+00:00, run_id=scheduled__2023-02-18T00:00:00+00:00, run_start_date=2023-09-11 06:43:34.734103+00:00, run_end_date=2023-09-11 06:43:39.515734+00:00, run_duration=4.781631, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-18 00:00:00+00:00, data_interval_end=2023-02-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:39.519+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-19T00:00:00+00:00, run_after=2023-02-20T00:00:00+00:00[0m
[[34m2023-09-11T06:43:40.790+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-20T00:00:00+00:00, run_after=2023-02-21T00:00:00+00:00[0m
[[34m2023-09-11T06:43:40.837+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:40.837+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:40.837+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:40.839+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:40.840+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:40.840+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:40.843+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:42.822+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:42.963+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:43.147+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:43.181+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:43.681+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:43.682+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:43.764+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:43.826+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:44.601+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:44.613+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:43.926206+00:00, run_end_date=2023-09-11 06:43:44.162456+00:00, run_duration=0.23625, state=success, executor_state=success, try_number=1, max_tries=0, job_id=54, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:40.838412+00:00, queued_by_job_id=2, pid=41214[0m
[[34m2023-09-11T06:43:44.952+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-21T00:00:00+00:00, run_after=2023-02-22T00:00:00+00:00[0m
[[34m2023-09-11T06:43:44.987+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-19 00:00:00+00:00: scheduled__2023-02-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:40.784475+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:44.987+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-19 00:00:00+00:00, run_id=scheduled__2023-02-19T00:00:00+00:00, run_start_date=2023-09-11 06:43:40.802792+00:00, run_end_date=2023-09-11 06:43:44.987779+00:00, run_duration=4.184987, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-19 00:00:00+00:00, data_interval_end=2023-02-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:44.991+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-20T00:00:00+00:00, run_after=2023-02-21T00:00:00+00:00[0m
[[34m2023-09-11T06:43:45.005+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:45.006+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:45.006+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:45.008+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:45.009+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:45.009+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:45.012+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:46.886+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:47.012+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:47.179+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:47.210+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:47.670+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:47.670+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:47.740+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:47.798+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:48.539+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:48.549+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:47.883377+00:00, run_end_date=2023-09-11 06:43:48.118864+00:00, run_duration=0.235487, state=success, executor_state=success, try_number=1, max_tries=0, job_id=55, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:45.007177+00:00, queued_by_job_id=2, pid=41223[0m
[[34m2023-09-11T06:43:48.920+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-21T00:00:00+00:00, run_after=2023-02-22T00:00:00+00:00[0m
[[34m2023-09-11T06:43:48.948+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-20 00:00:00+00:00: scheduled__2023-02-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:44.947518+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:48.948+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-20 00:00:00+00:00, run_id=scheduled__2023-02-20T00:00:00+00:00, run_start_date=2023-09-11 06:43:44.965088+00:00, run_end_date=2023-09-11 06:43:48.948512+00:00, run_duration=3.983424, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-20 00:00:00+00:00, data_interval_end=2023-02-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:48.952+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-21T00:00:00+00:00, run_after=2023-02-22T00:00:00+00:00[0m
[[34m2023-09-11T06:43:49.595+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-22T00:00:00+00:00, run_after=2023-02-23T00:00:00+00:00[0m
[[34m2023-09-11T06:43:49.656+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:49.657+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:49.657+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:49.661+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:49.662+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:49.662+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:49.665+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:51.651+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:51.795+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:51.982+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:52.018+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:52.515+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:52.515+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:52.597+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:52.659+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:53.384+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:53.396+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:52.747990+00:00, run_end_date=2023-09-11 06:43:52.977066+00:00, run_duration=0.229076, state=success, executor_state=success, try_number=1, max_tries=0, job_id=56, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:49.658613+00:00, queued_by_job_id=2, pid=41233[0m
[[34m2023-09-11T06:43:53.717+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-23T00:00:00+00:00, run_after=2023-02-24T00:00:00+00:00[0m
[[34m2023-09-11T06:43:53.755+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-21 00:00:00+00:00: scheduled__2023-02-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:49.586777+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:53.756+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-21 00:00:00+00:00, run_id=scheduled__2023-02-21T00:00:00+00:00, run_start_date=2023-09-11 06:43:49.614001+00:00, run_end_date=2023-09-11 06:43:53.756433+00:00, run_duration=4.142432, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-21 00:00:00+00:00, data_interval_end=2023-02-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:53.760+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-22T00:00:00+00:00, run_after=2023-02-23T00:00:00+00:00[0m
[[34m2023-09-11T06:43:53.776+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:53.776+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:53.777+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:53.779+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:53.779+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:53.780+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:53.783+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:55.848+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:43:55.986+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:56.175+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:56.208+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:43:56.695+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:43:56.695+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:43:56.768+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:43:56.826+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:43:57.631+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:43:57.642+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:43:56.914459+00:00, run_end_date=2023-09-11 06:43:57.160494+00:00, run_duration=0.246035, state=success, executor_state=success, try_number=1, max_tries=0, job_id=57, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:53.777868+00:00, queued_by_job_id=2, pid=41241[0m
[[34m2023-09-11T06:43:57.989+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-23T00:00:00+00:00, run_after=2023-02-24T00:00:00+00:00[0m
[[34m2023-09-11T06:43:58.014+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-22 00:00:00+00:00: scheduled__2023-02-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:53.712868+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:43:58.015+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-22 00:00:00+00:00, run_id=scheduled__2023-02-22T00:00:00+00:00, run_start_date=2023-09-11 06:43:53.731388+00:00, run_end_date=2023-09-11 06:43:58.014990+00:00, run_duration=4.283602, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-22 00:00:00+00:00, data_interval_end=2023-02-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:43:58.018+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-23T00:00:00+00:00, run_after=2023-02-24T00:00:00+00:00[0m
[[34m2023-09-11T06:43:58.723+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-24T00:00:00+00:00, run_after=2023-02-25T00:00:00+00:00[0m
[[34m2023-09-11T06:43:58.771+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:58.771+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:43:58.771+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:43:58.774+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:43:58.775+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:43:58.775+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:43:58.778+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:00.770+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:00.901+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:01.079+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:01.116+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:01.604+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:01.604+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:01.679+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:01.739+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:02.464+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:02.475+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:01.833431+00:00, run_end_date=2023-09-11 06:44:02.058760+00:00, run_duration=0.225329, state=success, executor_state=success, try_number=1, max_tries=0, job_id=58, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:43:58.772709+00:00, queued_by_job_id=2, pid=41251[0m
[[34m2023-09-11T06:44:02.854+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-25T00:00:00+00:00, run_after=2023-02-26T00:00:00+00:00[0m
[[34m2023-09-11T06:44:02.893+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-23 00:00:00+00:00: scheduled__2023-02-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:43:58.718777+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:02.894+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-23 00:00:00+00:00, run_id=scheduled__2023-02-23T00:00:00+00:00, run_start_date=2023-09-11 06:43:58.737093+00:00, run_end_date=2023-09-11 06:44:02.894107+00:00, run_duration=4.157014, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-23 00:00:00+00:00, data_interval_end=2023-02-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:02.897+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-24T00:00:00+00:00, run_after=2023-02-25T00:00:00+00:00[0m
[[34m2023-09-11T06:44:02.912+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:02.913+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:02.913+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:02.915+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:02.915+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:02.916+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:02.918+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:05.122+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:05.255+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:05.435+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:05.470+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:05.985+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:05.985+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:06.064+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:06.130+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:06.873+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:06.884+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:06.224407+00:00, run_end_date=2023-09-11 06:44:06.460022+00:00, run_duration=0.235615, state=success, executor_state=success, try_number=1, max_tries=0, job_id=59, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:02.913982+00:00, queued_by_job_id=2, pid=41258[0m
[[34m2023-09-11T06:44:07.133+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-25T00:00:00+00:00, run_after=2023-02-26T00:00:00+00:00[0m
[[34m2023-09-11T06:44:07.157+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-24 00:00:00+00:00: scheduled__2023-02-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:02.849127+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:07.157+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-24 00:00:00+00:00, run_id=scheduled__2023-02-24T00:00:00+00:00, run_start_date=2023-09-11 06:44:02.867174+00:00, run_end_date=2023-09-11 06:44:07.157709+00:00, run_duration=4.290535, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-24 00:00:00+00:00, data_interval_end=2023-02-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:07.161+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-25T00:00:00+00:00, run_after=2023-02-26T00:00:00+00:00[0m
[[34m2023-09-11T06:44:07.904+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-26T00:00:00+00:00, run_after=2023-02-27T00:00:00+00:00[0m
[[34m2023-09-11T06:44:07.951+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:07.951+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:07.951+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:07.953+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:07.954+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:07.955+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:07.957+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:09.914+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:10.051+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:10.247+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:10.282+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:10.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:10.775+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:10.851+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:10.913+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:11.691+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:11.702+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:11.004601+00:00, run_end_date=2023-09-11 06:44:11.273765+00:00, run_duration=0.269164, state=success, executor_state=success, try_number=1, max_tries=0, job_id=60, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:07.952358+00:00, queued_by_job_id=2, pid=41268[0m
[[34m2023-09-11T06:44:11.957+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-27T00:00:00+00:00, run_after=2023-02-28T00:00:00+00:00[0m
[[34m2023-09-11T06:44:11.994+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-25 00:00:00+00:00: scheduled__2023-02-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:07.899652+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:11.995+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-25 00:00:00+00:00, run_id=scheduled__2023-02-25T00:00:00+00:00, run_start_date=2023-09-11 06:44:07.918048+00:00, run_end_date=2023-09-11 06:44:11.995303+00:00, run_duration=4.077255, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-25 00:00:00+00:00, data_interval_end=2023-02-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:11.998+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-26T00:00:00+00:00, run_after=2023-02-27T00:00:00+00:00[0m
[[34m2023-09-11T06:44:12.019+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:12.020+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:12.020+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:12.023+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:12.025+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:12.025+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:12.029+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:14.016+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:14.157+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:14.344+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:14.380+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:14.879+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:14.879+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:14.954+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:15.014+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:15.758+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:15.769+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:15.102756+00:00, run_end_date=2023-09-11 06:44:15.337869+00:00, run_duration=0.235113, state=success, executor_state=success, try_number=1, max_tries=0, job_id=61, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:12.021654+00:00, queued_by_job_id=2, pid=41277[0m
[[34m2023-09-11T06:44:16.015+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-27T00:00:00+00:00, run_after=2023-02-28T00:00:00+00:00[0m
[[34m2023-09-11T06:44:16.041+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-26 00:00:00+00:00: scheduled__2023-02-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:11.951328+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:16.041+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-26 00:00:00+00:00, run_id=scheduled__2023-02-26T00:00:00+00:00, run_start_date=2023-09-11 06:44:11.969377+00:00, run_end_date=2023-09-11 06:44:16.041406+00:00, run_duration=4.072029, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-26 00:00:00+00:00, data_interval_end=2023-02-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:16.045+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-27T00:00:00+00:00, run_after=2023-02-28T00:00:00+00:00[0m
[[34m2023-09-11T06:44:17.061+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-28T00:00:00+00:00, run_after=2023-03-01T00:00:00+00:00[0m
[[34m2023-09-11T06:44:17.107+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:17.107+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:17.108+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:17.110+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:17.111+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:17.111+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:17.114+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:19.011+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:19.145+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:19.320+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:19.355+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:19.844+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:19.845+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:19.921+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:19.980+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:20.710+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:20.721+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:20.081163+00:00, run_end_date=2023-09-11 06:44:20.305638+00:00, run_duration=0.224475, state=success, executor_state=success, try_number=1, max_tries=0, job_id=62, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:17.108990+00:00, queued_by_job_id=2, pid=41288[0m
[[34m2023-09-11T06:44:20.998+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-01T00:00:00+00:00, run_after=2023-03-02T00:00:00+00:00[0m
[[34m2023-09-11T06:44:21.035+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-27 00:00:00+00:00: scheduled__2023-02-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:17.056518+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:21.036+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-27 00:00:00+00:00, run_id=scheduled__2023-02-27T00:00:00+00:00, run_start_date=2023-09-11 06:44:17.073053+00:00, run_end_date=2023-09-11 06:44:21.035994+00:00, run_duration=3.962941, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-27 00:00:00+00:00, data_interval_end=2023-02-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:21.040+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-02-28T00:00:00+00:00, run_after=2023-03-01T00:00:00+00:00[0m
[[34m2023-09-11T06:44:21.055+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:21.055+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:21.055+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-02-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:21.058+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:21.058+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:21.059+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:21.061+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-02-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:22.947+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:23.079+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:23.263+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:23.300+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:23.800+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:23.801+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:23.874+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-02-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:23.935+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-02-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:24.676+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-02-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:24.687+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-02-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:24.026863+00:00, run_end_date=2023-09-11 06:44:24.256542+00:00, run_duration=0.229679, state=success, executor_state=success, try_number=1, max_tries=0, job_id=63, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:21.056559+00:00, queued_by_job_id=2, pid=41297[0m
[[34m2023-09-11T06:44:24.924+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-01T00:00:00+00:00, run_after=2023-03-02T00:00:00+00:00[0m
[[34m2023-09-11T06:44:24.949+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-02-28 00:00:00+00:00: scheduled__2023-02-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:20.993710+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:24.950+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-02-28 00:00:00+00:00, run_id=scheduled__2023-02-28T00:00:00+00:00, run_start_date=2023-09-11 06:44:21.011906+00:00, run_end_date=2023-09-11 06:44:24.949938+00:00, run_duration=3.938032, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-02-28 00:00:00+00:00, data_interval_end=2023-03-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:24.953+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-01T00:00:00+00:00, run_after=2023-03-02T00:00:00+00:00[0m
[[34m2023-09-11T06:44:26.055+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-02T00:00:00+00:00, run_after=2023-03-03T00:00:00+00:00[0m
[[34m2023-09-11T06:44:26.100+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:26.100+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:26.101+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:26.103+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:26.104+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:26.104+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:26.107+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:28.061+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:28.202+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:28.381+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:28.415+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:28.895+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:28.895+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:28.970+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:29.030+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:29.751+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:29.762+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:29.120449+00:00, run_end_date=2023-09-11 06:44:29.348920+00:00, run_duration=0.228471, state=success, executor_state=success, try_number=1, max_tries=0, job_id=64, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:26.101873+00:00, queued_by_job_id=2, pid=41307[0m
[[34m2023-09-11T06:44:30.025+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-03T00:00:00+00:00, run_after=2023-03-04T00:00:00+00:00[0m
[[34m2023-09-11T06:44:30.059+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-01 00:00:00+00:00: scheduled__2023-03-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:26.050381+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:30.060+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-01 00:00:00+00:00, run_id=scheduled__2023-03-01T00:00:00+00:00, run_start_date=2023-09-11 06:44:26.068331+00:00, run_end_date=2023-09-11 06:44:30.060425+00:00, run_duration=3.992094, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-01 00:00:00+00:00, data_interval_end=2023-03-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:30.063+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-02T00:00:00+00:00, run_after=2023-03-03T00:00:00+00:00[0m
[[34m2023-09-11T06:44:30.080+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:30.080+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:30.080+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:30.082+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:30.083+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:30.083+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:30.086+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:32.024+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:32.180+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:32.391+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:32.429+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:32.926+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:32.927+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:33.002+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:33.064+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:33.802+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:33.813+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:33.159265+00:00, run_end_date=2023-09-11 06:44:33.389438+00:00, run_duration=0.230173, state=success, executor_state=success, try_number=1, max_tries=0, job_id=65, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:30.081369+00:00, queued_by_job_id=2, pid=41316[0m
[[34m2023-09-11T06:44:34.063+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-03T00:00:00+00:00, run_after=2023-03-04T00:00:00+00:00[0m
[[34m2023-09-11T06:44:34.088+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-02 00:00:00+00:00: scheduled__2023-03-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:30.019756+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:34.089+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-02 00:00:00+00:00, run_id=scheduled__2023-03-02T00:00:00+00:00, run_start_date=2023-09-11 06:44:30.037004+00:00, run_end_date=2023-09-11 06:44:34.089213+00:00, run_duration=4.052209, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-02 00:00:00+00:00, data_interval_end=2023-03-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:34.093+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-03T00:00:00+00:00, run_after=2023-03-04T00:00:00+00:00[0m
[[34m2023-09-11T06:44:35.020+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-04T00:00:00+00:00, run_after=2023-03-05T00:00:00+00:00[0m
[[34m2023-09-11T06:44:35.064+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:35.064+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:35.064+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:35.066+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:35.067+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:35.067+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:35.070+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:36.933+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:37.064+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:37.240+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:37.273+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:37.742+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:37.743+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:37.812+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:37.870+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:38.583+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:38.594+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:37.959929+00:00, run_end_date=2023-09-11 06:44:38.176959+00:00, run_duration=0.21703, state=success, executor_state=success, try_number=1, max_tries=0, job_id=66, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:35.065477+00:00, queued_by_job_id=2, pid=41326[0m
[[34m2023-09-11T06:44:38.854+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-05T00:00:00+00:00, run_after=2023-03-06T00:00:00+00:00[0m
[[34m2023-09-11T06:44:38.898+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-03 00:00:00+00:00: scheduled__2023-03-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:35.015712+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:38.899+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-03 00:00:00+00:00, run_id=scheduled__2023-03-03T00:00:00+00:00, run_start_date=2023-09-11 06:44:35.032079+00:00, run_end_date=2023-09-11 06:44:38.899137+00:00, run_duration=3.867058, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-03 00:00:00+00:00, data_interval_end=2023-03-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:38.902+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-04T00:00:00+00:00, run_after=2023-03-05T00:00:00+00:00[0m
[[34m2023-09-11T06:44:38.917+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:38.918+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:38.918+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:38.920+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:38.921+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:38.921+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:38.924+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:40.810+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:40.947+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:41.129+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:41.163+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:41.655+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:41.656+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:41.729+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:41.790+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:42.528+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:42.539+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:41.880312+00:00, run_end_date=2023-09-11 06:44:42.125643+00:00, run_duration=0.245331, state=success, executor_state=success, try_number=1, max_tries=0, job_id=67, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:38.918990+00:00, queued_by_job_id=2, pid=41333[0m
[[34m2023-09-11T06:44:42.779+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-05T00:00:00+00:00, run_after=2023-03-06T00:00:00+00:00[0m
[[34m2023-09-11T06:44:42.804+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-04 00:00:00+00:00: scheduled__2023-03-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:38.850011+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:42.805+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-04 00:00:00+00:00, run_id=scheduled__2023-03-04T00:00:00+00:00, run_start_date=2023-09-11 06:44:38.866325+00:00, run_end_date=2023-09-11 06:44:42.805560+00:00, run_duration=3.939235, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-04 00:00:00+00:00, data_interval_end=2023-03-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:42.810+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-05T00:00:00+00:00, run_after=2023-03-06T00:00:00+00:00[0m
[[34m2023-09-11T06:44:43.849+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-06T00:00:00+00:00, run_after=2023-03-07T00:00:00+00:00[0m
[[34m2023-09-11T06:44:43.895+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:43.895+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:43.895+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:43.898+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:43.898+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:43.898+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:43.901+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:45.794+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:45.930+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:46.108+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:46.149+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:46.642+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:46.642+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:46.740+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:46.814+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:47.528+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:47.539+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:46.902050+00:00, run_end_date=2023-09-11 06:44:47.130287+00:00, run_duration=0.228237, state=success, executor_state=success, try_number=1, max_tries=0, job_id=68, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:43.896689+00:00, queued_by_job_id=2, pid=41343[0m
[[34m2023-09-11T06:44:47.804+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-07T00:00:00+00:00, run_after=2023-03-08T00:00:00+00:00[0m
[[34m2023-09-11T06:44:47.840+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-05 00:00:00+00:00: scheduled__2023-03-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:43.845119+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:47.841+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-05 00:00:00+00:00, run_id=scheduled__2023-03-05T00:00:00+00:00, run_start_date=2023-09-11 06:44:43.862372+00:00, run_end_date=2023-09-11 06:44:47.840926+00:00, run_duration=3.978554, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-05 00:00:00+00:00, data_interval_end=2023-03-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:47.844+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-06T00:00:00+00:00, run_after=2023-03-07T00:00:00+00:00[0m
[[34m2023-09-11T06:44:47.859+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:47.860+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:47.860+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:47.862+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:47.863+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:47.863+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:47.879+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:49.770+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:49.904+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:50.082+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:50.116+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:50.590+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:50.591+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:50.664+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:50.723+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:51.462+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:51.474+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:50.812880+00:00, run_end_date=2023-09-11 06:44:51.046731+00:00, run_duration=0.233851, state=success, executor_state=success, try_number=1, max_tries=0, job_id=69, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:47.861385+00:00, queued_by_job_id=2, pid=41350[0m
[[34m2023-09-11T06:44:51.729+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-07T00:00:00+00:00, run_after=2023-03-08T00:00:00+00:00[0m
[[34m2023-09-11T06:44:51.755+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-06 00:00:00+00:00: scheduled__2023-03-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:47.799151+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:51.756+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-06 00:00:00+00:00, run_id=scheduled__2023-03-06T00:00:00+00:00, run_start_date=2023-09-11 06:44:47.817105+00:00, run_end_date=2023-09-11 06:44:51.756274+00:00, run_duration=3.939169, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-06 00:00:00+00:00, data_interval_end=2023-03-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:51.761+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-07T00:00:00+00:00, run_after=2023-03-08T00:00:00+00:00[0m
[[34m2023-09-11T06:44:52.800+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-08T00:00:00+00:00, run_after=2023-03-09T00:00:00+00:00[0m
[[34m2023-09-11T06:44:52.845+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:52.845+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:52.845+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:52.847+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:52.848+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:52.848+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:52.851+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:54.765+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:54.898+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:55.107+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:55.144+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:55.675+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:55.676+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:55.750+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:55.813+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:44:56.593+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:44:56.604+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:44:55.907753+00:00, run_end_date=2023-09-11 06:44:56.180103+00:00, run_duration=0.27235, state=success, executor_state=success, try_number=1, max_tries=0, job_id=70, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:52.846511+00:00, queued_by_job_id=2, pid=41361[0m
[[34m2023-09-11T06:44:56.869+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-09T00:00:00+00:00, run_after=2023-03-10T00:00:00+00:00[0m
[[34m2023-09-11T06:44:56.905+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-07 00:00:00+00:00: scheduled__2023-03-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:52.795832+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:44:56.905+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-07 00:00:00+00:00, run_id=scheduled__2023-03-07T00:00:00+00:00, run_start_date=2023-09-11 06:44:52.813089+00:00, run_end_date=2023-09-11 06:44:56.905467+00:00, run_duration=4.092378, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-07 00:00:00+00:00, data_interval_end=2023-03-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:44:56.909+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-08T00:00:00+00:00, run_after=2023-03-09T00:00:00+00:00[0m
[[34m2023-09-11T06:44:56.924+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:56.924+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:44:56.925+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:44:56.927+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:44:56.927+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:44:56.927+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:56.930+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:44:58.942+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:44:59.074+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:59.255+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:59.288+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:44:59.772+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:44:59.773+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:44:59.848+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:44:59.921+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:00.645+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:00.662+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:00.014981+00:00, run_end_date=2023-09-11 06:45:00.244499+00:00, run_duration=0.229518, state=success, executor_state=success, try_number=1, max_tries=0, job_id=71, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:44:56.925792+00:00, queued_by_job_id=2, pid=41370[0m
[[34m2023-09-11T06:45:00.679+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:45:00.812+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-09T00:00:00+00:00, run_after=2023-03-10T00:00:00+00:00[0m
[[34m2023-09-11T06:45:00.835+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-08 00:00:00+00:00: scheduled__2023-03-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:44:56.864443+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:00.835+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-08 00:00:00+00:00, run_id=scheduled__2023-03-08T00:00:00+00:00, run_start_date=2023-09-11 06:44:56.882951+00:00, run_end_date=2023-09-11 06:45:00.835621+00:00, run_duration=3.95267, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-08 00:00:00+00:00, data_interval_end=2023-03-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:00.839+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-09T00:00:00+00:00, run_after=2023-03-10T00:00:00+00:00[0m
[[34m2023-09-11T06:45:01.878+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-10T00:00:00+00:00, run_after=2023-03-11T00:00:00+00:00[0m
[[34m2023-09-11T06:45:01.928+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:01.929+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:01.929+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:01.931+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:01.932+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:01.932+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:01.935+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:03.919+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:04.050+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:04.225+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:04.257+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:04.781+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:04.781+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:04.852+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:04.916+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:05.662+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:05.672+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:05.001782+00:00, run_end_date=2023-09-11 06:45:05.236739+00:00, run_duration=0.234957, state=success, executor_state=success, try_number=1, max_tries=0, job_id=72, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:01.929994+00:00, queued_by_job_id=2, pid=41397[0m
[[34m2023-09-11T06:45:05.825+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-11T00:00:00+00:00, run_after=2023-03-12T00:00:00+00:00[0m
[[34m2023-09-11T06:45:05.862+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-09 00:00:00+00:00: scheduled__2023-03-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:01.873855+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:05.862+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-09 00:00:00+00:00, run_id=scheduled__2023-03-09T00:00:00+00:00, run_start_date=2023-09-11 06:45:01.897463+00:00, run_end_date=2023-09-11 06:45:05.862807+00:00, run_duration=3.965344, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-09 00:00:00+00:00, data_interval_end=2023-03-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:05.866+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-10T00:00:00+00:00, run_after=2023-03-11T00:00:00+00:00[0m
[[34m2023-09-11T06:45:05.881+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:05.882+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:05.882+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:05.884+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:05.885+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:05.885+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:05.899+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:07.842+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:07.978+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:08.159+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:08.192+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:08.675+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:08.675+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:08.744+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:08.802+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:09.552+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:09.564+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:08.890309+00:00, run_end_date=2023-09-11 06:45:09.125780+00:00, run_duration=0.235471, state=success, executor_state=success, try_number=1, max_tries=0, job_id=73, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:05.883396+00:00, queued_by_job_id=2, pid=41406[0m
[[34m2023-09-11T06:45:09.713+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-11T00:00:00+00:00, run_after=2023-03-12T00:00:00+00:00[0m
[[34m2023-09-11T06:45:09.741+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-10 00:00:00+00:00: scheduled__2023-03-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:05.819823+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:09.741+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-10 00:00:00+00:00, run_id=scheduled__2023-03-10T00:00:00+00:00, run_start_date=2023-09-11 06:45:05.839577+00:00, run_end_date=2023-09-11 06:45:09.741447+00:00, run_duration=3.90187, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-10 00:00:00+00:00, data_interval_end=2023-03-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:09.745+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-11T00:00:00+00:00, run_after=2023-03-12T00:00:00+00:00[0m
[[34m2023-09-11T06:45:10.812+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-12T00:00:00+00:00, run_after=2023-03-13T00:00:00+00:00[0m
[[34m2023-09-11T06:45:10.860+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:10.860+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:10.861+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:10.863+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:10.863+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:10.864+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:10.866+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:12.769+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:12.898+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:13.071+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:13.103+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:13.564+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:13.564+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:13.635+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:13.694+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:14.402+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:14.413+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:13.779460+00:00, run_end_date=2023-09-11 06:45:13.996424+00:00, run_duration=0.216964, state=success, executor_state=success, try_number=1, max_tries=0, job_id=74, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:10.861835+00:00, queued_by_job_id=2, pid=41416[0m
[[34m2023-09-11T06:45:14.575+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-13T00:00:00+00:00, run_after=2023-03-14T00:00:00+00:00[0m
[[34m2023-09-11T06:45:14.623+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-11 00:00:00+00:00: scheduled__2023-03-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:10.807038+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:14.623+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-11 00:00:00+00:00, run_id=scheduled__2023-03-11T00:00:00+00:00, run_start_date=2023-09-11 06:45:10.825530+00:00, run_end_date=2023-09-11 06:45:14.623656+00:00, run_duration=3.798126, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-11 00:00:00+00:00, data_interval_end=2023-03-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:14.626+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-12T00:00:00+00:00, run_after=2023-03-13T00:00:00+00:00[0m
[[34m2023-09-11T06:45:14.642+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:14.642+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:14.642+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:14.645+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:14.645+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:14.646+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:14.649+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:16.519+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:16.650+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:16.851+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:16.890+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:17.455+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:17.456+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:17.528+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:17.608+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:18.428+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:18.440+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:17.729668+00:00, run_end_date=2023-09-11 06:45:17.981491+00:00, run_duration=0.251823, state=success, executor_state=success, try_number=1, max_tries=0, job_id=75, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:14.643546+00:00, queued_by_job_id=2, pid=41425[0m
[[34m2023-09-11T06:45:18.687+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-13T00:00:00+00:00, run_after=2023-03-14T00:00:00+00:00[0m
[[34m2023-09-11T06:45:18.709+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-12 00:00:00+00:00: scheduled__2023-03-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:14.569848+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:18.710+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-12 00:00:00+00:00, run_id=scheduled__2023-03-12T00:00:00+00:00, run_start_date=2023-09-11 06:45:14.599264+00:00, run_end_date=2023-09-11 06:45:18.710408+00:00, run_duration=4.111144, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-12 00:00:00+00:00, data_interval_end=2023-03-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:18.713+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-13T00:00:00+00:00, run_after=2023-03-14T00:00:00+00:00[0m
[[34m2023-09-11T06:45:19.678+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-14T00:00:00+00:00, run_after=2023-03-15T00:00:00+00:00[0m
[[34m2023-09-11T06:45:19.732+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:19.733+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:19.733+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:19.735+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:19.735+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:19.736+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:19.739+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:21.701+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:21.838+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:22.016+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:22.050+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:22.546+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:22.546+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:22.619+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:22.683+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:23.458+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:23.471+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:22.775767+00:00, run_end_date=2023-09-11 06:45:23.011261+00:00, run_duration=0.235494, state=success, executor_state=success, try_number=1, max_tries=0, job_id=76, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:19.734095+00:00, queued_by_job_id=2, pid=41435[0m
[[34m2023-09-11T06:45:23.739+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-15T00:00:00+00:00, run_after=2023-03-16T00:00:00+00:00[0m
[[34m2023-09-11T06:45:23.778+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-13 00:00:00+00:00: scheduled__2023-03-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:19.673486+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:23.778+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-13 00:00:00+00:00, run_id=scheduled__2023-03-13T00:00:00+00:00, run_start_date=2023-09-11 06:45:19.699760+00:00, run_end_date=2023-09-11 06:45:23.778575+00:00, run_duration=4.078815, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-13 00:00:00+00:00, data_interval_end=2023-03-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:23.782+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-14T00:00:00+00:00, run_after=2023-03-15T00:00:00+00:00[0m
[[34m2023-09-11T06:45:23.799+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:23.799+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:23.799+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:23.802+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:23.803+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:23.804+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:23.807+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:25.775+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:25.911+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:26.090+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:26.125+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:26.608+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:26.609+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:26.683+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:26.743+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:27.497+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:27.509+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:26.832833+00:00, run_end_date=2023-09-11 06:45:27.076075+00:00, run_duration=0.243242, state=success, executor_state=success, try_number=1, max_tries=0, job_id=77, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:23.801328+00:00, queued_by_job_id=2, pid=41442[0m
[[34m2023-09-11T06:45:27.784+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-15T00:00:00+00:00, run_after=2023-03-16T00:00:00+00:00[0m
[[34m2023-09-11T06:45:27.808+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-14 00:00:00+00:00: scheduled__2023-03-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:23.733999+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:27.809+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-14 00:00:00+00:00, run_id=scheduled__2023-03-14T00:00:00+00:00, run_start_date=2023-09-11 06:45:23.754053+00:00, run_end_date=2023-09-11 06:45:27.808901+00:00, run_duration=4.054848, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-14 00:00:00+00:00, data_interval_end=2023-03-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:27.812+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-15T00:00:00+00:00, run_after=2023-03-16T00:00:00+00:00[0m
[[34m2023-09-11T06:45:28.804+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-16T00:00:00+00:00, run_after=2023-03-17T00:00:00+00:00[0m
[[34m2023-09-11T06:45:28.849+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:28.850+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:28.850+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:28.852+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:28.853+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:28.853+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:28.856+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:30.811+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:30.960+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:31.152+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:31.192+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:31.709+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:31.710+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:31.783+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:31.848+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:32.627+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:32.638+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:31.945613+00:00, run_end_date=2023-09-11 06:45:32.182979+00:00, run_duration=0.237366, state=success, executor_state=success, try_number=1, max_tries=0, job_id=78, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:28.850939+00:00, queued_by_job_id=2, pid=41452[0m
[[34m2023-09-11T06:45:32.911+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00[0m
[[34m2023-09-11T06:45:32.949+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-15 00:00:00+00:00: scheduled__2023-03-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:28.799554+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:32.950+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-15 00:00:00+00:00, run_id=scheduled__2023-03-15T00:00:00+00:00, run_start_date=2023-09-11 06:45:28.816592+00:00, run_end_date=2023-09-11 06:45:32.949879+00:00, run_duration=4.133287, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-15 00:00:00+00:00, data_interval_end=2023-03-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:32.954+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-16T00:00:00+00:00, run_after=2023-03-17T00:00:00+00:00[0m
[[34m2023-09-11T06:45:32.970+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:32.970+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:32.970+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:32.972+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:32.973+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:32.973+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:32.976+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:34.964+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:35.114+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:35.309+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:35.345+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:35.842+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:35.843+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:35.923+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:35.984+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:36.772+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:36.782+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:36.073198+00:00, run_end_date=2023-09-11 06:45:36.323834+00:00, run_duration=0.250636, state=success, executor_state=success, try_number=1, max_tries=0, job_id=79, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:32.971486+00:00, queued_by_job_id=2, pid=41459[0m
[[34m2023-09-11T06:45:37.029+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00[0m
[[34m2023-09-11T06:45:37.052+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-16 00:00:00+00:00: scheduled__2023-03-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:32.906784+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:37.052+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-16 00:00:00+00:00, run_id=scheduled__2023-03-16T00:00:00+00:00, run_start_date=2023-09-11 06:45:32.925811+00:00, run_end_date=2023-09-11 06:45:37.052740+00:00, run_duration=4.126929, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-16 00:00:00+00:00, data_interval_end=2023-03-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:37.056+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00[0m
[[34m2023-09-11T06:45:37.907+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00[0m
[[34m2023-09-11T06:45:37.952+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:37.952+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:37.953+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:37.955+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:37.956+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:37.956+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:37.959+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:39.990+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:40.131+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:40.315+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:40.351+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:40.909+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:40.909+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:40.982+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:41.046+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:41.873+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:41.885+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:41.138558+00:00, run_end_date=2023-09-11 06:45:41.405071+00:00, run_duration=0.266513, state=success, executor_state=success, try_number=1, max_tries=0, job_id=80, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:37.953814+00:00, queued_by_job_id=2, pid=41469[0m
[[34m2023-09-11T06:45:42.152+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-19T00:00:00+00:00, run_after=2023-03-20T00:00:00+00:00[0m
[[34m2023-09-11T06:45:42.190+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-17 00:00:00+00:00: scheduled__2023-03-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:37.903035+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:42.190+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-17 00:00:00+00:00, run_id=scheduled__2023-03-17T00:00:00+00:00, run_start_date=2023-09-11 06:45:37.921033+00:00, run_end_date=2023-09-11 06:45:42.190535+00:00, run_duration=4.269502, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-17 00:00:00+00:00, data_interval_end=2023-03-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:42.194+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00[0m
[[34m2023-09-11T06:45:42.210+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:42.210+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:42.210+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:42.212+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:42.213+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:42.213+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:42.229+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:44.337+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:44.479+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:44.662+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:44.696+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:45.208+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:45.209+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:45.290+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:45.357+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:46.154+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:46.166+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:45.452206+00:00, run_end_date=2023-09-11 06:45:45.704319+00:00, run_duration=0.252113, state=success, executor_state=success, try_number=1, max_tries=0, job_id=81, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:42.211328+00:00, queued_by_job_id=2, pid=41478[0m
[[34m2023-09-11T06:45:46.410+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-19T00:00:00+00:00, run_after=2023-03-20T00:00:00+00:00[0m
[[34m2023-09-11T06:45:46.435+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-18 00:00:00+00:00: scheduled__2023-03-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:42.147405+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:46.435+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-18 00:00:00+00:00, run_id=scheduled__2023-03-18T00:00:00+00:00, run_start_date=2023-09-11 06:45:42.165319+00:00, run_end_date=2023-09-11 06:45:46.435517+00:00, run_duration=4.270198, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-18 00:00:00+00:00, data_interval_end=2023-03-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:46.439+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-19T00:00:00+00:00, run_after=2023-03-20T00:00:00+00:00[0m
[[34m2023-09-11T06:45:47.210+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-20T00:00:00+00:00, run_after=2023-03-21T00:00:00+00:00[0m
[[34m2023-09-11T06:45:47.258+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:47.258+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:47.258+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:47.261+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:47.261+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:47.262+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:47.264+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:49.214+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:49.355+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:49.541+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:49.574+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:50.057+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:50.058+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:50.137+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:50.197+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:50.974+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:50.986+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:50.286266+00:00, run_end_date=2023-09-11 06:45:50.533361+00:00, run_duration=0.247095, state=success, executor_state=success, try_number=1, max_tries=0, job_id=82, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:47.259573+00:00, queued_by_job_id=2, pid=41488[0m
[[34m2023-09-11T06:45:51.255+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-21T00:00:00+00:00, run_after=2023-03-22T00:00:00+00:00[0m
[[34m2023-09-11T06:45:51.295+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-19 00:00:00+00:00: scheduled__2023-03-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:47.205427+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:51.296+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-19 00:00:00+00:00, run_id=scheduled__2023-03-19T00:00:00+00:00, run_start_date=2023-09-11 06:45:47.223455+00:00, run_end_date=2023-09-11 06:45:51.296681+00:00, run_duration=4.073226, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-19 00:00:00+00:00, data_interval_end=2023-03-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:51.301+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-20T00:00:00+00:00, run_after=2023-03-21T00:00:00+00:00[0m
[[34m2023-09-11T06:45:51.317+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:51.317+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:51.317+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:51.320+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:51.321+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:51.321+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:51.324+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:53.305+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:53.443+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:53.627+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:53.661+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:54.175+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:54.176+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:54.255+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:54.317+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:45:55.053+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:45:55.063+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:54.413512+00:00, run_end_date=2023-09-11 06:45:54.650214+00:00, run_duration=0.236702, state=success, executor_state=success, try_number=1, max_tries=0, job_id=83, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:51.318642+00:00, queued_by_job_id=2, pid=41497[0m
[[34m2023-09-11T06:45:55.317+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-21T00:00:00+00:00, run_after=2023-03-22T00:00:00+00:00[0m
[[34m2023-09-11T06:45:55.341+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-20 00:00:00+00:00: scheduled__2023-03-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:51.249310+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:45:55.342+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-20 00:00:00+00:00, run_id=scheduled__2023-03-20T00:00:00+00:00, run_start_date=2023-09-11 06:45:51.269833+00:00, run_end_date=2023-09-11 06:45:55.342154+00:00, run_duration=4.072321, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-20 00:00:00+00:00, data_interval_end=2023-03-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:45:55.345+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-21T00:00:00+00:00, run_after=2023-03-22T00:00:00+00:00[0m
[[34m2023-09-11T06:45:56.248+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-22T00:00:00+00:00, run_after=2023-03-23T00:00:00+00:00[0m
[[34m2023-09-11T06:45:56.292+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:56.293+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:45:56.293+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:45:56.295+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:45:56.295+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:45:56.296+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:56.298+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:45:58.338+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:45:58.489+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:58.671+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:58.705+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:45:59.195+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:45:59.195+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:45:59.272+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:45:59.333+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:00.143+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:00.155+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:45:59.430363+00:00, run_end_date=2023-09-11 06:45:59.669784+00:00, run_duration=0.239421, state=success, executor_state=success, try_number=1, max_tries=0, job_id=84, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:45:56.294090+00:00, queued_by_job_id=2, pid=41507[0m
[[34m2023-09-11T06:46:00.666+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-23T00:00:00+00:00, run_after=2023-03-24T00:00:00+00:00[0m
[[34m2023-09-11T06:46:00.703+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-21 00:00:00+00:00: scheduled__2023-03-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:45:56.244066+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:00.704+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-21 00:00:00+00:00, run_id=scheduled__2023-03-21T00:00:00+00:00, run_start_date=2023-09-11 06:45:56.260488+00:00, run_end_date=2023-09-11 06:46:00.704267+00:00, run_duration=4.443779, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-21 00:00:00+00:00, data_interval_end=2023-03-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:00.707+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-22T00:00:00+00:00, run_after=2023-03-23T00:00:00+00:00[0m
[[34m2023-09-11T06:46:00.723+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:00.723+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:00.723+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:00.726+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:00.726+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:00.727+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:00.730+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:02.667+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:02.804+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:02.994+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:03.027+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:03.514+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:03.514+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:03.587+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:03.647+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:04.371+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:04.382+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:03.736927+00:00, run_end_date=2023-09-11 06:46:03.964867+00:00, run_duration=0.22794, state=success, executor_state=success, try_number=1, max_tries=0, job_id=85, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:00.724649+00:00, queued_by_job_id=2, pid=41516[0m
[[34m2023-09-11T06:46:04.835+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-23T00:00:00+00:00, run_after=2023-03-24T00:00:00+00:00[0m
[[34m2023-09-11T06:46:04.859+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-22 00:00:00+00:00: scheduled__2023-03-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:00.660801+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:04.860+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-22 00:00:00+00:00, run_id=scheduled__2023-03-22T00:00:00+00:00, run_start_date=2023-09-11 06:46:00.679349+00:00, run_end_date=2023-09-11 06:46:04.860410+00:00, run_duration=4.181061, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-22 00:00:00+00:00, data_interval_end=2023-03-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:04.863+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-23T00:00:00+00:00, run_after=2023-03-24T00:00:00+00:00[0m
[[34m2023-09-11T06:46:05.421+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-24T00:00:00+00:00, run_after=2023-03-25T00:00:00+00:00[0m
[[34m2023-09-11T06:46:05.480+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:05.480+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:05.480+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:05.482+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:05.483+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:05.483+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:05.486+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:07.408+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:07.540+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:07.722+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:07.757+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:08.261+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:08.261+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:08.333+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:08.394+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:09.108+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:09.123+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:08.483415+00:00, run_end_date=2023-09-11 06:46:08.713286+00:00, run_duration=0.229871, state=success, executor_state=success, try_number=1, max_tries=0, job_id=86, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:05.481529+00:00, queued_by_job_id=2, pid=41526[0m
[[34m2023-09-11T06:46:09.702+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-25T00:00:00+00:00, run_after=2023-03-26T00:00:00+00:00[0m
[[34m2023-09-11T06:46:09.739+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-23 00:00:00+00:00: scheduled__2023-03-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:05.415374+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:09.740+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-23 00:00:00+00:00, run_id=scheduled__2023-03-23T00:00:00+00:00, run_start_date=2023-09-11 06:46:05.433221+00:00, run_end_date=2023-09-11 06:46:09.740299+00:00, run_duration=4.307078, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-23 00:00:00+00:00, data_interval_end=2023-03-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:09.744+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-24T00:00:00+00:00, run_after=2023-03-25T00:00:00+00:00[0m
[[34m2023-09-11T06:46:09.760+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:09.760+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:09.761+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:09.763+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:09.763+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:09.764+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:09.767+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:11.693+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:11.829+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:12.020+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:12.053+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:12.538+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:12.538+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:12.612+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:12.674+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:13.587+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:13.601+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:12.766227+00:00, run_end_date=2023-09-11 06:46:13.006359+00:00, run_duration=0.240132, state=success, executor_state=success, try_number=1, max_tries=0, job_id=87, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:09.761917+00:00, queued_by_job_id=2, pid=41535[0m
[[34m2023-09-11T06:46:14.114+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-25T00:00:00+00:00, run_after=2023-03-26T00:00:00+00:00[0m
[[34m2023-09-11T06:46:14.139+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-24 00:00:00+00:00: scheduled__2023-03-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:09.696883+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:14.140+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-24 00:00:00+00:00, run_id=scheduled__2023-03-24T00:00:00+00:00, run_start_date=2023-09-11 06:46:09.716263+00:00, run_end_date=2023-09-11 06:46:14.140344+00:00, run_duration=4.424081, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-24 00:00:00+00:00, data_interval_end=2023-03-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:14.144+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-25T00:00:00+00:00, run_after=2023-03-26T00:00:00+00:00[0m
[[34m2023-09-11T06:46:14.655+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-26T00:00:00+00:00, run_after=2023-03-27T00:00:00+00:00[0m
[[34m2023-09-11T06:46:14.699+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:14.699+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:14.700+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:14.702+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:14.703+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:14.703+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:14.706+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:16.637+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:16.772+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:16.966+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:17.012+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:17.557+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:17.557+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:17.630+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:17.689+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:18.482+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:18.493+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:17.777701+00:00, run_end_date=2023-09-11 06:46:18.012112+00:00, run_duration=0.234411, state=success, executor_state=success, try_number=1, max_tries=0, job_id=88, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:14.701015+00:00, queued_by_job_id=2, pid=41545[0m
[[34m2023-09-11T06:46:18.769+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-27T00:00:00+00:00, run_after=2023-03-28T00:00:00+00:00[0m
[[34m2023-09-11T06:46:18.809+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-25 00:00:00+00:00: scheduled__2023-03-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:14.650689+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:18.809+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-25 00:00:00+00:00, run_id=scheduled__2023-03-25T00:00:00+00:00, run_start_date=2023-09-11 06:46:14.667716+00:00, run_end_date=2023-09-11 06:46:18.809550+00:00, run_duration=4.141834, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-25 00:00:00+00:00, data_interval_end=2023-03-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:18.813+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-26T00:00:00+00:00, run_after=2023-03-27T00:00:00+00:00[0m
[[34m2023-09-11T06:46:18.829+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:18.829+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:18.830+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:18.832+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:18.833+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:18.833+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:18.836+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:20.823+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:20.974+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:21.169+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:21.204+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:21.717+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:21.717+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:21.795+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:21.861+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:22.840+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:22.851+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:21.957416+00:00, run_end_date=2023-09-11 06:46:22.411353+00:00, run_duration=0.453937, state=success, executor_state=success, try_number=1, max_tries=0, job_id=89, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:18.830822+00:00, queued_by_job_id=2, pid=41552[0m
[[34m2023-09-11T06:46:23.399+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-27T00:00:00+00:00, run_after=2023-03-28T00:00:00+00:00[0m
[[34m2023-09-11T06:46:23.426+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-26 00:00:00+00:00: scheduled__2023-03-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:18.763890+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:23.427+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-26 00:00:00+00:00, run_id=scheduled__2023-03-26T00:00:00+00:00, run_start_date=2023-09-11 06:46:18.782612+00:00, run_end_date=2023-09-11 06:46:23.427227+00:00, run_duration=4.644615, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-26 00:00:00+00:00, data_interval_end=2023-03-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:23.430+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-27T00:00:00+00:00, run_after=2023-03-28T00:00:00+00:00[0m
[[34m2023-09-11T06:46:24.252+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-28T00:00:00+00:00, run_after=2023-03-29T00:00:00+00:00[0m
[[34m2023-09-11T06:46:24.331+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:24.331+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:24.332+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:24.334+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:24.335+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:24.336+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:24.339+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:26.243+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:26.378+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:26.565+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:26.599+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:27.082+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:27.082+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:27.184+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:27.244+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:27.974+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:27.989+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:27.331666+00:00, run_end_date=2023-09-11 06:46:27.566152+00:00, run_duration=0.234486, state=success, executor_state=success, try_number=1, max_tries=0, job_id=90, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:24.332741+00:00, queued_by_job_id=2, pid=41562[0m
[[34m2023-09-11T06:46:28.496+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-29T00:00:00+00:00, run_after=2023-03-30T00:00:00+00:00[0m
[[34m2023-09-11T06:46:28.548+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-27 00:00:00+00:00: scheduled__2023-03-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:24.246822+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:28.549+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-27 00:00:00+00:00, run_id=scheduled__2023-03-27T00:00:00+00:00, run_start_date=2023-09-11 06:46:24.297233+00:00, run_end_date=2023-09-11 06:46:28.549332+00:00, run_duration=4.252099, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-27 00:00:00+00:00, data_interval_end=2023-03-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:28.553+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-28T00:00:00+00:00, run_after=2023-03-29T00:00:00+00:00[0m
[[34m2023-09-11T06:46:28.583+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:28.583+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:28.584+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:28.586+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:28.587+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:28.587+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:28.590+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:30.629+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:30.781+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:31.024+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:31.062+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:31.585+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:31.588+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:31.699+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:31.768+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:32.550+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:32.561+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:31.870545+00:00, run_end_date=2023-09-11 06:46:32.106138+00:00, run_duration=0.235593, state=success, executor_state=success, try_number=1, max_tries=0, job_id=91, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:28.585216+00:00, queued_by_job_id=2, pid=41571[0m
[[34m2023-09-11T06:46:33.006+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-29T00:00:00+00:00, run_after=2023-03-30T00:00:00+00:00[0m
[[34m2023-09-11T06:46:33.029+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-28 00:00:00+00:00: scheduled__2023-03-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:28.491486+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:33.030+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-28 00:00:00+00:00, run_id=scheduled__2023-03-28T00:00:00+00:00, run_start_date=2023-09-11 06:46:28.509598+00:00, run_end_date=2023-09-11 06:46:33.029973+00:00, run_duration=4.520375, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-28 00:00:00+00:00, data_interval_end=2023-03-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:33.033+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-29T00:00:00+00:00, run_after=2023-03-30T00:00:00+00:00[0m
[[34m2023-09-11T06:46:34.362+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-30T00:00:00+00:00, run_after=2023-03-31T00:00:00+00:00[0m
[[34m2023-09-11T06:46:34.416+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:34.416+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:34.417+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:34.419+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:34.420+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:34.420+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:34.423+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:36.435+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:36.568+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:36.742+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:36.775+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:37.263+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:37.263+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:37.337+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:37.402+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:38.204+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:38.215+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:37.497233+00:00, run_end_date=2023-09-11 06:46:37.746668+00:00, run_duration=0.249435, state=success, executor_state=success, try_number=1, max_tries=0, job_id=92, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:34.417960+00:00, queued_by_job_id=2, pid=41581[0m
[[34m2023-09-11T06:46:38.372+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-31T00:00:00+00:00, run_after=2023-04-01T00:00:00+00:00[0m
[[34m2023-09-11T06:46:38.410+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-29 00:00:00+00:00: scheduled__2023-03-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:34.357982+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:38.410+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-29 00:00:00+00:00, run_id=scheduled__2023-03-29T00:00:00+00:00, run_start_date=2023-09-11 06:46:34.375764+00:00, run_end_date=2023-09-11 06:46:38.410682+00:00, run_duration=4.034918, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-29 00:00:00+00:00, data_interval_end=2023-03-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:38.414+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-30T00:00:00+00:00, run_after=2023-03-31T00:00:00+00:00[0m
[[34m2023-09-11T06:46:38.429+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:38.430+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:38.430+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:38.432+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:38.433+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:38.433+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:38.436+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:40.435+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:40.578+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:40.761+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:40.797+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:41.362+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:41.363+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:41.493+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:41.596+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:42.838+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:42.869+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:41.732139+00:00, run_end_date=2023-09-11 06:46:42.202900+00:00, run_duration=0.470761, state=success, executor_state=success, try_number=1, max_tries=0, job_id=93, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:38.431244+00:00, queued_by_job_id=2, pid=41588[0m
[[34m2023-09-11T06:46:43.326+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-31T00:00:00+00:00, run_after=2023-04-01T00:00:00+00:00[0m
[[34m2023-09-11T06:46:43.353+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-30 00:00:00+00:00: scheduled__2023-03-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:38.365817+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:43.354+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-30 00:00:00+00:00, run_id=scheduled__2023-03-30T00:00:00+00:00, run_start_date=2023-09-11 06:46:38.386109+00:00, run_end_date=2023-09-11 06:46:43.353806+00:00, run_duration=4.967697, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-30 00:00:00+00:00, data_interval_end=2023-03-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:43.357+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-03-31T00:00:00+00:00, run_after=2023-04-01T00:00:00+00:00[0m
[[34m2023-09-11T06:46:44.649+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-01T00:00:00+00:00, run_after=2023-04-02T00:00:00+00:00[0m
[[34m2023-09-11T06:46:44.701+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:44.701+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:44.702+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-03-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:44.705+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:44.705+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:44.706+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:44.708+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:46.864+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:47.024+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:47.275+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:47.311+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:47.839+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:47.840+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:47.921+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-03-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:47.986+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-03-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:48.749+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-03-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:48.759+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-03-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:48.079553+00:00, run_end_date=2023-09-11 06:46:48.351322+00:00, run_duration=0.271769, state=success, executor_state=success, try_number=1, max_tries=0, job_id=94, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:44.703184+00:00, queued_by_job_id=2, pid=41600[0m
[[34m2023-09-11T06:46:49.260+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-02T00:00:00+00:00, run_after=2023-04-03T00:00:00+00:00[0m
[[34m2023-09-11T06:46:49.297+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-03-31 00:00:00+00:00: scheduled__2023-03-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:44.644358+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:49.297+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-03-31 00:00:00+00:00, run_id=scheduled__2023-03-31T00:00:00+00:00, run_start_date=2023-09-11 06:46:44.662211+00:00, run_end_date=2023-09-11 06:46:49.297401+00:00, run_duration=4.63519, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-03-31 00:00:00+00:00, data_interval_end=2023-04-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:49.301+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-01T00:00:00+00:00, run_after=2023-04-02T00:00:00+00:00[0m
[[34m2023-09-11T06:46:49.319+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:49.319+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:49.319+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:49.322+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:49.322+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:49.322+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:49.325+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:51.158+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:51.286+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:51.457+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:51.489+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:51.960+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:51.961+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:52.038+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:52.098+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:52.808+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:52.819+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:52.184516+00:00, run_end_date=2023-09-11 06:46:52.418701+00:00, run_duration=0.234185, state=success, executor_state=success, try_number=1, max_tries=0, job_id=95, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:49.320688+00:00, queued_by_job_id=2, pid=41607[0m
[[34m2023-09-11T06:46:53.071+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-02T00:00:00+00:00, run_after=2023-04-03T00:00:00+00:00[0m
[[34m2023-09-11T06:46:53.096+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-01 00:00:00+00:00: scheduled__2023-04-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:49.255687+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:53.097+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-01 00:00:00+00:00, run_id=scheduled__2023-04-01T00:00:00+00:00, run_start_date=2023-09-11 06:46:49.274699+00:00, run_end_date=2023-09-11 06:46:53.097132+00:00, run_duration=3.822433, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-01 00:00:00+00:00, data_interval_end=2023-04-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:53.100+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-02T00:00:00+00:00, run_after=2023-04-03T00:00:00+00:00[0m
[[34m2023-09-11T06:46:53.678+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-03T00:00:00+00:00, run_after=2023-04-04T00:00:00+00:00[0m
[[34m2023-09-11T06:46:53.723+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:53.723+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:53.723+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:53.725+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:53.726+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:53.726+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:53.729+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:55.587+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:55.715+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:55.887+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:55.920+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:56.391+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:46:56.392+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:56.463+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:46:56.520+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:46:57.282+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:46:57.293+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:46:56.606519+00:00, run_end_date=2023-09-11 06:46:56.824096+00:00, run_duration=0.217577, state=success, executor_state=success, try_number=1, max_tries=0, job_id=96, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:53.724427+00:00, queued_by_job_id=2, pid=41615[0m
[[34m2023-09-11T06:46:57.451+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-04T00:00:00+00:00, run_after=2023-04-05T00:00:00+00:00[0m
[[34m2023-09-11T06:46:57.488+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-02 00:00:00+00:00: scheduled__2023-04-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:53.673738+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:46:57.488+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-02 00:00:00+00:00, run_id=scheduled__2023-04-02T00:00:00+00:00, run_start_date=2023-09-11 06:46:53.690714+00:00, run_end_date=2023-09-11 06:46:57.488884+00:00, run_duration=3.79817, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-02 00:00:00+00:00, data_interval_end=2023-04-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:46:57.492+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-03T00:00:00+00:00, run_after=2023-04-04T00:00:00+00:00[0m
[[34m2023-09-11T06:46:57.508+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:57.508+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:46:57.509+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:46:57.511+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:46:57.511+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:46:57.512+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:57.515+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:46:59.396+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:46:59.528+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:46:59.739+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:46:59.771+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:00.257+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:00.258+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:00.335+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:00.395+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:01.105+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:01.117+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:00.481559+00:00, run_end_date=2023-09-11 06:47:00.700484+00:00, run_duration=0.218925, state=success, executor_state=success, try_number=1, max_tries=0, job_id=97, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:46:57.509839+00:00, queued_by_job_id=2, pid=41624[0m
[[34m2023-09-11T06:47:01.371+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-04T00:00:00+00:00, run_after=2023-04-05T00:00:00+00:00[0m
[[34m2023-09-11T06:47:01.408+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-03 00:00:00+00:00: scheduled__2023-04-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:46:57.446831+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:01.408+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-03 00:00:00+00:00, run_id=scheduled__2023-04-03T00:00:00+00:00, run_start_date=2023-09-11 06:46:57.465004+00:00, run_end_date=2023-09-11 06:47:01.408692+00:00, run_duration=3.943688, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-03 00:00:00+00:00, data_interval_end=2023-04-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:01.412+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-04T00:00:00+00:00, run_after=2023-04-05T00:00:00+00:00[0m
[[34m2023-09-11T06:47:02.541+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-05T00:00:00+00:00, run_after=2023-04-06T00:00:00+00:00[0m
[[34m2023-09-11T06:47:02.584+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:02.585+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:02.585+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:02.587+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:02.588+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:02.588+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:02.591+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:04.432+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:04.561+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:04.744+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:04.778+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:05.247+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:05.248+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:05.317+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:05.374+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:06.082+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:06.093+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:05.458716+00:00, run_end_date=2023-09-11 06:47:05.681532+00:00, run_duration=0.222816, state=success, executor_state=success, try_number=1, max_tries=0, job_id=98, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:02.586090+00:00, queued_by_job_id=2, pid=41634[0m
[[34m2023-09-11T06:47:06.260+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-06T00:00:00+00:00, run_after=2023-04-07T00:00:00+00:00[0m
[[34m2023-09-11T06:47:06.296+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-04 00:00:00+00:00: scheduled__2023-04-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:02.536847+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:06.297+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-04 00:00:00+00:00, run_id=scheduled__2023-04-04T00:00:00+00:00, run_start_date=2023-09-11 06:47:02.553575+00:00, run_end_date=2023-09-11 06:47:06.297204+00:00, run_duration=3.743629, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-04 00:00:00+00:00, data_interval_end=2023-04-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:06.300+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-05T00:00:00+00:00, run_after=2023-04-06T00:00:00+00:00[0m
[[34m2023-09-11T06:47:06.315+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:06.316+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:06.316+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:06.318+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:06.319+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:06.319+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:06.322+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:08.166+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:08.299+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:08.466+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:08.498+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:08.999+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:09.000+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:09.068+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:09.125+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:09.830+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:09.841+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:09.212598+00:00, run_end_date=2023-09-11 06:47:09.442642+00:00, run_duration=0.230044, state=success, executor_state=success, try_number=1, max_tries=0, job_id=99, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:06.317148+00:00, queued_by_job_id=2, pid=41643[0m
[[34m2023-09-11T06:47:09.988+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-06T00:00:00+00:00, run_after=2023-04-07T00:00:00+00:00[0m
[[34m2023-09-11T06:47:10.011+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-05 00:00:00+00:00: scheduled__2023-04-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:06.255673+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:10.011+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-05 00:00:00+00:00, run_id=scheduled__2023-04-05T00:00:00+00:00, run_start_date=2023-09-11 06:47:06.273911+00:00, run_end_date=2023-09-11 06:47:10.011721+00:00, run_duration=3.73781, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-05 00:00:00+00:00, data_interval_end=2023-04-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:10.015+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-06T00:00:00+00:00, run_after=2023-04-07T00:00:00+00:00[0m
[[34m2023-09-11T06:47:11.222+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-07T00:00:00+00:00, run_after=2023-04-08T00:00:00+00:00[0m
[[34m2023-09-11T06:47:11.267+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:11.267+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:11.267+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:11.270+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:11.270+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:11.270+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:11.273+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:13.106+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:13.238+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:13.412+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:13.444+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:13.902+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:13.902+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:13.971+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:14.028+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:14.746+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:14.756+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:14.119332+00:00, run_end_date=2023-09-11 06:47:14.339902+00:00, run_duration=0.22057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=100, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:11.268317+00:00, queued_by_job_id=2, pid=41653[0m
[[34m2023-09-11T06:47:15.013+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-08T00:00:00+00:00, run_after=2023-04-09T00:00:00+00:00[0m
[[34m2023-09-11T06:47:15.048+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-06 00:00:00+00:00: scheduled__2023-04-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:11.217913+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:15.049+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-06 00:00:00+00:00, run_id=scheduled__2023-04-06T00:00:00+00:00, run_start_date=2023-09-11 06:47:11.234900+00:00, run_end_date=2023-09-11 06:47:15.048856+00:00, run_duration=3.813956, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-06 00:00:00+00:00, data_interval_end=2023-04-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:15.052+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-07T00:00:00+00:00, run_after=2023-04-08T00:00:00+00:00[0m
[[34m2023-09-11T06:47:15.067+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:15.067+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:15.067+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:15.070+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:15.070+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:15.071+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:15.074+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:16.915+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:17.043+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:17.252+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:17.292+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:17.760+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:17.760+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:17.831+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:17.887+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:18.614+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:18.625+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:17.983053+00:00, run_end_date=2023-09-11 06:47:18.221688+00:00, run_duration=0.238635, state=success, executor_state=success, try_number=1, max_tries=0, job_id=101, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:15.068720+00:00, queued_by_job_id=2, pid=41660[0m
[[34m2023-09-11T06:47:18.878+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-08T00:00:00+00:00, run_after=2023-04-09T00:00:00+00:00[0m
[[34m2023-09-11T06:47:18.905+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-07 00:00:00+00:00: scheduled__2023-04-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:15.008522+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:18.905+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-07 00:00:00+00:00, run_id=scheduled__2023-04-07T00:00:00+00:00, run_start_date=2023-09-11 06:47:15.025732+00:00, run_end_date=2023-09-11 06:47:18.905453+00:00, run_duration=3.879721, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-07 00:00:00+00:00, data_interval_end=2023-04-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:18.908+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-08T00:00:00+00:00, run_after=2023-04-09T00:00:00+00:00[0m
[[34m2023-09-11T06:47:20.011+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-09T00:00:00+00:00, run_after=2023-04-10T00:00:00+00:00[0m
[[34m2023-09-11T06:47:20.055+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:20.055+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:20.055+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:20.057+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:20.058+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:20.058+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:20.061+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:21.912+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:22.042+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:22.218+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:22.251+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:22.722+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:22.723+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:22.793+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:22.850+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:23.559+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:23.570+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:22.940284+00:00, run_end_date=2023-09-11 06:47:23.162963+00:00, run_duration=0.222679, state=success, executor_state=success, try_number=1, max_tries=0, job_id=102, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:20.056323+00:00, queued_by_job_id=2, pid=41670[0m
[[34m2023-09-11T06:47:23.832+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-10T00:00:00+00:00, run_after=2023-04-11T00:00:00+00:00[0m
[[34m2023-09-11T06:47:23.868+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-08 00:00:00+00:00: scheduled__2023-04-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:20.006555+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:23.868+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-08 00:00:00+00:00, run_id=scheduled__2023-04-08T00:00:00+00:00, run_start_date=2023-09-11 06:47:20.023114+00:00, run_end_date=2023-09-11 06:47:23.868782+00:00, run_duration=3.845668, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-08 00:00:00+00:00, data_interval_end=2023-04-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:23.872+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-09T00:00:00+00:00, run_after=2023-04-10T00:00:00+00:00[0m
[[34m2023-09-11T06:47:23.887+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:23.898+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:23.898+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:23.901+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:23.901+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:23.902+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:23.918+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:25.790+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:25.927+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:26.097+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:26.135+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:26.600+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:26.601+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:26.671+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:26.731+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:27.437+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:27.448+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:26.815218+00:00, run_end_date=2023-09-11 06:47:27.046976+00:00, run_duration=0.231758, state=success, executor_state=success, try_number=1, max_tries=0, job_id=103, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:23.899626+00:00, queued_by_job_id=2, pid=41677[0m
[[34m2023-09-11T06:47:27.685+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-10T00:00:00+00:00, run_after=2023-04-11T00:00:00+00:00[0m
[[34m2023-09-11T06:47:27.708+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-09 00:00:00+00:00: scheduled__2023-04-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:23.827235+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:27.708+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-09 00:00:00+00:00, run_id=scheduled__2023-04-09T00:00:00+00:00, run_start_date=2023-09-11 06:47:23.845782+00:00, run_end_date=2023-09-11 06:47:27.708721+00:00, run_duration=3.862939, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-09 00:00:00+00:00, data_interval_end=2023-04-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:27.712+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-10T00:00:00+00:00, run_after=2023-04-11T00:00:00+00:00[0m
[[34m2023-09-11T06:47:28.802+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-11T00:00:00+00:00, run_after=2023-04-12T00:00:00+00:00[0m
[[34m2023-09-11T06:47:28.845+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:28.845+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:28.846+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:28.848+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:28.849+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:28.850+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:28.853+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:30.814+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:30.945+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:31.122+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:31.155+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:31.607+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:31.608+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:31.681+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:31.739+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:32.443+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:32.454+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:31.825062+00:00, run_end_date=2023-09-11 06:47:32.052237+00:00, run_duration=0.227175, state=success, executor_state=success, try_number=1, max_tries=0, job_id=104, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:28.846778+00:00, queued_by_job_id=2, pid=41687[0m
[[34m2023-09-11T06:47:32.728+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-12T00:00:00+00:00, run_after=2023-04-13T00:00:00+00:00[0m
[[34m2023-09-11T06:47:32.763+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-10 00:00:00+00:00: scheduled__2023-04-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:28.797709+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:32.764+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-10 00:00:00+00:00, run_id=scheduled__2023-04-10T00:00:00+00:00, run_start_date=2023-09-11 06:47:28.814521+00:00, run_end_date=2023-09-11 06:47:32.764071+00:00, run_duration=3.94955, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-10 00:00:00+00:00, data_interval_end=2023-04-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:32.767+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-11T00:00:00+00:00, run_after=2023-04-12T00:00:00+00:00[0m
[[34m2023-09-11T06:47:32.782+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:32.783+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:32.783+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:32.785+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:32.786+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:32.786+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:32.789+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:34.631+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:34.760+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:34.938+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:34.970+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:35.486+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:35.486+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:35.557+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:35.615+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:36.456+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:36.466+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:35.702321+00:00, run_end_date=2023-09-11 06:47:35.968966+00:00, run_duration=0.266645, state=success, executor_state=success, try_number=1, max_tries=0, job_id=105, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:32.784033+00:00, queued_by_job_id=2, pid=41696[0m
[[34m2023-09-11T06:47:36.716+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-12T00:00:00+00:00, run_after=2023-04-13T00:00:00+00:00[0m
[[34m2023-09-11T06:47:36.738+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-11 00:00:00+00:00: scheduled__2023-04-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:32.724186+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:36.739+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-11 00:00:00+00:00, run_id=scheduled__2023-04-11T00:00:00+00:00, run_start_date=2023-09-11 06:47:32.741461+00:00, run_end_date=2023-09-11 06:47:36.738968+00:00, run_duration=3.997507, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-11 00:00:00+00:00, data_interval_end=2023-04-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:36.742+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-12T00:00:00+00:00, run_after=2023-04-13T00:00:00+00:00[0m
[[34m2023-09-11T06:47:37.742+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-13T00:00:00+00:00, run_after=2023-04-14T00:00:00+00:00[0m
[[34m2023-09-11T06:47:37.785+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:37.785+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:37.785+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:37.787+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:37.788+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:37.788+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:37.791+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:39.887+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:40.041+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:40.368+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:40.410+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:40.955+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:40.957+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:41.074+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:41.156+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:41.940+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:41.952+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:41.251643+00:00, run_end_date=2023-09-11 06:47:41.529158+00:00, run_duration=0.277515, state=success, executor_state=success, try_number=1, max_tries=0, job_id=106, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:37.786419+00:00, queued_by_job_id=2, pid=41770[0m
[[34m2023-09-11T06:47:42.215+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-14T00:00:00+00:00, run_after=2023-04-15T00:00:00+00:00[0m
[[34m2023-09-11T06:47:42.251+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-12 00:00:00+00:00: scheduled__2023-04-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:37.737450+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:42.252+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-12 00:00:00+00:00, run_id=scheduled__2023-04-12T00:00:00+00:00, run_start_date=2023-09-11 06:47:37.754323+00:00, run_end_date=2023-09-11 06:47:42.252251+00:00, run_duration=4.497928, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-12 00:00:00+00:00, data_interval_end=2023-04-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:42.255+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-13T00:00:00+00:00, run_after=2023-04-14T00:00:00+00:00[0m
[[34m2023-09-11T06:47:42.270+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:42.271+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:42.271+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:42.273+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:42.274+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:42.274+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:42.276+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:44.319+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:44.453+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:44.633+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:44.666+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:45.153+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:45.154+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:45.227+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:45.289+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:46.044+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:46.055+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:45.376564+00:00, run_end_date=2023-09-11 06:47:45.611752+00:00, run_duration=0.235188, state=success, executor_state=success, try_number=1, max_tries=0, job_id=107, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:42.272017+00:00, queued_by_job_id=2, pid=41806[0m
[[34m2023-09-11T06:47:46.313+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-14T00:00:00+00:00, run_after=2023-04-15T00:00:00+00:00[0m
[[34m2023-09-11T06:47:46.336+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-13 00:00:00+00:00: scheduled__2023-04-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:42.210807+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:46.337+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-13 00:00:00+00:00, run_id=scheduled__2023-04-13T00:00:00+00:00, run_start_date=2023-09-11 06:47:42.228537+00:00, run_end_date=2023-09-11 06:47:46.337094+00:00, run_duration=4.108557, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-13 00:00:00+00:00, data_interval_end=2023-04-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:46.340+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-14T00:00:00+00:00, run_after=2023-04-15T00:00:00+00:00[0m
[[34m2023-09-11T06:47:46.762+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-15T00:00:00+00:00, run_after=2023-04-16T00:00:00+00:00[0m
[[34m2023-09-11T06:47:46.805+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:46.805+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:46.806+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:46.808+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:46.808+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:46.809+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:46.811+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:48.724+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:48.868+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:49.038+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:49.069+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:49.547+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:49.548+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:49.622+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:49.681+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:50.396+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:50.407+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:49.768485+00:00, run_end_date=2023-09-11 06:47:49.993459+00:00, run_duration=0.224974, state=success, executor_state=success, try_number=1, max_tries=0, job_id=108, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:46.806914+00:00, queued_by_job_id=2, pid=41816[0m
[[34m2023-09-11T06:47:50.674+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-16T00:00:00+00:00, run_after=2023-04-17T00:00:00+00:00[0m
[[34m2023-09-11T06:47:50.711+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-14 00:00:00+00:00: scheduled__2023-04-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:46.757740+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:50.712+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-14 00:00:00+00:00, run_id=scheduled__2023-04-14T00:00:00+00:00, run_start_date=2023-09-11 06:47:46.774458+00:00, run_end_date=2023-09-11 06:47:50.712433+00:00, run_duration=3.937975, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-14 00:00:00+00:00, data_interval_end=2023-04-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:50.716+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-15T00:00:00+00:00, run_after=2023-04-16T00:00:00+00:00[0m
[[34m2023-09-11T06:47:50.731+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:50.731+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:50.732+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:50.734+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:50.734+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:50.735+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:50.737+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:52.601+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:52.730+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:52.908+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:52.940+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:53.408+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:53.409+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:53.481+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:53.539+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:54.353+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:54.363+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:53.622716+00:00, run_end_date=2023-09-11 06:47:53.853126+00:00, run_duration=0.23041, state=success, executor_state=success, try_number=1, max_tries=0, job_id=109, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:50.732940+00:00, queued_by_job_id=2, pid=41826[0m
[[34m2023-09-11T06:47:54.621+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-16T00:00:00+00:00, run_after=2023-04-17T00:00:00+00:00[0m
[[34m2023-09-11T06:47:54.643+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-15 00:00:00+00:00: scheduled__2023-04-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:50.669448+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:54.643+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-15 00:00:00+00:00, run_id=scheduled__2023-04-15T00:00:00+00:00, run_start_date=2023-09-11 06:47:50.688733+00:00, run_end_date=2023-09-11 06:47:54.643625+00:00, run_duration=3.954892, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-15 00:00:00+00:00, data_interval_end=2023-04-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:54.647+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-16T00:00:00+00:00, run_after=2023-04-17T00:00:00+00:00[0m
[[34m2023-09-11T06:47:55.772+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-17T00:00:00+00:00, run_after=2023-04-18T00:00:00+00:00[0m
[[34m2023-09-11T06:47:55.817+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:55.817+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:55.817+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:55.819+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:55.820+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:55.820+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:55.823+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:57.767+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:47:57.893+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:58.061+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:58.094+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:47:58.587+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:47:58.588+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:47:58.672+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:47:58.733+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:47:59.453+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:47:59.464+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:47:58.824276+00:00, run_end_date=2023-09-11 06:47:59.047796+00:00, run_duration=0.22352, state=success, executor_state=success, try_number=1, max_tries=0, job_id=110, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:55.818315+00:00, queued_by_job_id=2, pid=41836[0m
[[34m2023-09-11T06:47:59.720+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-18T00:00:00+00:00, run_after=2023-04-19T00:00:00+00:00[0m
[[34m2023-09-11T06:47:59.755+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-16 00:00:00+00:00: scheduled__2023-04-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:55.768004+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:47:59.755+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-16 00:00:00+00:00, run_id=scheduled__2023-04-16T00:00:00+00:00, run_start_date=2023-09-11 06:47:55.785095+00:00, run_end_date=2023-09-11 06:47:59.755765+00:00, run_duration=3.97067, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-16 00:00:00+00:00, data_interval_end=2023-04-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:47:59.759+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-17T00:00:00+00:00, run_after=2023-04-18T00:00:00+00:00[0m
[[34m2023-09-11T06:47:59.800+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:59.801+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:47:59.801+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:47:59.803+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:47:59.803+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:47:59.804+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:47:59.819+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:01.768+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:01.914+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:02.086+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:02.119+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:02.599+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:02.600+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:02.670+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:02.741+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:03.483+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:03.494+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:02.828066+00:00, run_end_date=2023-09-11 06:48:03.052842+00:00, run_duration=0.224776, state=success, executor_state=success, try_number=1, max_tries=0, job_id=111, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:47:59.801869+00:00, queued_by_job_id=2, pid=41843[0m
[[34m2023-09-11T06:48:03.753+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-18T00:00:00+00:00, run_after=2023-04-19T00:00:00+00:00[0m
[[34m2023-09-11T06:48:03.777+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-17 00:00:00+00:00: scheduled__2023-04-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:47:59.715377+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:03.778+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-17 00:00:00+00:00, run_id=scheduled__2023-04-17T00:00:00+00:00, run_start_date=2023-09-11 06:47:59.733467+00:00, run_end_date=2023-09-11 06:48:03.777884+00:00, run_duration=4.044417, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-17 00:00:00+00:00, data_interval_end=2023-04-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:03.781+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-18T00:00:00+00:00, run_after=2023-04-19T00:00:00+00:00[0m
[[34m2023-09-11T06:48:04.823+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-19T00:00:00+00:00, run_after=2023-04-20T00:00:00+00:00[0m
[[34m2023-09-11T06:48:04.866+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:04.867+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:04.867+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:04.869+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:04.870+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:04.870+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:04.873+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:06.798+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:06.951+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:07.136+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:07.172+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:07.655+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:07.656+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:07.727+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:07.791+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:08.549+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:08.559+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:07.876990+00:00, run_end_date=2023-09-11 06:48:08.141079+00:00, run_duration=0.264089, state=success, executor_state=success, try_number=1, max_tries=0, job_id=112, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:04.867908+00:00, queued_by_job_id=2, pid=41853[0m
[[34m2023-09-11T06:48:08.824+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-20T00:00:00+00:00, run_after=2023-04-21T00:00:00+00:00[0m
[[34m2023-09-11T06:48:08.865+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-18 00:00:00+00:00: scheduled__2023-04-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:04.818320+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:08.865+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-18 00:00:00+00:00, run_id=scheduled__2023-04-18T00:00:00+00:00, run_start_date=2023-09-11 06:48:04.835138+00:00, run_end_date=2023-09-11 06:48:08.865602+00:00, run_duration=4.030464, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-18 00:00:00+00:00, data_interval_end=2023-04-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:08.869+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-19T00:00:00+00:00, run_after=2023-04-20T00:00:00+00:00[0m
[[34m2023-09-11T06:48:08.885+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:08.885+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:08.885+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:08.888+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:08.896+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:08.896+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:08.899+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:10.825+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:10.950+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:11.127+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:11.161+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:11.614+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:11.615+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:11.685+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:11.750+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:12.468+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:12.484+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:11.838211+00:00, run_end_date=2023-09-11 06:48:12.063945+00:00, run_duration=0.225734, state=success, executor_state=success, try_number=1, max_tries=0, job_id=113, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:08.886565+00:00, queued_by_job_id=2, pid=41860[0m
[[34m2023-09-11T06:48:12.628+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-20T00:00:00+00:00, run_after=2023-04-21T00:00:00+00:00[0m
[[34m2023-09-11T06:48:12.650+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-19 00:00:00+00:00: scheduled__2023-04-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:08.819089+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:12.650+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-19 00:00:00+00:00, run_id=scheduled__2023-04-19T00:00:00+00:00, run_start_date=2023-09-11 06:48:08.838532+00:00, run_end_date=2023-09-11 06:48:12.650639+00:00, run_duration=3.812107, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-19 00:00:00+00:00, data_interval_end=2023-04-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:12.653+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-20T00:00:00+00:00, run_after=2023-04-21T00:00:00+00:00[0m
[[34m2023-09-11T06:48:13.785+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-21T00:00:00+00:00, run_after=2023-04-22T00:00:00+00:00[0m
[[34m2023-09-11T06:48:13.837+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:13.838+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:13.838+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:13.840+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:13.841+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:13.842+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:13.844+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:15.850+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:15.980+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:16.180+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:16.225+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:16.708+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:16.708+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:16.778+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:16.850+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:17.649+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:17.660+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:16.944587+00:00, run_end_date=2023-09-11 06:48:17.169610+00:00, run_duration=0.225023, state=success, executor_state=success, try_number=1, max_tries=0, job_id=114, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:13.839308+00:00, queued_by_job_id=2, pid=41870[0m
[[34m2023-09-11T06:48:17.945+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-22T00:00:00+00:00, run_after=2023-04-23T00:00:00+00:00[0m
[[34m2023-09-11T06:48:17.984+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-20 00:00:00+00:00: scheduled__2023-04-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:13.778159+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:17.984+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-20 00:00:00+00:00, run_id=scheduled__2023-04-20T00:00:00+00:00, run_start_date=2023-09-11 06:48:13.799848+00:00, run_end_date=2023-09-11 06:48:17.984680+00:00, run_duration=4.184832, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-20 00:00:00+00:00, data_interval_end=2023-04-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:17.988+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-21T00:00:00+00:00, run_after=2023-04-22T00:00:00+00:00[0m
[[34m2023-09-11T06:48:18.005+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:18.005+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:18.006+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:18.008+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:18.008+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:18.009+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:18.011+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:20.089+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:20.254+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:20.454+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:20.494+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:21.058+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:21.058+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:21.129+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:21.187+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:21.903+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:21.914+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:21.271393+00:00, run_end_date=2023-09-11 06:48:21.495099+00:00, run_duration=0.223706, state=success, executor_state=success, try_number=1, max_tries=0, job_id=115, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:18.006728+00:00, queued_by_job_id=2, pid=41879[0m
[[34m2023-09-11T06:48:22.160+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-22T00:00:00+00:00, run_after=2023-04-23T00:00:00+00:00[0m
[[34m2023-09-11T06:48:22.196+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-21 00:00:00+00:00: scheduled__2023-04-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:17.940277+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:22.197+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-21 00:00:00+00:00, run_id=scheduled__2023-04-21T00:00:00+00:00, run_start_date=2023-09-11 06:48:17.959819+00:00, run_end_date=2023-09-11 06:48:22.197039+00:00, run_duration=4.23722, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-21 00:00:00+00:00, data_interval_end=2023-04-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:22.200+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-22T00:00:00+00:00, run_after=2023-04-23T00:00:00+00:00[0m
[[34m2023-09-11T06:48:22.958+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-23T00:00:00+00:00, run_after=2023-04-24T00:00:00+00:00[0m
[[34m2023-09-11T06:48:23.006+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:23.006+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:23.007+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:23.009+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:23.010+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:23.010+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:23.014+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:24.985+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:25.117+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:25.290+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:25.322+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:25.789+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:25.790+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:25.860+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:25.923+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:26.626+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:26.637+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:26.007667+00:00, run_end_date=2023-09-11 06:48:26.230050+00:00, run_duration=0.222383, state=success, executor_state=success, try_number=1, max_tries=0, job_id=116, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:23.007861+00:00, queued_by_job_id=2, pid=41889[0m
[[34m2023-09-11T06:48:26.904+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-24T00:00:00+00:00, run_after=2023-04-25T00:00:00+00:00[0m
[[34m2023-09-11T06:48:26.940+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-22 00:00:00+00:00: scheduled__2023-04-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:22.953765+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:26.941+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-22 00:00:00+00:00, run_id=scheduled__2023-04-22T00:00:00+00:00, run_start_date=2023-09-11 06:48:22.970873+00:00, run_end_date=2023-09-11 06:48:26.941361+00:00, run_duration=3.970488, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-22 00:00:00+00:00, data_interval_end=2023-04-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:26.944+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-23T00:00:00+00:00, run_after=2023-04-24T00:00:00+00:00[0m
[[34m2023-09-11T06:48:26.960+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:26.960+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:26.961+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:26.963+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:26.963+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:26.964+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:26.966+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:28.822+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:28.955+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:29.125+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:29.158+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:29.634+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:29.635+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:29.708+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:29.767+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:30.510+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:30.521+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:29.850728+00:00, run_end_date=2023-09-11 06:48:30.077431+00:00, run_duration=0.226703, state=success, executor_state=success, try_number=1, max_tries=0, job_id=117, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:26.961810+00:00, queued_by_job_id=2, pid=41898[0m
[[34m2023-09-11T06:48:30.767+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-24T00:00:00+00:00, run_after=2023-04-25T00:00:00+00:00[0m
[[34m2023-09-11T06:48:30.790+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-23 00:00:00+00:00: scheduled__2023-04-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:26.899461+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:30.790+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-23 00:00:00+00:00, run_id=scheduled__2023-04-23T00:00:00+00:00, run_start_date=2023-09-11 06:48:26.917969+00:00, run_end_date=2023-09-11 06:48:30.790840+00:00, run_duration=3.872871, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-23 00:00:00+00:00, data_interval_end=2023-04-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:30.794+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-24T00:00:00+00:00, run_after=2023-04-25T00:00:00+00:00[0m
[[34m2023-09-11T06:48:31.902+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-25T00:00:00+00:00, run_after=2023-04-26T00:00:00+00:00[0m
[[34m2023-09-11T06:48:31.945+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:31.945+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:31.945+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:31.948+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:31.948+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:31.949+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:31.951+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:33.790+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:33.916+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:34.109+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:34.142+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:34.620+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:34.620+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:34.691+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:34.747+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:35.465+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:35.475+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:34.835777+00:00, run_end_date=2023-09-11 06:48:35.065233+00:00, run_duration=0.229456, state=success, executor_state=success, try_number=1, max_tries=0, job_id=118, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:31.946582+00:00, queued_by_job_id=2, pid=41908[0m
[[34m2023-09-11T06:48:35.742+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-26T00:00:00+00:00, run_after=2023-04-27T00:00:00+00:00[0m
[[34m2023-09-11T06:48:35.778+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-24 00:00:00+00:00: scheduled__2023-04-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:31.898015+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:35.778+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-24 00:00:00+00:00, run_id=scheduled__2023-04-24T00:00:00+00:00, run_start_date=2023-09-11 06:48:31.914937+00:00, run_end_date=2023-09-11 06:48:35.778698+00:00, run_duration=3.863761, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-24 00:00:00+00:00, data_interval_end=2023-04-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:35.782+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-25T00:00:00+00:00, run_after=2023-04-26T00:00:00+00:00[0m
[[34m2023-09-11T06:48:35.797+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:35.797+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:35.798+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:35.800+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:35.801+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:35.801+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:35.804+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:37.658+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:37.793+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:37.975+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:38.009+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:38.484+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:38.485+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:38.557+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:38.615+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:39.330+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:39.340+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:38.700203+00:00, run_end_date=2023-09-11 06:48:38.947325+00:00, run_duration=0.247122, state=success, executor_state=success, try_number=1, max_tries=0, job_id=119, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:35.798948+00:00, queued_by_job_id=2, pid=41917[0m
[[34m2023-09-11T06:48:39.598+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-27T00:00:00+00:00, run_after=2023-04-28T00:00:00+00:00[0m
[[34m2023-09-11T06:48:39.634+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-25 00:00:00+00:00: scheduled__2023-04-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:35.737150+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:39.635+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-25 00:00:00+00:00, run_id=scheduled__2023-04-25T00:00:00+00:00, run_start_date=2023-09-11 06:48:35.755007+00:00, run_end_date=2023-09-11 06:48:39.635075+00:00, run_duration=3.880068, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-25 00:00:00+00:00, data_interval_end=2023-04-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:39.638+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-26T00:00:00+00:00, run_after=2023-04-27T00:00:00+00:00[0m
[[34m2023-09-11T06:48:39.652+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:39.653+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:39.653+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:39.655+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:39.656+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:39.656+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:39.659+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:41.502+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:41.632+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:41.806+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:41.838+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:42.317+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:42.318+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:42.387+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:42.452+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:43.153+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:43.164+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:42.535768+00:00, run_end_date=2023-09-11 06:48:42.756913+00:00, run_duration=0.221145, state=success, executor_state=success, try_number=1, max_tries=0, job_id=120, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:39.654146+00:00, queued_by_job_id=2, pid=41925[0m
[[34m2023-09-11T06:48:43.428+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-27T00:00:00+00:00, run_after=2023-04-28T00:00:00+00:00[0m
[[34m2023-09-11T06:48:43.450+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-26 00:00:00+00:00: scheduled__2023-04-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:39.593152+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:43.451+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-26 00:00:00+00:00, run_id=scheduled__2023-04-26T00:00:00+00:00, run_start_date=2023-09-11 06:48:39.612716+00:00, run_end_date=2023-09-11 06:48:43.451171+00:00, run_duration=3.838455, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-26 00:00:00+00:00, data_interval_end=2023-04-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:43.463+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-27T00:00:00+00:00, run_after=2023-04-28T00:00:00+00:00[0m
[[34m2023-09-11T06:48:44.735+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-28T00:00:00+00:00, run_after=2023-04-29T00:00:00+00:00[0m
[[34m2023-09-11T06:48:44.792+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:44.792+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:44.792+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:44.795+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:44.795+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:44.795+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:44.798+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:46.625+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:46.755+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:46.934+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:46.967+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:47.468+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:47.469+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:47.552+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:47.619+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:48.361+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:48.372+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:47.706966+00:00, run_end_date=2023-09-11 06:48:47.967308+00:00, run_duration=0.260342, state=success, executor_state=success, try_number=1, max_tries=0, job_id=121, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:44.793714+00:00, queued_by_job_id=2, pid=41935[0m
[[34m2023-09-11T06:48:48.685+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-29T00:00:00+00:00, run_after=2023-04-30T00:00:00+00:00[0m
[[34m2023-09-11T06:48:48.723+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-27 00:00:00+00:00: scheduled__2023-04-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:44.729524+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:48.723+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-27 00:00:00+00:00, run_id=scheduled__2023-04-27T00:00:00+00:00, run_start_date=2023-09-11 06:48:44.748010+00:00, run_end_date=2023-09-11 06:48:48.723381+00:00, run_duration=3.975371, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-27 00:00:00+00:00, data_interval_end=2023-04-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:48.727+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-28T00:00:00+00:00, run_after=2023-04-29T00:00:00+00:00[0m
[[34m2023-09-11T06:48:48.742+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:48.743+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:48.743+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:48.745+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:48.746+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:48.746+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:48.749+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:50.633+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:50.772+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:50.958+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:50.991+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:51.466+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:51.467+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:51.536+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:51.600+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:52.339+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:52.351+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:51.685337+00:00, run_end_date=2023-09-11 06:48:51.920050+00:00, run_duration=0.234713, state=success, executor_state=success, try_number=1, max_tries=0, job_id=122, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:48.744108+00:00, queued_by_job_id=2, pid=41942[0m
[[34m2023-09-11T06:48:52.511+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-29T00:00:00+00:00, run_after=2023-04-30T00:00:00+00:00[0m
[[34m2023-09-11T06:48:52.533+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-28 00:00:00+00:00: scheduled__2023-04-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:48.680011+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:52.534+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-28 00:00:00+00:00, run_id=scheduled__2023-04-28T00:00:00+00:00, run_start_date=2023-09-11 06:48:48.698791+00:00, run_end_date=2023-09-11 06:48:52.534144+00:00, run_duration=3.835353, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-28 00:00:00+00:00, data_interval_end=2023-04-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:52.537+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-29T00:00:00+00:00, run_after=2023-04-30T00:00:00+00:00[0m
[[34m2023-09-11T06:48:53.440+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-30T00:00:00+00:00, run_after=2023-05-01T00:00:00+00:00[0m
[[34m2023-09-11T06:48:53.490+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:53.490+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:53.491+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:53.493+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:53.494+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:53.494+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:53.496+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:55.420+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:55.597+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:55.769+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:55.802+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:56.278+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:48:56.279+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:56.351+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:48:56.410+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:48:57.253+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:48:57.264+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:48:56.501019+00:00, run_end_date=2023-09-11 06:48:56.730100+00:00, run_duration=0.229081, state=success, executor_state=success, try_number=1, max_tries=0, job_id=123, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:53.491749+00:00, queued_by_job_id=2, pid=41952[0m
[[34m2023-09-11T06:48:57.455+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-01T00:00:00+00:00, run_after=2023-05-02T00:00:00+00:00[0m
[[34m2023-09-11T06:48:57.491+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-29 00:00:00+00:00: scheduled__2023-04-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:53.435579+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:48:57.492+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-29 00:00:00+00:00, run_id=scheduled__2023-04-29T00:00:00+00:00, run_start_date=2023-09-11 06:48:53.452714+00:00, run_end_date=2023-09-11 06:48:57.492193+00:00, run_duration=4.039479, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-29 00:00:00+00:00, data_interval_end=2023-04-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:48:57.496+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-04-30T00:00:00+00:00, run_after=2023-05-01T00:00:00+00:00[0m
[[34m2023-09-11T06:48:57.511+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:57.512+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:48:57.512+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-04-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:48:57.514+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:48:57.515+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:48:57.515+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:57.518+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-04-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:48:59.353+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:48:59.486+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:48:59.681+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:48:59.714+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:00.184+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:00.185+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:00.261+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-04-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:00.318+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-04-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:01.027+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-04-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:01.038+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-04-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:00.403296+00:00, run_end_date=2023-09-11 06:49:00.633946+00:00, run_duration=0.23065, state=success, executor_state=success, try_number=1, max_tries=0, job_id=124, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:48:57.513246+00:00, queued_by_job_id=2, pid=41961[0m
[[34m2023-09-11T06:49:01.181+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-01T00:00:00+00:00, run_after=2023-05-02T00:00:00+00:00[0m
[[34m2023-09-11T06:49:01.203+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-04-30 00:00:00+00:00: scheduled__2023-04-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:48:57.450152+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:01.204+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-04-30 00:00:00+00:00, run_id=scheduled__2023-04-30T00:00:00+00:00, run_start_date=2023-09-11 06:48:57.468381+00:00, run_end_date=2023-09-11 06:49:01.203946+00:00, run_duration=3.735565, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-04-30 00:00:00+00:00, data_interval_end=2023-05-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:01.207+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-01T00:00:00+00:00, run_after=2023-05-02T00:00:00+00:00[0m
[[34m2023-09-11T06:49:02.701+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-02T00:00:00+00:00, run_after=2023-05-03T00:00:00+00:00[0m
[[34m2023-09-11T06:49:02.751+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:02.751+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:02.752+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:02.754+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:02.754+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:02.755+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:02.757+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:04.798+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:04.949+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:05.125+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:05.158+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:05.623+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:05.623+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:05.693+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:05.753+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:06.455+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:06.465+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:05.839629+00:00, run_end_date=2023-09-11 06:49:06.067037+00:00, run_duration=0.227408, state=success, executor_state=success, try_number=1, max_tries=0, job_id=125, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:02.752904+00:00, queued_by_job_id=2, pid=41971[0m
[[34m2023-09-11T06:49:06.631+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-03T00:00:00+00:00, run_after=2023-05-04T00:00:00+00:00[0m
[[34m2023-09-11T06:49:06.666+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-01 00:00:00+00:00: scheduled__2023-05-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:02.695390+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:06.666+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-01 00:00:00+00:00, run_id=scheduled__2023-05-01T00:00:00+00:00, run_start_date=2023-09-11 06:49:02.718034+00:00, run_end_date=2023-09-11 06:49:06.666656+00:00, run_duration=3.948622, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-01 00:00:00+00:00, data_interval_end=2023-05-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:06.670+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-02T00:00:00+00:00, run_after=2023-05-03T00:00:00+00:00[0m
[[34m2023-09-11T06:49:06.685+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:06.685+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:06.685+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:06.687+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:06.688+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:06.688+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:06.691+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:08.523+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:08.652+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:08.820+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:08.853+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:09.331+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:09.332+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:09.404+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:09.463+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:10.185+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:10.196+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:09.554222+00:00, run_end_date=2023-09-11 06:49:09.788598+00:00, run_duration=0.234376, state=success, executor_state=success, try_number=1, max_tries=0, job_id=126, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:06.686434+00:00, queued_by_job_id=2, pid=41980[0m
[[34m2023-09-11T06:49:10.447+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-03T00:00:00+00:00, run_after=2023-05-04T00:00:00+00:00[0m
[[34m2023-09-11T06:49:10.469+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-02 00:00:00+00:00: scheduled__2023-05-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:06.626026+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:10.470+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-02 00:00:00+00:00, run_id=scheduled__2023-05-02T00:00:00+00:00, run_start_date=2023-09-11 06:49:06.643503+00:00, run_end_date=2023-09-11 06:49:10.470258+00:00, run_duration=3.826755, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-02 00:00:00+00:00, data_interval_end=2023-05-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:10.473+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-03T00:00:00+00:00, run_after=2023-05-04T00:00:00+00:00[0m
[[34m2023-09-11T06:49:11.737+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-04T00:00:00+00:00, run_after=2023-05-05T00:00:00+00:00[0m
[[34m2023-09-11T06:49:11.782+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:11.782+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:11.783+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:11.785+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:11.786+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:11.786+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:11.789+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:13.642+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:13.774+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:13.953+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:13.990+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:14.447+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:14.447+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:14.516+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:14.571+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:15.276+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:15.287+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:14.656968+00:00, run_end_date=2023-09-11 06:49:14.876580+00:00, run_duration=0.219612, state=success, executor_state=success, try_number=1, max_tries=0, job_id=127, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:11.783876+00:00, queued_by_job_id=2, pid=41990[0m
[[34m2023-09-11T06:49:15.548+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-05T00:00:00+00:00, run_after=2023-05-06T00:00:00+00:00[0m
[[34m2023-09-11T06:49:15.592+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-03 00:00:00+00:00: scheduled__2023-05-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:11.732523+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:15.593+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-03 00:00:00+00:00, run_id=scheduled__2023-05-03T00:00:00+00:00, run_start_date=2023-09-11 06:49:11.750544+00:00, run_end_date=2023-09-11 06:49:15.592951+00:00, run_duration=3.842407, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-03 00:00:00+00:00, data_interval_end=2023-05-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:15.596+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-04T00:00:00+00:00, run_after=2023-05-05T00:00:00+00:00[0m
[[34m2023-09-11T06:49:15.613+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:15.613+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:15.613+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:15.615+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:15.616+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:15.616+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:15.619+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:17.442+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:17.609+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:17.800+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:17.832+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:18.316+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:18.317+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:18.385+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:18.448+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:19.163+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:19.173+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:18.536073+00:00, run_end_date=2023-09-11 06:49:18.752322+00:00, run_duration=0.216249, state=success, executor_state=success, try_number=1, max_tries=0, job_id=128, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:15.614500+00:00, queued_by_job_id=2, pid=41999[0m
[[34m2023-09-11T06:49:19.459+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-05T00:00:00+00:00, run_after=2023-05-06T00:00:00+00:00[0m
[[34m2023-09-11T06:49:19.483+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-04 00:00:00+00:00: scheduled__2023-05-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:15.542899+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:19.483+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-04 00:00:00+00:00, run_id=scheduled__2023-05-04T00:00:00+00:00, run_start_date=2023-09-11 06:49:15.569526+00:00, run_end_date=2023-09-11 06:49:19.483368+00:00, run_duration=3.913842, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-04 00:00:00+00:00, data_interval_end=2023-05-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:19.486+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-05T00:00:00+00:00, run_after=2023-05-06T00:00:00+00:00[0m
[[34m2023-09-11T06:49:20.459+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-06T00:00:00+00:00, run_after=2023-05-07T00:00:00+00:00[0m
[[34m2023-09-11T06:49:20.502+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:20.502+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:20.503+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:20.505+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:20.505+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:20.505+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:20.508+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:22.313+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:22.439+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:22.613+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:22.646+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:23.116+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:23.117+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:23.184+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:23.243+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:24.003+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:24.013+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:23.331490+00:00, run_end_date=2023-09-11 06:49:23.553595+00:00, run_duration=0.222105, state=success, executor_state=success, try_number=1, max_tries=0, job_id=129, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:20.503801+00:00, queued_by_job_id=2, pid=42007[0m
[[34m2023-09-11T06:49:24.331+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-07T00:00:00+00:00, run_after=2023-05-08T00:00:00+00:00[0m
[[34m2023-09-11T06:49:24.367+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-05 00:00:00+00:00: scheduled__2023-05-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:20.454440+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:24.367+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-05 00:00:00+00:00, run_id=scheduled__2023-05-05T00:00:00+00:00, run_start_date=2023-09-11 06:49:20.472450+00:00, run_end_date=2023-09-11 06:49:24.367362+00:00, run_duration=3.894912, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-05 00:00:00+00:00, data_interval_end=2023-05-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:24.371+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-06T00:00:00+00:00, run_after=2023-05-07T00:00:00+00:00[0m
[[34m2023-09-11T06:49:24.386+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:24.386+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:24.387+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:24.394+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:24.395+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:24.395+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:24.398+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:26.222+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:26.354+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:26.527+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:26.558+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:27.032+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:27.032+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:27.101+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:27.165+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:27.892+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:27.903+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:27.251609+00:00, run_end_date=2023-09-11 06:49:27.477876+00:00, run_duration=0.226267, state=success, executor_state=success, try_number=1, max_tries=0, job_id=130, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:24.387701+00:00, queued_by_job_id=2, pid=42014[0m
[[34m2023-09-11T06:49:28.044+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-07T00:00:00+00:00, run_after=2023-05-08T00:00:00+00:00[0m
[[34m2023-09-11T06:49:28.067+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-06 00:00:00+00:00: scheduled__2023-05-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:24.325735+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:28.067+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-06 00:00:00+00:00, run_id=scheduled__2023-05-06T00:00:00+00:00, run_start_date=2023-09-11 06:49:24.344001+00:00, run_end_date=2023-09-11 06:49:28.067697+00:00, run_duration=3.723696, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-06 00:00:00+00:00, data_interval_end=2023-05-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:28.071+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-07T00:00:00+00:00, run_after=2023-05-08T00:00:00+00:00[0m
[[34m2023-09-11T06:49:29.312+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-08T00:00:00+00:00, run_after=2023-05-09T00:00:00+00:00[0m
[[34m2023-09-11T06:49:29.357+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:29.357+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:29.357+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:29.359+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:29.360+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:29.360+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:29.363+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:31.190+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:31.321+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:31.502+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:31.535+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:31.994+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:31.995+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:32.064+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:32.124+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:32.821+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:32.832+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:32.209655+00:00, run_end_date=2023-09-11 06:49:32.435671+00:00, run_duration=0.226016, state=success, executor_state=success, try_number=1, max_tries=0, job_id=131, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:29.358437+00:00, queued_by_job_id=2, pid=42024[0m
[[34m2023-09-11T06:49:33.086+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-09T00:00:00+00:00, run_after=2023-05-10T00:00:00+00:00[0m
[[34m2023-09-11T06:49:33.123+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-07 00:00:00+00:00: scheduled__2023-05-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:29.308002+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:33.123+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-07 00:00:00+00:00, run_id=scheduled__2023-05-07T00:00:00+00:00, run_start_date=2023-09-11 06:49:29.325733+00:00, run_end_date=2023-09-11 06:49:33.123671+00:00, run_duration=3.797938, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-07 00:00:00+00:00, data_interval_end=2023-05-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:33.127+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-08T00:00:00+00:00, run_after=2023-05-09T00:00:00+00:00[0m
[[34m2023-09-11T06:49:33.142+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:33.142+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:33.142+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:33.145+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:33.145+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:33.145+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:33.148+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:34.985+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:35.110+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:35.286+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:35.322+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:35.794+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:35.795+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:35.865+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:35.934+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:36.631+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:36.642+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:36.019951+00:00, run_end_date=2023-09-11 06:49:36.240417+00:00, run_duration=0.220466, state=success, executor_state=success, try_number=1, max_tries=0, job_id=132, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:33.143529+00:00, queued_by_job_id=2, pid=42033[0m
[[34m2023-09-11T06:49:36.892+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-09T00:00:00+00:00, run_after=2023-05-10T00:00:00+00:00[0m
[[34m2023-09-11T06:49:36.915+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-08 00:00:00+00:00: scheduled__2023-05-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:33.082240+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:36.915+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-08 00:00:00+00:00, run_id=scheduled__2023-05-08T00:00:00+00:00, run_start_date=2023-09-11 06:49:33.099053+00:00, run_end_date=2023-09-11 06:49:36.915501+00:00, run_duration=3.816448, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-08 00:00:00+00:00, data_interval_end=2023-05-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:36.919+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-09T00:00:00+00:00, run_after=2023-05-10T00:00:00+00:00[0m
[[34m2023-09-11T06:49:38.267+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-10T00:00:00+00:00, run_after=2023-05-11T00:00:00+00:00[0m
[[34m2023-09-11T06:49:38.311+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:38.312+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:38.312+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:38.314+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:38.314+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:38.315+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:38.317+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:40.146+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:40.276+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:40.447+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:40.480+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:40.947+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:40.947+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:41.017+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:41.075+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:41.772+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:41.783+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:41.160381+00:00, run_end_date=2023-09-11 06:49:41.384497+00:00, run_duration=0.224116, state=success, executor_state=success, try_number=1, max_tries=0, job_id=133, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:38.312985+00:00, queued_by_job_id=2, pid=42043[0m
[[34m2023-09-11T06:49:42.497+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-11T00:00:00+00:00, run_after=2023-05-12T00:00:00+00:00[0m
[[34m2023-09-11T06:49:42.531+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-09 00:00:00+00:00: scheduled__2023-05-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:38.261638+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:42.531+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-09 00:00:00+00:00, run_id=scheduled__2023-05-09T00:00:00+00:00, run_start_date=2023-09-11 06:49:38.281082+00:00, run_end_date=2023-09-11 06:49:42.531686+00:00, run_duration=4.250604, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-09 00:00:00+00:00, data_interval_end=2023-05-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:42.535+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-10T00:00:00+00:00, run_after=2023-05-11T00:00:00+00:00[0m
[[34m2023-09-11T06:49:42.554+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:42.555+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:42.555+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:42.557+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:42.558+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:42.558+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:42.561+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:44.371+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:44.504+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:44.674+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:44.705+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:45.175+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:45.176+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:45.247+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:45.306+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:46.091+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:46.102+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:45.392761+00:00, run_end_date=2023-09-11 06:49:45.612979+00:00, run_duration=0.220218, state=success, executor_state=success, try_number=1, max_tries=0, job_id=134, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:42.556205+00:00, queued_by_job_id=2, pid=42052[0m
[[34m2023-09-11T06:49:46.370+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-11T00:00:00+00:00, run_after=2023-05-12T00:00:00+00:00[0m
[[34m2023-09-11T06:49:46.397+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-10 00:00:00+00:00: scheduled__2023-05-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:42.491998+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:46.398+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-10 00:00:00+00:00, run_id=scheduled__2023-05-10T00:00:00+00:00, run_start_date=2023-09-11 06:49:42.509583+00:00, run_end_date=2023-09-11 06:49:46.397941+00:00, run_duration=3.888358, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-10 00:00:00+00:00, data_interval_end=2023-05-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:46.401+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-11T00:00:00+00:00, run_after=2023-05-12T00:00:00+00:00[0m
[[34m2023-09-11T06:49:47.065+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-12T00:00:00+00:00, run_after=2023-05-13T00:00:00+00:00[0m
[[34m2023-09-11T06:49:47.108+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:47.109+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:47.109+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:47.111+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:47.112+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:47.112+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:47.115+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:48.967+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:49.091+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:49.262+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:49.296+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:49.760+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:49.761+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:49.830+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:49.885+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:50.585+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:50.595+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:49.968152+00:00, run_end_date=2023-09-11 06:49:50.181723+00:00, run_duration=0.213571, state=success, executor_state=success, try_number=1, max_tries=0, job_id=135, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:47.110019+00:00, queued_by_job_id=2, pid=42062[0m
[[34m2023-09-11T06:49:51.693+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-13T00:00:00+00:00, run_after=2023-05-14T00:00:00+00:00[0m
[[34m2023-09-11T06:49:51.726+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-11 00:00:00+00:00: scheduled__2023-05-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:47.060137+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:51.726+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-11 00:00:00+00:00, run_id=scheduled__2023-05-11T00:00:00+00:00, run_start_date=2023-09-11 06:49:47.076889+00:00, run_end_date=2023-09-11 06:49:51.726771+00:00, run_duration=4.649882, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-11 00:00:00+00:00, data_interval_end=2023-05-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:51.730+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-12T00:00:00+00:00, run_after=2023-05-13T00:00:00+00:00[0m
[[34m2023-09-11T06:49:51.744+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:51.745+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:51.745+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:51.748+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:51.749+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:51.749+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:51.752+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:53.594+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:53.732+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:53.910+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:53.942+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:54.409+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:54.410+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:54.480+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:54.544+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:49:55.239+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:49:55.251+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:49:54.630023+00:00, run_end_date=2023-09-11 06:49:54.849055+00:00, run_duration=0.219032, state=success, executor_state=success, try_number=1, max_tries=0, job_id=136, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:51.746373+00:00, queued_by_job_id=2, pid=42071[0m
[[34m2023-09-11T06:49:55.796+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-13T00:00:00+00:00, run_after=2023-05-14T00:00:00+00:00[0m
[[34m2023-09-11T06:49:55.819+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-12 00:00:00+00:00: scheduled__2023-05-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:51.688021+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:49:55.820+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-12 00:00:00+00:00, run_id=scheduled__2023-05-12T00:00:00+00:00, run_start_date=2023-09-11 06:49:51.705110+00:00, run_end_date=2023-09-11 06:49:55.819841+00:00, run_duration=4.114731, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-12 00:00:00+00:00, data_interval_end=2023-05-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:49:55.823+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-13T00:00:00+00:00, run_after=2023-05-14T00:00:00+00:00[0m
[[34m2023-09-11T06:49:56.691+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-14T00:00:00+00:00, run_after=2023-05-15T00:00:00+00:00[0m
[[34m2023-09-11T06:49:56.735+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:56.735+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:49:56.735+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:49:56.737+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:49:56.738+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:49:56.738+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:56.741+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:49:58.910+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:49:59.058+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:59.267+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:59.303+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:49:59.770+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:49:59.771+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:49:59.839+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:49:59.908+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:00.661+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:00.672+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:00.000840+00:00, run_end_date=2023-09-11 06:50:00.221774+00:00, run_duration=0.220934, state=success, executor_state=success, try_number=1, max_tries=0, job_id=137, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:49:56.736431+00:00, queued_by_job_id=2, pid=42082[0m
[[34m2023-09-11T06:50:00.701+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:50:01.147+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-15T00:00:00+00:00, run_after=2023-05-16T00:00:00+00:00[0m
[[34m2023-09-11T06:50:01.191+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-13 00:00:00+00:00: scheduled__2023-05-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:49:56.686710+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:01.193+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-13 00:00:00+00:00, run_id=scheduled__2023-05-13T00:00:00+00:00, run_start_date=2023-09-11 06:49:56.703420+00:00, run_end_date=2023-09-11 06:50:01.193295+00:00, run_duration=4.489875, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-13 00:00:00+00:00, data_interval_end=2023-05-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:01.199+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-14T00:00:00+00:00, run_after=2023-05-15T00:00:00+00:00[0m
[[34m2023-09-11T06:50:01.220+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:01.221+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:01.221+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:01.225+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:01.226+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:01.226+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:01.229+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:03.137+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:03.272+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:03.449+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:03.486+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:03.984+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:03.985+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:04.054+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:04.113+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:04.873+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:04.884+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:04.211560+00:00, run_end_date=2023-09-11 06:50:04.461743+00:00, run_duration=0.250183, state=success, executor_state=success, try_number=1, max_tries=0, job_id=138, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:01.222656+00:00, queued_by_job_id=2, pid=42091[0m
[[34m2023-09-11T06:50:05.288+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-15T00:00:00+00:00, run_after=2023-05-16T00:00:00+00:00[0m
[[34m2023-09-11T06:50:05.311+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-14 00:00:00+00:00: scheduled__2023-05-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:01.142939+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:05.312+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-14 00:00:00+00:00, run_id=scheduled__2023-05-14T00:00:00+00:00, run_start_date=2023-09-11 06:50:01.161715+00:00, run_end_date=2023-09-11 06:50:05.311890+00:00, run_duration=4.150175, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-14 00:00:00+00:00, data_interval_end=2023-05-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:05.315+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-15T00:00:00+00:00, run_after=2023-05-16T00:00:00+00:00[0m
[[34m2023-09-11T06:50:06.102+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-16T00:00:00+00:00, run_after=2023-05-17T00:00:00+00:00[0m
[[34m2023-09-11T06:50:06.146+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:06.147+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:06.147+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:06.149+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:06.150+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:06.150+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:06.153+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:07.999+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:08.126+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:08.300+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:08.333+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:08.803+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:08.803+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:08.873+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:08.930+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:09.667+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:09.678+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:09.015407+00:00, run_end_date=2023-09-11 06:50:09.250457+00:00, run_duration=0.23505, state=success, executor_state=success, try_number=1, max_tries=0, job_id=139, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:06.148204+00:00, queued_by_job_id=2, pid=42101[0m
[[34m2023-09-11T06:50:09.948+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-17T00:00:00+00:00, run_after=2023-05-18T00:00:00+00:00[0m
[[34m2023-09-11T06:50:09.984+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-15 00:00:00+00:00: scheduled__2023-05-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:06.097029+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:09.984+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-15 00:00:00+00:00, run_id=scheduled__2023-05-15T00:00:00+00:00, run_start_date=2023-09-11 06:50:06.114953+00:00, run_end_date=2023-09-11 06:50:09.984468+00:00, run_duration=3.869515, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-15 00:00:00+00:00, data_interval_end=2023-05-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:09.987+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-16T00:00:00+00:00, run_after=2023-05-17T00:00:00+00:00[0m
[[34m2023-09-11T06:50:10.003+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:10.003+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:10.003+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:10.005+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:10.006+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:10.006+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:10.009+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:11.832+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:11.970+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:12.139+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:12.170+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:12.636+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:12.637+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:12.707+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:12.767+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:13.483+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:13.495+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:12.849267+00:00, run_end_date=2023-09-11 06:50:13.065356+00:00, run_duration=0.216089, state=success, executor_state=success, try_number=1, max_tries=0, job_id=140, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:10.004201+00:00, queued_by_job_id=2, pid=42108[0m
[[34m2023-09-11T06:50:13.894+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-17T00:00:00+00:00, run_after=2023-05-18T00:00:00+00:00[0m
[[34m2023-09-11T06:50:13.918+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-16 00:00:00+00:00: scheduled__2023-05-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:09.943684+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:13.919+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-16 00:00:00+00:00, run_id=scheduled__2023-05-16T00:00:00+00:00, run_start_date=2023-09-11 06:50:09.961505+00:00, run_end_date=2023-09-11 06:50:13.919036+00:00, run_duration=3.957531, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-16 00:00:00+00:00, data_interval_end=2023-05-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:13.923+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-17T00:00:00+00:00, run_after=2023-05-18T00:00:00+00:00[0m
[[34m2023-09-11T06:50:15.137+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-18T00:00:00+00:00, run_after=2023-05-19T00:00:00+00:00[0m
[[34m2023-09-11T06:50:15.181+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:15.182+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:15.182+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:15.184+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:15.185+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:15.185+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:15.187+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:17.011+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:17.141+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:17.317+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:17.350+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:17.861+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:17.862+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:17.933+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:17.989+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:18.713+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:18.723+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:18.082606+00:00, run_end_date=2023-09-11 06:50:18.307204+00:00, run_duration=0.224598, state=success, executor_state=success, try_number=1, max_tries=0, job_id=141, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:15.182944+00:00, queued_by_job_id=2, pid=42118[0m
[[34m2023-09-11T06:50:19.253+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-19T00:00:00+00:00, run_after=2023-05-20T00:00:00+00:00[0m
[[34m2023-09-11T06:50:19.288+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-17 00:00:00+00:00: scheduled__2023-05-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:15.133011+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:19.288+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-17 00:00:00+00:00, run_id=scheduled__2023-05-17T00:00:00+00:00, run_start_date=2023-09-11 06:50:15.149518+00:00, run_end_date=2023-09-11 06:50:19.288709+00:00, run_duration=4.139191, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-17 00:00:00+00:00, data_interval_end=2023-05-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:19.293+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-18T00:00:00+00:00, run_after=2023-05-19T00:00:00+00:00[0m
[[34m2023-09-11T06:50:19.308+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:19.309+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:19.309+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:19.311+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:19.312+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:19.312+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:19.315+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:21.139+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:21.268+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:21.437+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:21.469+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:21.929+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:21.929+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:22.000+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:22.057+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:22.757+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:22.767+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:22.143861+00:00, run_end_date=2023-09-11 06:50:22.363800+00:00, run_duration=0.219939, state=success, executor_state=success, try_number=1, max_tries=0, job_id=142, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:19.310022+00:00, queued_by_job_id=2, pid=42125[0m
[[34m2023-09-11T06:50:23.110+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-19T00:00:00+00:00, run_after=2023-05-20T00:00:00+00:00[0m
[[34m2023-09-11T06:50:23.136+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-18 00:00:00+00:00: scheduled__2023-05-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:19.248200+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:23.137+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-18 00:00:00+00:00, run_id=scheduled__2023-05-18T00:00:00+00:00, run_start_date=2023-09-11 06:50:19.265971+00:00, run_end_date=2023-09-11 06:50:23.137316+00:00, run_duration=3.871345, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-18 00:00:00+00:00, data_interval_end=2023-05-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:23.141+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-19T00:00:00+00:00, run_after=2023-05-20T00:00:00+00:00[0m
[[34m2023-09-11T06:50:24.247+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-20T00:00:00+00:00, run_after=2023-05-21T00:00:00+00:00[0m
[[34m2023-09-11T06:50:24.292+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:24.292+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:24.293+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:24.295+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:24.296+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:24.296+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:24.299+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:26.136+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:26.263+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:26.436+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:26.467+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:26.932+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:26.933+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:27.001+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:27.057+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:27.756+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:27.766+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:27.141939+00:00, run_end_date=2023-09-11 06:50:27.362327+00:00, run_duration=0.220388, state=success, executor_state=success, try_number=1, max_tries=0, job_id=143, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:24.293802+00:00, queued_by_job_id=2, pid=42135[0m
[[34m2023-09-11T06:50:27.937+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-21T00:00:00+00:00, run_after=2023-05-22T00:00:00+00:00[0m
[[34m2023-09-11T06:50:27.974+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-19 00:00:00+00:00: scheduled__2023-05-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:24.242204+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:27.975+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-19 00:00:00+00:00, run_id=scheduled__2023-05-19T00:00:00+00:00, run_start_date=2023-09-11 06:50:24.260589+00:00, run_end_date=2023-09-11 06:50:27.975354+00:00, run_duration=3.714765, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-19 00:00:00+00:00, data_interval_end=2023-05-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:27.978+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-20T00:00:00+00:00, run_after=2023-05-21T00:00:00+00:00[0m
[[34m2023-09-11T06:50:27.993+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:27.994+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:27.994+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:27.996+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:27.996+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:27.997+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:27.999+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:29.837+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:29.968+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:30.141+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:30.173+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:30.643+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:30.643+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:30.713+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:30.771+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:31.486+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:31.497+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:30.854434+00:00, run_end_date=2023-09-11 06:50:31.088527+00:00, run_duration=0.234093, state=success, executor_state=success, try_number=1, max_tries=0, job_id=144, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:27.995016+00:00, queued_by_job_id=2, pid=42144[0m
[[34m2023-09-11T06:50:31.897+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-21T00:00:00+00:00, run_after=2023-05-22T00:00:00+00:00[0m
[[34m2023-09-11T06:50:31.920+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-20 00:00:00+00:00: scheduled__2023-05-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:27.933153+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:31.920+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-20 00:00:00+00:00, run_id=scheduled__2023-05-20T00:00:00+00:00, run_start_date=2023-09-11 06:50:27.951924+00:00, run_end_date=2023-09-11 06:50:31.920451+00:00, run_duration=3.968527, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-20 00:00:00+00:00, data_interval_end=2023-05-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:31.924+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-21T00:00:00+00:00, run_after=2023-05-22T00:00:00+00:00[0m
[[34m2023-09-11T06:50:33.147+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-22T00:00:00+00:00, run_after=2023-05-23T00:00:00+00:00[0m
[[34m2023-09-11T06:50:33.191+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:33.192+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:33.192+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:33.194+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:33.195+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:33.195+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:33.197+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:35.049+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:35.175+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:35.348+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:35.381+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:35.843+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:35.844+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:35.915+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:35.971+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:36.678+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:36.689+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:36.054469+00:00, run_end_date=2023-09-11 06:50:36.269632+00:00, run_duration=0.215163, state=success, executor_state=success, try_number=1, max_tries=0, job_id=145, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:33.193133+00:00, queued_by_job_id=2, pid=42154[0m
[[34m2023-09-11T06:50:36.955+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-23T00:00:00+00:00, run_after=2023-05-24T00:00:00+00:00[0m
[[34m2023-09-11T06:50:36.991+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-21 00:00:00+00:00: scheduled__2023-05-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:33.143451+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:36.992+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-21 00:00:00+00:00, run_id=scheduled__2023-05-21T00:00:00+00:00, run_start_date=2023-09-11 06:50:33.160352+00:00, run_end_date=2023-09-11 06:50:36.992309+00:00, run_duration=3.831957, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-21 00:00:00+00:00, data_interval_end=2023-05-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:36.995+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-22T00:00:00+00:00, run_after=2023-05-23T00:00:00+00:00[0m
[[34m2023-09-11T06:50:37.027+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:37.027+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:37.027+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:37.033+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:37.034+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:37.035+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:37.037+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:38.920+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:39.050+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:39.230+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:39.264+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:39.726+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:39.726+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:39.799+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:39.855+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:40.571+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:40.582+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:39.938382+00:00, run_end_date=2023-09-11 06:50:40.159545+00:00, run_duration=0.221163, state=success, executor_state=success, try_number=1, max_tries=0, job_id=146, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:37.031738+00:00, queued_by_job_id=2, pid=42164[0m
[[34m2023-09-11T06:50:40.942+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-23T00:00:00+00:00, run_after=2023-05-24T00:00:00+00:00[0m
[[34m2023-09-11T06:50:40.965+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-22 00:00:00+00:00: scheduled__2023-05-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:36.950453+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:40.965+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-22 00:00:00+00:00, run_id=scheduled__2023-05-22T00:00:00+00:00, run_start_date=2023-09-11 06:50:36.968861+00:00, run_end_date=2023-09-11 06:50:40.965655+00:00, run_duration=3.996794, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-22 00:00:00+00:00, data_interval_end=2023-05-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:40.969+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-23T00:00:00+00:00, run_after=2023-05-24T00:00:00+00:00[0m
[[34m2023-09-11T06:50:41.771+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-24T00:00:00+00:00, run_after=2023-05-25T00:00:00+00:00[0m
[[34m2023-09-11T06:50:41.816+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:41.816+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:41.816+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:41.818+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:41.819+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:41.819+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:41.822+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:43.650+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:43.780+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:43.967+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:44.000+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:44.463+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:44.464+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:44.545+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:44.625+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:45.358+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:45.369+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:44.734095+00:00, run_end_date=2023-09-11 06:50:44.957772+00:00, run_duration=0.223677, state=success, executor_state=success, try_number=1, max_tries=0, job_id=147, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:41.817322+00:00, queued_by_job_id=2, pid=42174[0m
[[34m2023-09-11T06:50:45.538+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-25T00:00:00+00:00, run_after=2023-05-26T00:00:00+00:00[0m
[[34m2023-09-11T06:50:45.575+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-23 00:00:00+00:00: scheduled__2023-05-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:41.766780+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:45.575+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-23 00:00:00+00:00, run_id=scheduled__2023-05-23T00:00:00+00:00, run_start_date=2023-09-11 06:50:41.784385+00:00, run_end_date=2023-09-11 06:50:45.575634+00:00, run_duration=3.791249, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-23 00:00:00+00:00, data_interval_end=2023-05-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:45.578+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-24T00:00:00+00:00, run_after=2023-05-25T00:00:00+00:00[0m
[[34m2023-09-11T06:50:45.594+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:45.594+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:45.594+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:45.596+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:45.597+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:45.597+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:45.600+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:47.451+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:47.601+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:47.804+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:47.847+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:48.364+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:48.365+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:48.436+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:48.495+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:49.338+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:49.349+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:48.584356+00:00, run_end_date=2023-09-11 06:50:48.829067+00:00, run_duration=0.244711, state=success, executor_state=success, try_number=1, max_tries=0, job_id=148, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:45.595219+00:00, queued_by_job_id=2, pid=42181[0m
[[34m2023-09-11T06:50:49.587+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-25T00:00:00+00:00, run_after=2023-05-26T00:00:00+00:00[0m
[[34m2023-09-11T06:50:49.611+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-24 00:00:00+00:00: scheduled__2023-05-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:45.533773+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:49.612+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-24 00:00:00+00:00, run_id=scheduled__2023-05-24T00:00:00+00:00, run_start_date=2023-09-11 06:50:45.552308+00:00, run_end_date=2023-09-11 06:50:49.612402+00:00, run_duration=4.060094, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-24 00:00:00+00:00, data_interval_end=2023-05-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:49.615+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-25T00:00:00+00:00, run_after=2023-05-26T00:00:00+00:00[0m
[[34m2023-09-11T06:50:50.590+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-26T00:00:00+00:00, run_after=2023-05-27T00:00:00+00:00[0m
[[34m2023-09-11T06:50:50.634+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:50.635+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:50.635+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:50.637+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:50.638+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:50.638+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:50.641+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:52.866+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:53.067+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:53.259+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:53.293+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:53.828+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:53.829+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:53.920+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:53.979+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:54.795+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:54.808+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:54.064896+00:00, run_end_date=2023-09-11 06:50:54.299782+00:00, run_duration=0.234886, state=success, executor_state=success, try_number=1, max_tries=0, job_id=149, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:50.635975+00:00, queued_by_job_id=2, pid=42193[0m
[[34m2023-09-11T06:50:54.973+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-27T00:00:00+00:00, run_after=2023-05-28T00:00:00+00:00[0m
[[34m2023-09-11T06:50:55.025+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-25 00:00:00+00:00: scheduled__2023-05-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:50.585536+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:55.026+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-25 00:00:00+00:00, run_id=scheduled__2023-05-25T00:00:00+00:00, run_start_date=2023-09-11 06:50:50.603199+00:00, run_end_date=2023-09-11 06:50:55.026263+00:00, run_duration=4.423064, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-25 00:00:00+00:00, data_interval_end=2023-05-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:55.029+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-26T00:00:00+00:00, run_after=2023-05-27T00:00:00+00:00[0m
[[34m2023-09-11T06:50:55.045+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:55.045+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:55.045+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:55.048+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:55.048+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:55.048+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:55.051+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:56.881+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:50:57.007+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:57.207+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:57.239+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:50:57.713+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:50:57.714+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:50:57.783+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:50:57.842+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:50:58.557+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:50:58.568+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:50:57.926431+00:00, run_end_date=2023-09-11 06:50:58.147432+00:00, run_duration=0.221001, state=success, executor_state=success, try_number=1, max_tries=0, job_id=150, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:55.046624+00:00, queued_by_job_id=2, pid=42200[0m
[[34m2023-09-11T06:50:58.711+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-27T00:00:00+00:00, run_after=2023-05-28T00:00:00+00:00[0m
[[34m2023-09-11T06:50:58.734+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-26 00:00:00+00:00: scheduled__2023-05-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:54.968349+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:50:58.734+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-26 00:00:00+00:00, run_id=scheduled__2023-05-26T00:00:00+00:00, run_start_date=2023-09-11 06:50:55.003272+00:00, run_end_date=2023-09-11 06:50:58.734685+00:00, run_duration=3.731413, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-26 00:00:00+00:00, data_interval_end=2023-05-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:50:58.738+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-27T00:00:00+00:00, run_after=2023-05-28T00:00:00+00:00[0m
[[34m2023-09-11T06:50:59.913+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-28T00:00:00+00:00, run_after=2023-05-29T00:00:00+00:00[0m
[[34m2023-09-11T06:50:59.967+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:59.967+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:50:59.967+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:50:59.969+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:50:59.970+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:50:59.970+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:50:59.973+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:01.829+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:02.016+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:02.218+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:02.259+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:02.744+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:02.745+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:02.819+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:02.881+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:03.621+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:03.636+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:02.970861+00:00, run_end_date=2023-09-11 06:51:03.204881+00:00, run_duration=0.23402, state=success, executor_state=success, try_number=1, max_tries=0, job_id=151, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:50:59.968231+00:00, queued_by_job_id=2, pid=42210[0m
[[34m2023-09-11T06:51:03.922+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-29T00:00:00+00:00, run_after=2023-05-30T00:00:00+00:00[0m
[[34m2023-09-11T06:51:03.958+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-27 00:00:00+00:00: scheduled__2023-05-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:50:59.909266+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:03.958+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-27 00:00:00+00:00, run_id=scheduled__2023-05-27T00:00:00+00:00, run_start_date=2023-09-11 06:50:59.935286+00:00, run_end_date=2023-09-11 06:51:03.958585+00:00, run_duration=4.023299, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-27 00:00:00+00:00, data_interval_end=2023-05-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:03.962+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-28T00:00:00+00:00, run_after=2023-05-29T00:00:00+00:00[0m
[[34m2023-09-11T06:51:03.979+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:03.980+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:03.980+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:03.982+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:03.983+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:03.983+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:03.986+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:06.286+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:06.448+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:06.681+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:06.743+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:07.531+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:07.532+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:07.661+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:07.777+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:08.788+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:08.800+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:07.881721+00:00, run_end_date=2023-09-11 06:51:08.215722+00:00, run_duration=0.334001, state=success, executor_state=success, try_number=1, max_tries=0, job_id=152, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:03.981072+00:00, queued_by_job_id=2, pid=42219[0m
[[34m2023-09-11T06:51:09.069+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-29T00:00:00+00:00, run_after=2023-05-30T00:00:00+00:00[0m
[[34m2023-09-11T06:51:09.096+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-28 00:00:00+00:00: scheduled__2023-05-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:03.917450+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:09.096+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-28 00:00:00+00:00, run_id=scheduled__2023-05-28T00:00:00+00:00, run_start_date=2023-09-11 06:51:03.935166+00:00, run_end_date=2023-09-11 06:51:09.096502+00:00, run_duration=5.161336, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-28 00:00:00+00:00, data_interval_end=2023-05-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:09.099+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-29T00:00:00+00:00, run_after=2023-05-30T00:00:00+00:00[0m
[[34m2023-09-11T06:51:10.089+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-30T00:00:00+00:00, run_after=2023-05-31T00:00:00+00:00[0m
[[34m2023-09-11T06:51:10.169+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:10.169+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:10.169+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:10.175+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:10.175+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:10.176+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:10.179+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:12.291+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:12.442+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:12.634+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:12.671+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:13.190+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:13.191+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:13.268+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:13.335+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:14.162+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:14.174+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:13.432458+00:00, run_end_date=2023-09-11 06:51:13.701137+00:00, run_duration=0.268679, state=success, executor_state=success, try_number=1, max_tries=0, job_id=153, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:10.172958+00:00, queued_by_job_id=2, pid=42229[0m
[[34m2023-09-11T06:51:14.448+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-31T00:00:00+00:00, run_after=2023-06-01T00:00:00+00:00[0m
[[34m2023-09-11T06:51:14.490+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-29 00:00:00+00:00: scheduled__2023-05-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:10.083529+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:14.490+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-29 00:00:00+00:00, run_id=scheduled__2023-05-29T00:00:00+00:00, run_start_date=2023-09-11 06:51:10.131469+00:00, run_end_date=2023-09-11 06:51:14.490784+00:00, run_duration=4.359315, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-29 00:00:00+00:00, data_interval_end=2023-05-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:14.494+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-30T00:00:00+00:00, run_after=2023-05-31T00:00:00+00:00[0m
[[34m2023-09-11T06:51:14.511+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:14.511+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:14.512+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:14.514+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:14.514+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:14.514+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:14.517+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:16.551+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:16.706+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:16.894+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:16.931+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:17.485+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:17.486+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:17.564+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:17.662+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:18.555+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:18.566+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:17.771515+00:00, run_end_date=2023-09-11 06:51:18.076870+00:00, run_duration=0.305355, state=success, executor_state=success, try_number=1, max_tries=0, job_id=154, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:14.512687+00:00, queued_by_job_id=2, pid=42238[0m
[[34m2023-09-11T06:51:18.818+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-31T00:00:00+00:00, run_after=2023-06-01T00:00:00+00:00[0m
[[34m2023-09-11T06:51:18.845+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-30 00:00:00+00:00: scheduled__2023-05-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:14.443468+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:18.845+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-30 00:00:00+00:00, run_id=scheduled__2023-05-30T00:00:00+00:00, run_start_date=2023-09-11 06:51:14.464065+00:00, run_end_date=2023-09-11 06:51:18.845635+00:00, run_duration=4.38157, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-30 00:00:00+00:00, data_interval_end=2023-05-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:18.849+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-05-31T00:00:00+00:00, run_after=2023-06-01T00:00:00+00:00[0m
[[34m2023-09-11T06:51:19.464+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-01T00:00:00+00:00, run_after=2023-06-02T00:00:00+00:00[0m
[[34m2023-09-11T06:51:19.516+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:19.517+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:19.517+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-05-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:19.520+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:19.522+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:19.522+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:19.525+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:21.605+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:21.750+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:21.942+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:21.978+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:22.496+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:22.496+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:22.580+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-05-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:22.647+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-05-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:23.479+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-05-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:23.491+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-05-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:22.743488+00:00, run_end_date=2023-09-11 06:51:22.996973+00:00, run_duration=0.253485, state=success, executor_state=success, try_number=1, max_tries=0, job_id=155, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:19.518043+00:00, queued_by_job_id=2, pid=42248[0m
[[34m2023-09-11T06:51:23.751+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-02T00:00:00+00:00, run_after=2023-06-03T00:00:00+00:00[0m
[[34m2023-09-11T06:51:23.793+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-05-31 00:00:00+00:00: scheduled__2023-05-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:19.459098+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:23.793+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-05-31 00:00:00+00:00, run_id=scheduled__2023-05-31T00:00:00+00:00, run_start_date=2023-09-11 06:51:19.479622+00:00, run_end_date=2023-09-11 06:51:23.793730+00:00, run_duration=4.314108, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-05-31 00:00:00+00:00, data_interval_end=2023-06-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:23.797+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-01T00:00:00+00:00, run_after=2023-06-02T00:00:00+00:00[0m
[[34m2023-09-11T06:51:23.815+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:23.815+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:23.815+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:23.819+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:23.821+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:23.821+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:23.824+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:25.945+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:26.091+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:26.281+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:26.316+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:26.868+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:26.869+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:26.949+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:27.014+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:27.826+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:27.837+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:27.110113+00:00, run_end_date=2023-09-11 06:51:27.365927+00:00, run_duration=0.255814, state=success, executor_state=success, try_number=1, max_tries=0, job_id=156, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:23.816956+00:00, queued_by_job_id=2, pid=42255[0m
[[34m2023-09-11T06:51:28.095+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-02T00:00:00+00:00, run_after=2023-06-03T00:00:00+00:00[0m
[[34m2023-09-11T06:51:28.122+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-01 00:00:00+00:00: scheduled__2023-06-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:23.746549+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:28.123+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-01 00:00:00+00:00, run_id=scheduled__2023-06-01T00:00:00+00:00, run_start_date=2023-09-11 06:51:23.765519+00:00, run_end_date=2023-09-11 06:51:28.123097+00:00, run_duration=4.357578, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-01 00:00:00+00:00, data_interval_end=2023-06-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:28.126+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-02T00:00:00+00:00, run_after=2023-06-03T00:00:00+00:00[0m
[[34m2023-09-11T06:51:28.825+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-03T00:00:00+00:00, run_after=2023-06-04T00:00:00+00:00[0m
[[34m2023-09-11T06:51:28.874+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:28.874+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:28.875+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:28.877+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:28.878+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:28.878+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:28.881+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:30.945+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:31.092+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:31.281+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:31.327+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:31.908+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:31.909+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:31.985+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:32.051+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:32.867+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:32.879+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:32.149418+00:00, run_end_date=2023-09-11 06:51:32.394654+00:00, run_duration=0.245236, state=success, executor_state=success, try_number=1, max_tries=0, job_id=157, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:28.875904+00:00, queued_by_job_id=2, pid=42265[0m
[[34m2023-09-11T06:51:33.156+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-04T00:00:00+00:00, run_after=2023-06-05T00:00:00+00:00[0m
[[34m2023-09-11T06:51:33.195+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-02 00:00:00+00:00: scheduled__2023-06-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:28.818604+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:33.195+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-02 00:00:00+00:00, run_id=scheduled__2023-06-02T00:00:00+00:00, run_start_date=2023-09-11 06:51:28.839695+00:00, run_end_date=2023-09-11 06:51:33.195798+00:00, run_duration=4.356103, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-02 00:00:00+00:00, data_interval_end=2023-06-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:33.199+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-03T00:00:00+00:00, run_after=2023-06-04T00:00:00+00:00[0m
[[34m2023-09-11T06:51:33.216+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:33.216+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:33.216+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:33.218+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:33.219+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:33.220+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:33.223+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:35.326+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:35.473+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:35.667+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:35.704+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:36.224+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:36.225+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:36.302+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:36.368+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:37.200+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:37.212+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:36.462978+00:00, run_end_date=2023-09-11 06:51:36.711263+00:00, run_duration=0.248285, state=success, executor_state=success, try_number=1, max_tries=0, job_id=158, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:33.217330+00:00, queued_by_job_id=2, pid=42274[0m
[[34m2023-09-11T06:51:37.525+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-04T00:00:00+00:00, run_after=2023-06-05T00:00:00+00:00[0m
[[34m2023-09-11T06:51:37.549+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-03 00:00:00+00:00: scheduled__2023-06-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:33.149711+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:37.550+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-03 00:00:00+00:00, run_id=scheduled__2023-06-03T00:00:00+00:00, run_start_date=2023-09-11 06:51:33.169691+00:00, run_end_date=2023-09-11 06:51:37.550295+00:00, run_duration=4.380604, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-03 00:00:00+00:00, data_interval_end=2023-06-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:37.555+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-04T00:00:00+00:00, run_after=2023-06-05T00:00:00+00:00[0m
[[34m2023-09-11T06:51:38.477+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-05T00:00:00+00:00, run_after=2023-06-06T00:00:00+00:00[0m
[[34m2023-09-11T06:51:38.527+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:38.528+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:38.528+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:38.530+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:38.530+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:38.531+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:38.533+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:40.561+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:40.705+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:40.905+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:40.941+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:41.459+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:41.459+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:41.538+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:41.602+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:42.416+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:42.428+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:41.697259+00:00, run_end_date=2023-09-11 06:51:41.956023+00:00, run_duration=0.258764, state=success, executor_state=success, try_number=1, max_tries=0, job_id=159, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:38.528999+00:00, queued_by_job_id=2, pid=42284[0m
[[34m2023-09-11T06:51:42.687+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-06T00:00:00+00:00, run_after=2023-06-07T00:00:00+00:00[0m
[[34m2023-09-11T06:51:42.726+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-04 00:00:00+00:00: scheduled__2023-06-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:38.473062+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:42.726+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-04 00:00:00+00:00, run_id=scheduled__2023-06-04T00:00:00+00:00, run_start_date=2023-09-11 06:51:38.491221+00:00, run_end_date=2023-09-11 06:51:42.726817+00:00, run_duration=4.235596, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-04 00:00:00+00:00, data_interval_end=2023-06-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:42.730+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-05T00:00:00+00:00, run_after=2023-06-06T00:00:00+00:00[0m
[[34m2023-09-11T06:51:42.747+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:42.747+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:42.747+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:42.749+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:42.750+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:42.750+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:42.754+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:44.848+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:44.994+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:45.194+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:45.230+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:45.773+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:45.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:45.851+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:45.918+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:46.734+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:46.746+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:46.014619+00:00, run_end_date=2023-09-11 06:51:46.261548+00:00, run_duration=0.246929, state=success, executor_state=success, try_number=1, max_tries=0, job_id=160, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:42.748295+00:00, queued_by_job_id=2, pid=42293[0m
[[34m2023-09-11T06:51:47.000+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-06T00:00:00+00:00, run_after=2023-06-07T00:00:00+00:00[0m
[[34m2023-09-11T06:51:47.026+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-05 00:00:00+00:00: scheduled__2023-06-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:42.681660+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:47.026+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-05 00:00:00+00:00, run_id=scheduled__2023-06-05T00:00:00+00:00, run_start_date=2023-09-11 06:51:42.700857+00:00, run_end_date=2023-09-11 06:51:47.026827+00:00, run_duration=4.32597, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-05 00:00:00+00:00, data_interval_end=2023-06-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:47.030+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-06T00:00:00+00:00, run_after=2023-06-07T00:00:00+00:00[0m
[[34m2023-09-11T06:51:47.686+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-07T00:00:00+00:00, run_after=2023-06-08T00:00:00+00:00[0m
[[34m2023-09-11T06:51:47.733+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:47.734+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:47.734+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:47.738+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:47.738+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:47.739+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:47.741+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:49.866+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:50.020+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:50.213+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:50.249+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:50.764+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:50.764+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:50.841+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:50.914+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:51.737+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:51.748+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:51.011581+00:00, run_end_date=2023-09-11 06:51:51.262449+00:00, run_duration=0.250868, state=success, executor_state=success, try_number=1, max_tries=0, job_id=161, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:47.734940+00:00, queued_by_job_id=2, pid=42303[0m
[[34m2023-09-11T06:51:52.012+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-08T00:00:00+00:00, run_after=2023-06-09T00:00:00+00:00[0m
[[34m2023-09-11T06:51:52.052+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-06 00:00:00+00:00: scheduled__2023-06-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:47.681359+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:52.053+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-06 00:00:00+00:00, run_id=scheduled__2023-06-06T00:00:00+00:00, run_start_date=2023-09-11 06:51:47.700323+00:00, run_end_date=2023-09-11 06:51:52.053690+00:00, run_duration=4.353367, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-06 00:00:00+00:00, data_interval_end=2023-06-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:52.057+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-07T00:00:00+00:00, run_after=2023-06-08T00:00:00+00:00[0m
[[34m2023-09-11T06:51:52.075+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:52.075+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:52.076+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:52.078+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:52.078+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:52.079+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:52.081+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:54.123+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:54.265+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:54.457+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:54.493+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:55.014+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:51:55.015+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:55.094+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:51:55.158+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:51:55.966+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:51:55.978+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:51:55.250020+00:00, run_end_date=2023-09-11 06:51:55.523778+00:00, run_duration=0.273758, state=success, executor_state=success, try_number=1, max_tries=0, job_id=162, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:52.076789+00:00, queued_by_job_id=2, pid=42312[0m
[[34m2023-09-11T06:51:56.227+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-08T00:00:00+00:00, run_after=2023-06-09T00:00:00+00:00[0m
[[34m2023-09-11T06:51:56.252+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-07 00:00:00+00:00: scheduled__2023-06-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:52.006953+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:51:56.253+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-07 00:00:00+00:00, run_id=scheduled__2023-06-07T00:00:00+00:00, run_start_date=2023-09-11 06:51:52.027201+00:00, run_end_date=2023-09-11 06:51:56.253414+00:00, run_duration=4.226213, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-07 00:00:00+00:00, data_interval_end=2023-06-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:51:56.257+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-08T00:00:00+00:00, run_after=2023-06-09T00:00:00+00:00[0m
[[34m2023-09-11T06:51:57.309+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-09T00:00:00+00:00, run_after=2023-06-10T00:00:00+00:00[0m
[[34m2023-09-11T06:51:57.362+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:57.363+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:51:57.363+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:51:57.365+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:51:57.366+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:51:57.366+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:57.370+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:51:59.480+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:51:59.628+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:51:59.822+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:51:59.861+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:00.421+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:00.422+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:00.499+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:00.565+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:01.383+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:01.407+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:00.663573+00:00, run_end_date=2023-09-11 06:52:00.909032+00:00, run_duration=0.245459, state=success, executor_state=success, try_number=1, max_tries=0, job_id=163, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:51:57.363944+00:00, queued_by_job_id=2, pid=42322[0m
[[34m2023-09-11T06:52:01.581+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-10T00:00:00+00:00, run_after=2023-06-11T00:00:00+00:00[0m
[[34m2023-09-11T06:52:01.624+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-08 00:00:00+00:00: scheduled__2023-06-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:51:57.304325+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:01.624+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-08 00:00:00+00:00, run_id=scheduled__2023-06-08T00:00:00+00:00, run_start_date=2023-09-11 06:51:57.326485+00:00, run_end_date=2023-09-11 06:52:01.624800+00:00, run_duration=4.298315, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-08 00:00:00+00:00, data_interval_end=2023-06-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:01.628+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-09T00:00:00+00:00, run_after=2023-06-10T00:00:00+00:00[0m
[[34m2023-09-11T06:52:01.644+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:01.645+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:01.645+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:01.647+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:01.647+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:01.648+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:01.666+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:03.765+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:03.941+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:04.144+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:04.181+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:04.719+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:04.720+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:04.797+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:04.862+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:05.756+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:05.767+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:04.963304+00:00, run_end_date=2023-09-11 06:52:05.220397+00:00, run_duration=0.257093, state=success, executor_state=success, try_number=1, max_tries=0, job_id=164, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:01.646119+00:00, queued_by_job_id=2, pid=42331[0m
[[34m2023-09-11T06:52:06.013+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-10T00:00:00+00:00, run_after=2023-06-11T00:00:00+00:00[0m
[[34m2023-09-11T06:52:06.040+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-09 00:00:00+00:00: scheduled__2023-06-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:01.576821+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:06.041+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-09 00:00:00+00:00, run_id=scheduled__2023-06-09T00:00:00+00:00, run_start_date=2023-09-11 06:52:01.596176+00:00, run_end_date=2023-09-11 06:52:06.041237+00:00, run_duration=4.445061, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-09 00:00:00+00:00, data_interval_end=2023-06-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:06.044+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-10T00:00:00+00:00, run_after=2023-06-11T00:00:00+00:00[0m
[[34m2023-09-11T06:52:06.684+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-11T00:00:00+00:00, run_after=2023-06-12T00:00:00+00:00[0m
[[34m2023-09-11T06:52:06.732+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:06.732+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:06.732+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:06.735+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:06.736+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:06.737+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:06.740+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:08.782+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:08.939+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:09.129+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:09.166+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:09.713+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:09.714+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:09.794+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:09.859+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:10.676+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:10.689+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:09.956297+00:00, run_end_date=2023-09-11 06:52:10.205970+00:00, run_duration=0.249673, state=success, executor_state=success, try_number=1, max_tries=0, job_id=165, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:06.733487+00:00, queued_by_job_id=2, pid=42341[0m
[[34m2023-09-11T06:52:10.962+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-12T00:00:00+00:00, run_after=2023-06-13T00:00:00+00:00[0m
[[34m2023-09-11T06:52:11.003+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-10 00:00:00+00:00: scheduled__2023-06-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:06.679749+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:11.004+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-10 00:00:00+00:00, run_id=scheduled__2023-06-10T00:00:00+00:00, run_start_date=2023-09-11 06:52:06.698445+00:00, run_end_date=2023-09-11 06:52:11.003892+00:00, run_duration=4.305447, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-10 00:00:00+00:00, data_interval_end=2023-06-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:11.007+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-11T00:00:00+00:00, run_after=2023-06-12T00:00:00+00:00[0m
[[34m2023-09-11T06:52:11.024+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:11.024+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:11.025+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:11.027+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:11.027+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:11.027+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:11.030+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:13.101+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:13.249+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:13.458+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:13.495+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:14.020+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:14.021+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:14.100+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:14.166+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:14.991+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:15.004+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:14.262481+00:00, run_end_date=2023-09-11 06:52:14.509114+00:00, run_duration=0.246633, state=success, executor_state=success, try_number=1, max_tries=0, job_id=166, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:11.025767+00:00, queued_by_job_id=2, pid=42350[0m
[[34m2023-09-11T06:52:15.256+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-12T00:00:00+00:00, run_after=2023-06-13T00:00:00+00:00[0m
[[34m2023-09-11T06:52:15.281+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-11 00:00:00+00:00: scheduled__2023-06-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:10.957756+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:15.282+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-11 00:00:00+00:00, run_id=scheduled__2023-06-11T00:00:00+00:00, run_start_date=2023-09-11 06:52:10.978118+00:00, run_end_date=2023-09-11 06:52:15.282320+00:00, run_duration=4.304202, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-11 00:00:00+00:00, data_interval_end=2023-06-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:15.287+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-12T00:00:00+00:00, run_after=2023-06-13T00:00:00+00:00[0m
[[34m2023-09-11T06:52:15.965+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-13T00:00:00+00:00, run_after=2023-06-14T00:00:00+00:00[0m
[[34m2023-09-11T06:52:16.014+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:16.015+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:16.015+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:16.017+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:16.019+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:16.019+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:16.023+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:18.173+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:18.345+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:18.557+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:18.597+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:19.194+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:19.195+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:19.272+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:19.332+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:20.151+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:20.162+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:19.438906+00:00, run_end_date=2023-09-11 06:52:19.707990+00:00, run_duration=0.269084, state=success, executor_state=success, try_number=1, max_tries=0, job_id=167, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:16.016028+00:00, queued_by_job_id=2, pid=42360[0m
[[34m2023-09-11T06:52:20.444+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-14T00:00:00+00:00, run_after=2023-06-15T00:00:00+00:00[0m
[[34m2023-09-11T06:52:20.488+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-12 00:00:00+00:00: scheduled__2023-06-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:15.960287+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:20.489+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-12 00:00:00+00:00, run_id=scheduled__2023-06-12T00:00:00+00:00, run_start_date=2023-09-11 06:52:15.979834+00:00, run_end_date=2023-09-11 06:52:20.489033+00:00, run_duration=4.509199, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-12 00:00:00+00:00, data_interval_end=2023-06-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:20.492+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-13T00:00:00+00:00, run_after=2023-06-14T00:00:00+00:00[0m
[[34m2023-09-11T06:52:20.513+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:20.513+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:20.514+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:20.516+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:20.517+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:20.518+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:20.522+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:22.837+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:22.989+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:23.182+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:23.220+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:23.743+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:23.744+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:23.821+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:23.884+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:24.792+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:24.804+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:23.989612+00:00, run_end_date=2023-09-11 06:52:24.270669+00:00, run_duration=0.281057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=168, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:20.514898+00:00, queued_by_job_id=2, pid=42369[0m
[[34m2023-09-11T06:52:25.047+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-14T00:00:00+00:00, run_after=2023-06-15T00:00:00+00:00[0m
[[34m2023-09-11T06:52:25.073+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-13 00:00:00+00:00: scheduled__2023-06-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:20.436166+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:25.073+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-13 00:00:00+00:00, run_id=scheduled__2023-06-13T00:00:00+00:00, run_start_date=2023-09-11 06:52:20.460747+00:00, run_end_date=2023-09-11 06:52:25.073710+00:00, run_duration=4.612963, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-13 00:00:00+00:00, data_interval_end=2023-06-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:25.077+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-14T00:00:00+00:00, run_after=2023-06-15T00:00:00+00:00[0m
[[34m2023-09-11T06:52:25.421+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-15T00:00:00+00:00, run_after=2023-06-16T00:00:00+00:00[0m
[[34m2023-09-11T06:52:25.496+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:25.496+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:25.497+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:25.499+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:25.500+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:25.500+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:25.504+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:27.627+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:27.790+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:27.982+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:28.020+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:28.615+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:28.615+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:28.694+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:28.774+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:29.624+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:29.636+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:28.866094+00:00, run_end_date=2023-09-11 06:52:29.113662+00:00, run_duration=0.247568, state=success, executor_state=success, try_number=1, max_tries=0, job_id=169, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:25.497916+00:00, queued_by_job_id=2, pid=42377[0m
[[34m2023-09-11T06:52:29.923+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-16T00:00:00+00:00, run_after=2023-06-17T00:00:00+00:00[0m
[[34m2023-09-11T06:52:29.963+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-14 00:00:00+00:00: scheduled__2023-06-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:25.414785+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:29.964+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-14 00:00:00+00:00, run_id=scheduled__2023-06-14T00:00:00+00:00, run_start_date=2023-09-11 06:52:25.446692+00:00, run_end_date=2023-09-11 06:52:29.964008+00:00, run_duration=4.517316, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-14 00:00:00+00:00, data_interval_end=2023-06-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:29.968+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-15T00:00:00+00:00, run_after=2023-06-16T00:00:00+00:00[0m
[[34m2023-09-11T06:52:29.985+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:29.986+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:29.986+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:29.989+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:29.989+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:29.989+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:29.993+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:32.055+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:32.208+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:32.406+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:32.443+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:32.979+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:32.979+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:33.059+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:33.125+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:33.944+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:33.956+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:33.223363+00:00, run_end_date=2023-09-11 06:52:33.482911+00:00, run_duration=0.259548, state=success, executor_state=success, try_number=1, max_tries=0, job_id=170, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:29.987373+00:00, queued_by_job_id=2, pid=42386[0m
[[34m2023-09-11T06:52:34.214+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-16T00:00:00+00:00, run_after=2023-06-17T00:00:00+00:00[0m
[[34m2023-09-11T06:52:34.239+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-15 00:00:00+00:00: scheduled__2023-06-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:29.916295+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:34.240+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-15 00:00:00+00:00, run_id=scheduled__2023-06-15T00:00:00+00:00, run_start_date=2023-09-11 06:52:29.938234+00:00, run_end_date=2023-09-11 06:52:34.240069+00:00, run_duration=4.301835, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-15 00:00:00+00:00, data_interval_end=2023-06-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:34.243+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-16T00:00:00+00:00, run_after=2023-06-17T00:00:00+00:00[0m
[[34m2023-09-11T06:52:34.948+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-17T00:00:00+00:00, run_after=2023-06-18T00:00:00+00:00[0m
[[34m2023-09-11T06:52:34.997+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:34.997+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:34.998+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:35.001+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:35.002+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:35.003+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:35.006+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:37.026+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:37.169+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:37.364+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:37.404+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:37.956+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:37.957+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:38.042+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:38.109+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:39.064+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:39.076+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:38.207264+00:00, run_end_date=2023-09-11 06:52:38.585413+00:00, run_duration=0.378149, state=success, executor_state=success, try_number=1, max_tries=0, job_id=171, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:34.998854+00:00, queued_by_job_id=2, pid=42396[0m
[[34m2023-09-11T06:52:39.353+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-18T00:00:00+00:00, run_after=2023-06-19T00:00:00+00:00[0m
[[34m2023-09-11T06:52:39.393+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-16 00:00:00+00:00: scheduled__2023-06-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:34.944236+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:39.394+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-16 00:00:00+00:00, run_id=scheduled__2023-06-16T00:00:00+00:00, run_start_date=2023-09-11 06:52:34.962929+00:00, run_end_date=2023-09-11 06:52:39.394101+00:00, run_duration=4.431172, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-16 00:00:00+00:00, data_interval_end=2023-06-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:39.397+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-17T00:00:00+00:00, run_after=2023-06-18T00:00:00+00:00[0m
[[34m2023-09-11T06:52:39.413+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:39.414+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:39.414+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:39.417+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:39.418+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:39.418+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:39.422+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:41.496+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:41.640+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:41.833+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:41.870+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:42.409+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:42.409+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:42.490+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:42.557+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:43.408+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:43.421+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:42.657290+00:00, run_end_date=2023-09-11 06:52:42.931297+00:00, run_duration=0.274007, state=success, executor_state=success, try_number=1, max_tries=0, job_id=172, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:39.415163+00:00, queued_by_job_id=2, pid=42403[0m
[[34m2023-09-11T06:52:43.563+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-18T00:00:00+00:00, run_after=2023-06-19T00:00:00+00:00[0m
[[34m2023-09-11T06:52:43.589+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-17 00:00:00+00:00: scheduled__2023-06-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:39.346989+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:43.590+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-17 00:00:00+00:00, run_id=scheduled__2023-06-17T00:00:00+00:00, run_start_date=2023-09-11 06:52:39.368567+00:00, run_end_date=2023-09-11 06:52:43.590004+00:00, run_duration=4.221437, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-17 00:00:00+00:00, data_interval_end=2023-06-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:43.593+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-18T00:00:00+00:00, run_after=2023-06-19T00:00:00+00:00[0m
[[34m2023-09-11T06:52:44.999+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-19T00:00:00+00:00, run_after=2023-06-20T00:00:00+00:00[0m
[[34m2023-09-11T06:52:45.047+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:45.047+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:45.048+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:45.051+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:45.052+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:45.052+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:45.055+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:47.143+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:47.288+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:47.488+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:47.525+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:48.062+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:48.062+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:48.145+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:48.237+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:49.063+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:49.075+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:48.346812+00:00, run_end_date=2023-09-11 06:52:48.623153+00:00, run_duration=0.276341, state=success, executor_state=success, try_number=1, max_tries=0, job_id=173, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:45.048881+00:00, queued_by_job_id=2, pid=42415[0m
[[34m2023-09-11T06:52:49.417+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-20T00:00:00+00:00, run_after=2023-06-21T00:00:00+00:00[0m
[[34m2023-09-11T06:52:49.458+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-18 00:00:00+00:00: scheduled__2023-06-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:44.994362+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:49.459+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-18 00:00:00+00:00, run_id=scheduled__2023-06-18T00:00:00+00:00, run_start_date=2023-09-11 06:52:45.013028+00:00, run_end_date=2023-09-11 06:52:49.458921+00:00, run_duration=4.445893, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-18 00:00:00+00:00, data_interval_end=2023-06-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:49.462+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-19T00:00:00+00:00, run_after=2023-06-20T00:00:00+00:00[0m
[[34m2023-09-11T06:52:49.479+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:49.479+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:49.480+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:49.482+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:49.484+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:49.485+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:49.488+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:51.548+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:51.696+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:51.889+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:51.925+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:52.445+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:52.446+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:52.525+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:52.589+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:53.404+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:53.414+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:52.683861+00:00, run_end_date=2023-09-11 06:52:52.941726+00:00, run_duration=0.257865, state=success, executor_state=success, try_number=1, max_tries=0, job_id=174, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:49.480975+00:00, queued_by_job_id=2, pid=42422[0m
[[34m2023-09-11T06:52:53.696+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-20T00:00:00+00:00, run_after=2023-06-21T00:00:00+00:00[0m
[[34m2023-09-11T06:52:53.722+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-19 00:00:00+00:00: scheduled__2023-06-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:49.411184+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:53.722+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-19 00:00:00+00:00, run_id=scheduled__2023-06-19T00:00:00+00:00, run_start_date=2023-09-11 06:52:49.431759+00:00, run_end_date=2023-09-11 06:52:53.722655+00:00, run_duration=4.290896, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-19 00:00:00+00:00, data_interval_end=2023-06-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:53.726+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-20T00:00:00+00:00, run_after=2023-06-21T00:00:00+00:00[0m
[[34m2023-09-11T06:52:54.411+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-21T00:00:00+00:00, run_after=2023-06-22T00:00:00+00:00[0m
[[34m2023-09-11T06:52:54.463+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:54.463+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:54.464+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:54.467+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:54.468+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:54.468+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:54.471+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:56.514+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:52:56.662+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:56.854+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:56.890+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:52:57.448+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:52:57.450+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:52:57.533+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:52:57.596+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:52:58.451+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:52:58.462+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:52:57.707380+00:00, run_end_date=2023-09-11 06:52:57.963868+00:00, run_duration=0.256488, state=success, executor_state=success, try_number=1, max_tries=0, job_id=175, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:54.464787+00:00, queued_by_job_id=2, pid=42432[0m
[[34m2023-09-11T06:52:58.679+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-22T00:00:00+00:00, run_after=2023-06-23T00:00:00+00:00[0m
[[34m2023-09-11T06:52:58.720+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-20 00:00:00+00:00: scheduled__2023-06-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:54.406564+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:52:58.720+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-20 00:00:00+00:00, run_id=scheduled__2023-06-20T00:00:00+00:00, run_start_date=2023-09-11 06:52:54.426732+00:00, run_end_date=2023-09-11 06:52:58.720561+00:00, run_duration=4.293829, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-20 00:00:00+00:00, data_interval_end=2023-06-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:52:58.724+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-21T00:00:00+00:00, run_after=2023-06-22T00:00:00+00:00[0m
[[34m2023-09-11T06:52:58.741+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:58.741+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:52:58.741+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:52:58.743+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:52:58.744+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:52:58.744+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:52:58.747+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:00.818+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:00.963+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:01.156+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:01.191+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:01.718+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:01.718+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:01.806+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:01.872+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:02.714+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:02.726+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:01.979415+00:00, run_end_date=2023-09-11 06:53:02.234854+00:00, run_duration=0.255439, state=success, executor_state=success, try_number=1, max_tries=0, job_id=176, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:52:58.742177+00:00, queued_by_job_id=2, pid=42441[0m
[[34m2023-09-11T06:53:03.063+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-22T00:00:00+00:00, run_after=2023-06-23T00:00:00+00:00[0m
[[34m2023-09-11T06:53:03.090+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-21 00:00:00+00:00: scheduled__2023-06-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:52:58.674380+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:03.090+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-21 00:00:00+00:00, run_id=scheduled__2023-06-21T00:00:00+00:00, run_start_date=2023-09-11 06:52:58.693443+00:00, run_end_date=2023-09-11 06:53:03.090781+00:00, run_duration=4.397338, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-21 00:00:00+00:00, data_interval_end=2023-06-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:03.094+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-22T00:00:00+00:00, run_after=2023-06-23T00:00:00+00:00[0m
[[34m2023-09-11T06:53:03.767+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-23T00:00:00+00:00, run_after=2023-06-24T00:00:00+00:00[0m
[[34m2023-09-11T06:53:03.813+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:03.814+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:03.814+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:03.818+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:03.818+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:03.819+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:03.822+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:05.928+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:06.090+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:06.289+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:06.326+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:06.858+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:06.858+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:06.938+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:07.003+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:07.810+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:07.823+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:07.100482+00:00, run_end_date=2023-09-11 06:53:07.352590+00:00, run_duration=0.252108, state=success, executor_state=success, try_number=1, max_tries=0, job_id=177, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:03.815195+00:00, queued_by_job_id=2, pid=42451[0m
[[34m2023-09-11T06:53:08.095+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-24T00:00:00+00:00, run_after=2023-06-25T00:00:00+00:00[0m
[[34m2023-09-11T06:53:08.137+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-22 00:00:00+00:00: scheduled__2023-06-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:03.761094+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:08.138+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-22 00:00:00+00:00, run_id=scheduled__2023-06-22T00:00:00+00:00, run_start_date=2023-09-11 06:53:03.780863+00:00, run_end_date=2023-09-11 06:53:08.138182+00:00, run_duration=4.357319, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-22 00:00:00+00:00, data_interval_end=2023-06-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:08.141+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-23T00:00:00+00:00, run_after=2023-06-24T00:00:00+00:00[0m
[[34m2023-09-11T06:53:08.158+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:08.159+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:08.159+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:08.161+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:08.162+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:08.162+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:08.166+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:10.231+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:10.375+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:10.576+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:10.615+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:11.137+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:11.138+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:11.216+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:11.282+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:12.092+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:12.104+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:11.374525+00:00, run_end_date=2023-09-11 06:53:11.622657+00:00, run_duration=0.248132, state=success, executor_state=success, try_number=1, max_tries=0, job_id=178, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:08.159973+00:00, queued_by_job_id=2, pid=42460[0m
[[34m2023-09-11T06:53:12.344+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-24T00:00:00+00:00, run_after=2023-06-25T00:00:00+00:00[0m
[[34m2023-09-11T06:53:12.371+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-23 00:00:00+00:00: scheduled__2023-06-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:08.090052+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:12.371+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-23 00:00:00+00:00, run_id=scheduled__2023-06-23T00:00:00+00:00, run_start_date=2023-09-11 06:53:08.111110+00:00, run_end_date=2023-09-11 06:53:12.371614+00:00, run_duration=4.260504, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-23 00:00:00+00:00, data_interval_end=2023-06-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:12.375+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-24T00:00:00+00:00, run_after=2023-06-25T00:00:00+00:00[0m
[[34m2023-09-11T06:53:12.979+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-25T00:00:00+00:00, run_after=2023-06-26T00:00:00+00:00[0m
[[34m2023-09-11T06:53:13.028+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:13.028+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:13.028+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:13.030+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:13.031+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:13.032+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:13.035+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:15.107+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:15.258+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:15.458+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:15.493+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:16.020+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:16.021+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:16.102+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:16.170+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:16.994+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:17.007+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:16.262601+00:00, run_end_date=2023-09-11 06:53:16.503452+00:00, run_duration=0.240851, state=success, executor_state=success, try_number=1, max_tries=0, job_id=179, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:13.029238+00:00, queued_by_job_id=2, pid=42470[0m
[[34m2023-09-11T06:53:17.273+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-26T00:00:00+00:00, run_after=2023-06-27T00:00:00+00:00[0m
[[34m2023-09-11T06:53:17.311+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-24 00:00:00+00:00: scheduled__2023-06-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:12.974926+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:17.311+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-24 00:00:00+00:00, run_id=scheduled__2023-06-24T00:00:00+00:00, run_start_date=2023-09-11 06:53:12.993280+00:00, run_end_date=2023-09-11 06:53:17.311593+00:00, run_duration=4.318313, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-24 00:00:00+00:00, data_interval_end=2023-06-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:17.316+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-25T00:00:00+00:00, run_after=2023-06-26T00:00:00+00:00[0m
[[34m2023-09-11T06:53:17.335+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:17.335+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:17.335+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:17.338+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:17.338+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:17.339+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:17.342+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:19.439+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:19.601+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:19.811+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:19.851+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:20.390+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:20.391+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:20.470+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:20.538+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:21.346+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:21.358+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:20.633732+00:00, run_end_date=2023-09-11 06:53:20.878816+00:00, run_duration=0.245084, state=success, executor_state=success, try_number=1, max_tries=0, job_id=180, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:17.336535+00:00, queued_by_job_id=2, pid=42479[0m
[[34m2023-09-11T06:53:21.630+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-26T00:00:00+00:00, run_after=2023-06-27T00:00:00+00:00[0m
[[34m2023-09-11T06:53:21.658+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-25 00:00:00+00:00: scheduled__2023-06-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:17.267458+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:21.658+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-25 00:00:00+00:00, run_id=scheduled__2023-06-25T00:00:00+00:00, run_start_date=2023-09-11 06:53:17.287165+00:00, run_end_date=2023-09-11 06:53:21.658765+00:00, run_duration=4.3716, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-25 00:00:00+00:00, data_interval_end=2023-06-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:21.662+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-26T00:00:00+00:00, run_after=2023-06-27T00:00:00+00:00[0m
[[34m2023-09-11T06:53:22.274+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-27T00:00:00+00:00, run_after=2023-06-28T00:00:00+00:00[0m
[[34m2023-09-11T06:53:22.324+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:22.324+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:22.324+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:22.327+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:22.327+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:22.327+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:22.330+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:24.464+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:24.612+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:24.803+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:24.840+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:25.383+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:25.384+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:25.468+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:25.530+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:26.345+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:26.358+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:25.626531+00:00, run_end_date=2023-09-11 06:53:25.877401+00:00, run_duration=0.25087, state=success, executor_state=success, try_number=1, max_tries=0, job_id=181, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:22.325549+00:00, queued_by_job_id=2, pid=42489[0m
[[34m2023-09-11T06:53:26.640+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-28T00:00:00+00:00, run_after=2023-06-29T00:00:00+00:00[0m
[[34m2023-09-11T06:53:26.679+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-26 00:00:00+00:00: scheduled__2023-06-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:22.270094+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:26.679+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-26 00:00:00+00:00, run_id=scheduled__2023-06-26T00:00:00+00:00, run_start_date=2023-09-11 06:53:22.288850+00:00, run_end_date=2023-09-11 06:53:26.679687+00:00, run_duration=4.390837, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-26 00:00:00+00:00, data_interval_end=2023-06-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:26.685+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-27T00:00:00+00:00, run_after=2023-06-28T00:00:00+00:00[0m
[[34m2023-09-11T06:53:26.702+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:26.703+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:26.703+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:26.705+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:26.706+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:26.706+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:26.709+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:28.870+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:29.027+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:29.221+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:29.257+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:29.794+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:29.795+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:29.885+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:29.955+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:30.774+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:30.786+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:30.047378+00:00, run_end_date=2023-09-11 06:53:30.303707+00:00, run_duration=0.256329, state=success, executor_state=success, try_number=1, max_tries=0, job_id=182, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:26.704021+00:00, queued_by_job_id=2, pid=42498[0m
[[34m2023-09-11T06:53:31.045+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-28T00:00:00+00:00, run_after=2023-06-29T00:00:00+00:00[0m
[[34m2023-09-11T06:53:31.071+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-27 00:00:00+00:00: scheduled__2023-06-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:26.635222+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:31.071+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-27 00:00:00+00:00, run_id=scheduled__2023-06-27T00:00:00+00:00, run_start_date=2023-09-11 06:53:26.654258+00:00, run_end_date=2023-09-11 06:53:31.071662+00:00, run_duration=4.417404, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-27 00:00:00+00:00, data_interval_end=2023-06-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:31.075+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-28T00:00:00+00:00, run_after=2023-06-29T00:00:00+00:00[0m
[[34m2023-09-11T06:53:31.698+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-29T00:00:00+00:00, run_after=2023-06-30T00:00:00+00:00[0m
[[34m2023-09-11T06:53:31.749+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:31.749+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:31.750+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:31.753+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:31.754+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:31.754+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:31.757+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:33.860+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:34.026+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:34.222+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:34.258+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:34.779+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:34.781+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:34.858+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:34.928+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:35.765+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:35.776+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:35.024519+00:00, run_end_date=2023-09-11 06:53:35.274413+00:00, run_duration=0.249894, state=success, executor_state=success, try_number=1, max_tries=0, job_id=183, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:31.751082+00:00, queued_by_job_id=2, pid=42508[0m
[[34m2023-09-11T06:53:36.241+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-30T00:00:00+00:00, run_after=2023-07-01T00:00:00+00:00[0m
[[34m2023-09-11T06:53:36.285+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-28 00:00:00+00:00: scheduled__2023-06-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:31.692134+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:36.285+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-28 00:00:00+00:00, run_id=scheduled__2023-06-28T00:00:00+00:00, run_start_date=2023-09-11 06:53:31.712501+00:00, run_end_date=2023-09-11 06:53:36.285660+00:00, run_duration=4.573159, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-28 00:00:00+00:00, data_interval_end=2023-06-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:36.289+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-29T00:00:00+00:00, run_after=2023-06-30T00:00:00+00:00[0m
[[34m2023-09-11T06:53:36.310+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:36.310+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:36.310+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:36.312+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:36.313+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:36.314+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:36.317+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:38.423+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:38.583+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:38.777+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:38.815+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:39.362+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:39.362+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:39.451+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:39.516+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:40.335+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:40.346+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:39.615865+00:00, run_end_date=2023-09-11 06:53:39.867130+00:00, run_duration=0.251265, state=success, executor_state=success, try_number=1, max_tries=0, job_id=184, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:36.311347+00:00, queued_by_job_id=2, pid=42517[0m
[[34m2023-09-11T06:53:40.896+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-30T00:00:00+00:00, run_after=2023-07-01T00:00:00+00:00[0m
[[34m2023-09-11T06:53:40.927+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-29 00:00:00+00:00: scheduled__2023-06-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:36.236421+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:40.927+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-29 00:00:00+00:00, run_id=scheduled__2023-06-29T00:00:00+00:00, run_start_date=2023-09-11 06:53:36.257633+00:00, run_end_date=2023-09-11 06:53:40.927454+00:00, run_duration=4.669821, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-29 00:00:00+00:00, data_interval_end=2023-06-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:40.932+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-06-30T00:00:00+00:00, run_after=2023-07-01T00:00:00+00:00[0m
[[34m2023-09-11T06:53:41.724+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-01T00:00:00+00:00, run_after=2023-07-02T00:00:00+00:00[0m
[[34m2023-09-11T06:53:41.779+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:41.779+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:41.780+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-06-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:41.784+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:41.785+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:41.785+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:41.788+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:43.920+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:44.066+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:44.258+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:44.295+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:44.835+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:44.836+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:44.919+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-06-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:44.987+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-06-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:45.865+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:45.880+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-06-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:45.082346+00:00, run_end_date=2023-09-11 06:53:45.365642+00:00, run_duration=0.283296, state=success, executor_state=success, try_number=1, max_tries=0, job_id=185, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:41.782889+00:00, queued_by_job_id=2, pid=42527[0m
[[34m2023-09-11T06:53:46.366+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-02T00:00:00+00:00, run_after=2023-07-03T00:00:00+00:00[0m
[[34m2023-09-11T06:53:46.406+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-06-30 00:00:00+00:00: scheduled__2023-06-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:41.719318+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:46.406+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-06-30 00:00:00+00:00, run_id=scheduled__2023-06-30T00:00:00+00:00, run_start_date=2023-09-11 06:53:41.741378+00:00, run_end_date=2023-09-11 06:53:46.406694+00:00, run_duration=4.665316, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-30 00:00:00+00:00, data_interval_end=2023-07-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:46.410+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-01T00:00:00+00:00, run_after=2023-07-02T00:00:00+00:00[0m
[[34m2023-09-11T06:53:46.427+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:46.427+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:46.427+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:46.429+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:46.431+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:46.431+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:46.454+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:48.545+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:48.691+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:48.903+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:48.941+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:49.498+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:49.499+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:49.587+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:49.655+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:50.478+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:50.490+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:49.756028+00:00, run_end_date=2023-09-11 06:53:50.016009+00:00, run_duration=0.259981, state=success, executor_state=success, try_number=1, max_tries=0, job_id=186, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:46.428155+00:00, queued_by_job_id=2, pid=42536[0m
[[34m2023-09-11T06:53:50.967+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-02T00:00:00+00:00, run_after=2023-07-03T00:00:00+00:00[0m
[[34m2023-09-11T06:53:50.992+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-01 00:00:00+00:00: scheduled__2023-07-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:46.359183+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:50.993+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-01 00:00:00+00:00, run_id=scheduled__2023-07-01T00:00:00+00:00, run_start_date=2023-09-11 06:53:46.378935+00:00, run_end_date=2023-09-11 06:53:50.992977+00:00, run_duration=4.614042, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-01 00:00:00+00:00, data_interval_end=2023-07-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:50.997+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-02T00:00:00+00:00, run_after=2023-07-03T00:00:00+00:00[0m
[[34m2023-09-11T06:53:51.558+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-03T00:00:00+00:00, run_after=2023-07-04T00:00:00+00:00[0m
[[34m2023-09-11T06:53:51.610+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:51.610+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:51.610+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:51.613+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:51.614+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:51.614+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:51.635+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:53.772+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:53.925+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:54.119+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:54.156+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:54.685+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:54.685+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:54.760+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:54.828+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:53:55.651+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:53:55.661+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:54.926140+00:00, run_end_date=2023-09-11 06:53:55.172862+00:00, run_duration=0.246722, state=success, executor_state=success, try_number=1, max_tries=0, job_id=187, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:51.611437+00:00, queued_by_job_id=2, pid=42546[0m
[[34m2023-09-11T06:53:56.241+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-04T00:00:00+00:00, run_after=2023-07-05T00:00:00+00:00[0m
[[34m2023-09-11T06:53:56.285+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-02 00:00:00+00:00: scheduled__2023-07-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:51.554107+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:53:56.285+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-02 00:00:00+00:00, run_id=scheduled__2023-07-02T00:00:00+00:00, run_start_date=2023-09-11 06:53:51.573062+00:00, run_end_date=2023-09-11 06:53:56.285875+00:00, run_duration=4.712813, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-02 00:00:00+00:00, data_interval_end=2023-07-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:53:56.289+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-03T00:00:00+00:00, run_after=2023-07-04T00:00:00+00:00[0m
[[34m2023-09-11T06:53:56.306+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:56.307+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:53:56.307+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:53:56.309+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:53:56.310+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:53:56.310+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:56.314+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:53:58.433+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:53:58.582+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:58.782+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:58.818+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:53:59.359+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:53:59.359+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:53:59.449+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:53:59.515+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:00.413+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:00.425+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:53:59.632552+00:00, run_end_date=2023-09-11 06:53:59.883452+00:00, run_duration=0.2509, state=success, executor_state=success, try_number=1, max_tries=0, job_id=188, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:53:56.308148+00:00, queued_by_job_id=2, pid=42555[0m
[[34m2023-09-11T06:54:00.875+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-04T00:00:00+00:00, run_after=2023-07-05T00:00:00+00:00[0m
[[34m2023-09-11T06:54:00.901+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-03 00:00:00+00:00: scheduled__2023-07-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:53:56.236325+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:00.902+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-03 00:00:00+00:00, run_id=scheduled__2023-07-03T00:00:00+00:00, run_start_date=2023-09-11 06:53:56.257869+00:00, run_end_date=2023-09-11 06:54:00.902006+00:00, run_duration=4.644137, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-03 00:00:00+00:00, data_interval_end=2023-07-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:00.905+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-04T00:00:00+00:00, run_after=2023-07-05T00:00:00+00:00[0m
[[34m2023-09-11T06:54:02.294+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-05T00:00:00+00:00, run_after=2023-07-06T00:00:00+00:00[0m
[[34m2023-09-11T06:54:02.343+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:02.344+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:02.344+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:02.348+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:02.349+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:02.350+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:02.352+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:04.733+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:04.958+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:05.194+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:05.230+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:05.750+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:05.751+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:05.839+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:05.909+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:06.823+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:06.835+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:06.019628+00:00, run_end_date=2023-09-11 06:54:06.294999+00:00, run_duration=0.275371, state=success, executor_state=success, try_number=1, max_tries=0, job_id=189, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:02.345368+00:00, queued_by_job_id=2, pid=42565[0m
[[34m2023-09-11T06:54:07.299+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-06T00:00:00+00:00, run_after=2023-07-07T00:00:00+00:00[0m
[[34m2023-09-11T06:54:07.338+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-04 00:00:00+00:00: scheduled__2023-07-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:02.288842+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:07.338+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-04 00:00:00+00:00, run_id=scheduled__2023-07-04T00:00:00+00:00, run_start_date=2023-09-11 06:54:02.306828+00:00, run_end_date=2023-09-11 06:54:07.338338+00:00, run_duration=5.03151, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-04 00:00:00+00:00, data_interval_end=2023-07-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:07.342+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-05T00:00:00+00:00, run_after=2023-07-06T00:00:00+00:00[0m
[[34m2023-09-11T06:54:07.359+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:07.359+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:07.359+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:07.361+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:07.363+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:07.363+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:07.367+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:09.478+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:09.625+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:09.835+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:09.870+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:10.407+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:10.408+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:10.486+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:10.551+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:11.354+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:11.367+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:10.648024+00:00, run_end_date=2023-09-11 06:54:10.901319+00:00, run_duration=0.253295, state=success, executor_state=success, try_number=1, max_tries=0, job_id=190, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:07.360357+00:00, queued_by_job_id=2, pid=42574[0m
[[34m2023-09-11T06:54:11.880+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-06T00:00:00+00:00, run_after=2023-07-07T00:00:00+00:00[0m
[[34m2023-09-11T06:54:11.911+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-05 00:00:00+00:00: scheduled__2023-07-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:07.293528+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:11.913+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-05 00:00:00+00:00, run_id=scheduled__2023-07-05T00:00:00+00:00, run_start_date=2023-09-11 06:54:07.314011+00:00, run_end_date=2023-09-11 06:54:11.912537+00:00, run_duration=4.598526, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-05 00:00:00+00:00, data_interval_end=2023-07-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:11.917+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-06T00:00:00+00:00, run_after=2023-07-07T00:00:00+00:00[0m
[[34m2023-09-11T06:54:13.497+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-07T00:00:00+00:00, run_after=2023-07-08T00:00:00+00:00[0m
[[34m2023-09-11T06:54:13.545+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:13.546+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:13.547+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:13.549+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:13.550+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:13.550+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:13.554+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:15.660+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:15.807+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:16.001+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:16.036+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:16.587+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:16.588+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:16.671+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:16.741+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:17.559+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:17.571+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:16.839026+00:00, run_end_date=2023-09-11 06:54:17.098050+00:00, run_duration=0.259024, state=success, executor_state=success, try_number=1, max_tries=0, job_id=191, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:13.547939+00:00, queued_by_job_id=2, pid=42585[0m
[[34m2023-09-11T06:54:18.085+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-08T00:00:00+00:00, run_after=2023-07-09T00:00:00+00:00[0m
[[34m2023-09-11T06:54:18.131+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-06 00:00:00+00:00: scheduled__2023-07-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:13.491010+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:18.132+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-06 00:00:00+00:00, run_id=scheduled__2023-07-06T00:00:00+00:00, run_start_date=2023-09-11 06:54:13.510710+00:00, run_end_date=2023-09-11 06:54:18.132029+00:00, run_duration=4.621319, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-06 00:00:00+00:00, data_interval_end=2023-07-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:18.135+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-07T00:00:00+00:00, run_after=2023-07-08T00:00:00+00:00[0m
[[34m2023-09-11T06:54:18.154+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:18.154+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:18.155+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:18.158+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:18.159+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:18.160+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:18.167+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:20.350+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:20.494+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:20.692+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:20.735+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:21.280+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:21.281+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:21.357+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:21.422+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:22.226+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:22.238+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:21.515069+00:00, run_end_date=2023-09-11 06:54:21.769449+00:00, run_duration=0.25438, state=success, executor_state=success, try_number=1, max_tries=0, job_id=192, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:18.156628+00:00, queued_by_job_id=2, pid=42594[0m
[[34m2023-09-11T06:54:22.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-08T00:00:00+00:00, run_after=2023-07-09T00:00:00+00:00[0m
[[34m2023-09-11T06:54:22.701+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-07 00:00:00+00:00: scheduled__2023-07-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:18.080520+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:22.702+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-07 00:00:00+00:00, run_id=scheduled__2023-07-07T00:00:00+00:00, run_start_date=2023-09-11 06:54:18.100551+00:00, run_end_date=2023-09-11 06:54:22.702209+00:00, run_duration=4.601658, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-07 00:00:00+00:00, data_interval_end=2023-07-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:22.705+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-08T00:00:00+00:00, run_after=2023-07-09T00:00:00+00:00[0m
[[34m2023-09-11T06:54:24.248+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-09T00:00:00+00:00, run_after=2023-07-10T00:00:00+00:00[0m
[[34m2023-09-11T06:54:24.296+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:24.297+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:24.297+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:24.301+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:24.302+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:24.302+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:24.306+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:26.439+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:26.584+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:26.775+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:26.811+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:27.333+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:27.334+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:27.415+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:27.482+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:28.466+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:28.478+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:27.613790+00:00, run_end_date=2023-09-11 06:54:27.971178+00:00, run_duration=0.357388, state=success, executor_state=success, try_number=1, max_tries=0, job_id=193, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:24.298815+00:00, queued_by_job_id=2, pid=42604[0m
[[34m2023-09-11T06:54:28.648+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-10T00:00:00+00:00, run_after=2023-07-11T00:00:00+00:00[0m
[[34m2023-09-11T06:54:28.689+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-08 00:00:00+00:00: scheduled__2023-07-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:24.242705+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:28.689+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-08 00:00:00+00:00, run_id=scheduled__2023-07-08T00:00:00+00:00, run_start_date=2023-09-11 06:54:24.262263+00:00, run_end_date=2023-09-11 06:54:28.689636+00:00, run_duration=4.427373, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-08 00:00:00+00:00, data_interval_end=2023-07-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:28.693+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-09T00:00:00+00:00, run_after=2023-07-10T00:00:00+00:00[0m
[[34m2023-09-11T06:54:28.709+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:28.709+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:28.709+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:28.713+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:28.714+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:28.714+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:28.717+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:31.022+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:31.186+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:31.402+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:31.458+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:32.154+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:32.154+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:32.239+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:32.304+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:33.555+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:33.567+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:32.401298+00:00, run_end_date=2023-09-11 06:54:32.649178+00:00, run_duration=0.24788, state=success, executor_state=success, try_number=1, max_tries=0, job_id=194, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:28.710663+00:00, queued_by_job_id=2, pid=42613[0m
[[34m2023-09-11T06:54:34.618+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-10T00:00:00+00:00, run_after=2023-07-11T00:00:00+00:00[0m
[[34m2023-09-11T06:54:34.665+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-09 00:00:00+00:00: scheduled__2023-07-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:28.641240+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:34.667+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-09 00:00:00+00:00, run_id=scheduled__2023-07-09T00:00:00+00:00, run_start_date=2023-09-11 06:54:28.664843+00:00, run_end_date=2023-09-11 06:54:34.666824+00:00, run_duration=6.001981, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-09 00:00:00+00:00, data_interval_end=2023-07-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:34.672+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-10T00:00:00+00:00, run_after=2023-07-11T00:00:00+00:00[0m
[[34m2023-09-11T06:54:36.036+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-11T00:00:00+00:00, run_after=2023-07-12T00:00:00+00:00[0m
[[34m2023-09-11T06:54:36.102+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:36.103+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:36.103+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:36.105+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:36.106+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:36.106+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:36.109+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:38.205+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:38.385+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:38.689+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:38.732+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:39.489+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:39.490+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:39.607+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:39.738+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:40.815+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:40.832+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:39.871413+00:00, run_end_date=2023-09-11 06:54:40.248140+00:00, run_duration=0.376727, state=success, executor_state=success, try_number=1, max_tries=0, job_id=195, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:36.104169+00:00, queued_by_job_id=2, pid=42627[0m
[[34m2023-09-11T06:54:41.253+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-12T00:00:00+00:00, run_after=2023-07-13T00:00:00+00:00[0m
[[34m2023-09-11T06:54:41.292+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-10 00:00:00+00:00: scheduled__2023-07-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:36.031482+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:41.292+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-10 00:00:00+00:00, run_id=scheduled__2023-07-10T00:00:00+00:00, run_start_date=2023-09-11 06:54:36.050323+00:00, run_end_date=2023-09-11 06:54:41.292819+00:00, run_duration=5.242496, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-10 00:00:00+00:00, data_interval_end=2023-07-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:41.298+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-11T00:00:00+00:00, run_after=2023-07-12T00:00:00+00:00[0m
[[34m2023-09-11T06:54:41.315+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:41.317+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:41.317+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:41.319+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:41.320+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:41.320+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:41.323+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:43.689+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:43.843+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:44.072+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:44.109+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:44.691+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:44.692+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:44.772+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:44.836+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:45.750+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:45.761+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:44.949330+00:00, run_end_date=2023-09-11 06:54:45.238007+00:00, run_duration=0.288677, state=success, executor_state=success, try_number=1, max_tries=0, job_id=196, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:41.318364+00:00, queued_by_job_id=2, pid=42636[0m
[[34m2023-09-11T06:54:46.031+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-12T00:00:00+00:00, run_after=2023-07-13T00:00:00+00:00[0m
[[34m2023-09-11T06:54:46.057+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-11 00:00:00+00:00: scheduled__2023-07-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:41.245852+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:46.058+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-11 00:00:00+00:00, run_id=scheduled__2023-07-11T00:00:00+00:00, run_start_date=2023-09-11 06:54:41.267260+00:00, run_end_date=2023-09-11 06:54:46.058030+00:00, run_duration=4.79077, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-11 00:00:00+00:00, data_interval_end=2023-07-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:46.063+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-12T00:00:00+00:00, run_after=2023-07-13T00:00:00+00:00[0m
[[34m2023-09-11T06:54:47.436+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-13T00:00:00+00:00, run_after=2023-07-14T00:00:00+00:00[0m
[[34m2023-09-11T06:54:47.488+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:47.489+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:47.489+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:47.491+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:47.492+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:47.492+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:47.496+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:49.803+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:49.984+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:50.190+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:50.226+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:50.783+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:50.784+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:50.905+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:51.001+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:51.819+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:51.831+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:51.092809+00:00, run_end_date=2023-09-11 06:54:51.371963+00:00, run_duration=0.279154, state=success, executor_state=success, try_number=1, max_tries=0, job_id=197, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:47.490254+00:00, queued_by_job_id=2, pid=42646[0m
[[34m2023-09-11T06:54:52.000+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-14T00:00:00+00:00, run_after=2023-07-15T00:00:00+00:00[0m
[[34m2023-09-11T06:54:52.038+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-12 00:00:00+00:00: scheduled__2023-07-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:47.431722+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:52.038+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-12 00:00:00+00:00, run_id=scheduled__2023-07-12T00:00:00+00:00, run_start_date=2023-09-11 06:54:47.450896+00:00, run_end_date=2023-09-11 06:54:52.038704+00:00, run_duration=4.587808, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-12 00:00:00+00:00, data_interval_end=2023-07-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:52.042+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-13T00:00:00+00:00, run_after=2023-07-14T00:00:00+00:00[0m
[[34m2023-09-11T06:54:52.058+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:52.058+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:52.058+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:52.062+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:52.063+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:52.063+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:52.066+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:54.152+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:54.277+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:54.453+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:54.487+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:54.970+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:54:54.971+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:55.039+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:54:55.097+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:54:55.858+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:54:55.868+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:54:55.192531+00:00, run_end_date=2023-09-11 06:54:55.427636+00:00, run_duration=0.235105, state=success, executor_state=success, try_number=1, max_tries=0, job_id=198, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:52.059527+00:00, queued_by_job_id=2, pid=42655[0m
[[34m2023-09-11T06:54:56.156+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-14T00:00:00+00:00, run_after=2023-07-15T00:00:00+00:00[0m
[[34m2023-09-11T06:54:56.191+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-13 00:00:00+00:00: scheduled__2023-07-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:51.994203+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:54:56.191+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-13 00:00:00+00:00, run_id=scheduled__2023-07-13T00:00:00+00:00, run_start_date=2023-09-11 06:54:52.014384+00:00, run_end_date=2023-09-11 06:54:56.191606+00:00, run_duration=4.177222, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-13 00:00:00+00:00, data_interval_end=2023-07-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:54:56.198+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-14T00:00:00+00:00, run_after=2023-07-15T00:00:00+00:00[0m
[[34m2023-09-11T06:54:57.053+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-15T00:00:00+00:00, run_after=2023-07-16T00:00:00+00:00[0m
[[34m2023-09-11T06:54:57.101+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:57.102+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:54:57.102+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:54:57.104+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:54:57.105+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:54:57.105+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:57.107+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:54:59.205+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:54:59.347+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:54:59.539+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:54:59.574+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:00.098+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:00.099+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:00.182+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:00.245+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:01.069+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:01.081+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:00.343275+00:00, run_end_date=2023-09-11 06:55:00.586382+00:00, run_duration=0.243107, state=success, executor_state=success, try_number=1, max_tries=0, job_id=199, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:54:57.103006+00:00, queued_by_job_id=2, pid=42665[0m
[[34m2023-09-11T06:55:01.106+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T06:55:01.259+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-16T00:00:00+00:00, run_after=2023-07-17T00:00:00+00:00[0m
[[34m2023-09-11T06:55:01.306+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-14 00:00:00+00:00: scheduled__2023-07-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:54:57.045636+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:01.306+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-14 00:00:00+00:00, run_id=scheduled__2023-07-14T00:00:00+00:00, run_start_date=2023-09-11 06:54:57.067179+00:00, run_end_date=2023-09-11 06:55:01.306496+00:00, run_duration=4.239317, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-14 00:00:00+00:00, data_interval_end=2023-07-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:01.310+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-15T00:00:00+00:00, run_after=2023-07-16T00:00:00+00:00[0m
[[34m2023-09-11T06:55:01.326+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:01.327+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:01.327+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:01.330+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:01.331+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:01.331+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:01.334+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:03.435+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:03.633+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:03.906+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:03.944+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:04.545+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:04.546+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:04.636+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:04.704+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:05.667+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:05.681+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:04.804175+00:00, run_end_date=2023-09-11 06:55:05.078817+00:00, run_duration=0.274642, state=success, executor_state=success, try_number=1, max_tries=0, job_id=200, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:01.329273+00:00, queued_by_job_id=2, pid=42674[0m
[[34m2023-09-11T06:55:05.847+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-16T00:00:00+00:00, run_after=2023-07-17T00:00:00+00:00[0m
[[34m2023-09-11T06:55:05.880+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-15 00:00:00+00:00: scheduled__2023-07-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:01.254254+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:05.881+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-15 00:00:00+00:00, run_id=scheduled__2023-07-15T00:00:00+00:00, run_start_date=2023-09-11 06:55:01.281975+00:00, run_end_date=2023-09-11 06:55:05.881119+00:00, run_duration=4.599144, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-15 00:00:00+00:00, data_interval_end=2023-07-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:05.884+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-16T00:00:00+00:00, run_after=2023-07-17T00:00:00+00:00[0m
[[34m2023-09-11T06:55:07.010+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-17T00:00:00+00:00, run_after=2023-07-18T00:00:00+00:00[0m
[[34m2023-09-11T06:55:07.053+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:07.054+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:07.054+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:07.056+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:07.057+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:07.057+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:07.060+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:09.742+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:09.899+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:10.231+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:10.283+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:10.863+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:10.864+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:10.964+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:11.040+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:11.921+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:11.932+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:11.147530+00:00, run_end_date=2023-09-11 06:55:11.454943+00:00, run_duration=0.307413, state=success, executor_state=success, try_number=1, max_tries=0, job_id=201, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:07.055183+00:00, queued_by_job_id=2, pid=42684[0m
[[34m2023-09-11T06:55:12.201+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-18T00:00:00+00:00, run_after=2023-07-19T00:00:00+00:00[0m
[[34m2023-09-11T06:55:12.241+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-16 00:00:00+00:00: scheduled__2023-07-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:07.005075+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:12.241+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-16 00:00:00+00:00, run_id=scheduled__2023-07-16T00:00:00+00:00, run_start_date=2023-09-11 06:55:07.022617+00:00, run_end_date=2023-09-11 06:55:12.241411+00:00, run_duration=5.218794, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-16 00:00:00+00:00, data_interval_end=2023-07-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:12.246+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-17T00:00:00+00:00, run_after=2023-07-18T00:00:00+00:00[0m
[[34m2023-09-11T06:55:12.262+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:12.263+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:12.263+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:12.265+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:12.266+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:12.266+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:12.269+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:14.338+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:14.479+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:14.657+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:14.690+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:15.206+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:15.206+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:15.283+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:15.346+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:16.189+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:16.201+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:15.440375+00:00, run_end_date=2023-09-11 06:55:15.689680+00:00, run_duration=0.249305, state=success, executor_state=success, try_number=1, max_tries=0, job_id=202, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:12.264074+00:00, queued_by_job_id=2, pid=42693[0m
[[34m2023-09-11T06:55:16.449+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-18T00:00:00+00:00, run_after=2023-07-19T00:00:00+00:00[0m
[[34m2023-09-11T06:55:16.475+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-17 00:00:00+00:00: scheduled__2023-07-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:12.196003+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:16.476+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-17 00:00:00+00:00, run_id=scheduled__2023-07-17T00:00:00+00:00, run_start_date=2023-09-11 06:55:12.216049+00:00, run_end_date=2023-09-11 06:55:16.476678+00:00, run_duration=4.260629, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-17 00:00:00+00:00, data_interval_end=2023-07-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:16.482+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-18T00:00:00+00:00, run_after=2023-07-19T00:00:00+00:00[0m
[[34m2023-09-11T06:55:17.218+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-19T00:00:00+00:00, run_after=2023-07-20T00:00:00+00:00[0m
[[34m2023-09-11T06:55:17.266+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:17.266+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:17.267+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:17.269+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:17.269+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:17.270+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:17.273+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:19.329+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:19.464+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:19.649+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:19.683+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:20.178+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:20.178+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:20.255+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:20.318+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:21.057+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:21.068+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:20.407619+00:00, run_end_date=2023-09-11 06:55:20.642279+00:00, run_duration=0.23466, state=success, executor_state=success, try_number=1, max_tries=0, job_id=203, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:17.267791+00:00, queued_by_job_id=2, pid=42703[0m
[[34m2023-09-11T06:55:21.370+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-20T00:00:00+00:00, run_after=2023-07-21T00:00:00+00:00[0m
[[34m2023-09-11T06:55:21.408+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-18 00:00:00+00:00: scheduled__2023-07-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:17.213662+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:21.409+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-18 00:00:00+00:00, run_id=scheduled__2023-07-18T00:00:00+00:00, run_start_date=2023-09-11 06:55:17.232115+00:00, run_end_date=2023-09-11 06:55:21.409374+00:00, run_duration=4.177259, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-18 00:00:00+00:00, data_interval_end=2023-07-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:21.413+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-19T00:00:00+00:00, run_after=2023-07-20T00:00:00+00:00[0m
[[34m2023-09-11T06:55:21.430+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:21.430+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:21.431+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:21.433+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:21.434+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:21.434+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:21.436+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:23.558+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:23.699+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:23.879+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:23.915+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:24.410+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:24.410+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:24.483+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:24.545+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:25.351+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:25.362+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:24.637263+00:00, run_end_date=2023-09-11 06:55:24.882040+00:00, run_duration=0.244777, state=success, executor_state=success, try_number=1, max_tries=0, job_id=204, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:21.431948+00:00, queued_by_job_id=2, pid=42712[0m
[[34m2023-09-11T06:55:25.633+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-20T00:00:00+00:00, run_after=2023-07-21T00:00:00+00:00[0m
[[34m2023-09-11T06:55:25.657+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-19 00:00:00+00:00: scheduled__2023-07-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:21.365018+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:25.657+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-19 00:00:00+00:00, run_id=scheduled__2023-07-19T00:00:00+00:00, run_start_date=2023-09-11 06:55:21.384868+00:00, run_end_date=2023-09-11 06:55:25.657474+00:00, run_duration=4.272606, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-19 00:00:00+00:00, data_interval_end=2023-07-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:25.662+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-20T00:00:00+00:00, run_after=2023-07-21T00:00:00+00:00[0m
[[34m2023-09-11T06:55:26.339+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-21T00:00:00+00:00, run_after=2023-07-22T00:00:00+00:00[0m
[[34m2023-09-11T06:55:26.387+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:26.387+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:26.388+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:26.390+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:26.391+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:26.391+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:26.394+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:28.366+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:28.502+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:28.680+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:28.714+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:29.238+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:29.238+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:29.313+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:29.375+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:30.136+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:30.147+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:29.466422+00:00, run_end_date=2023-09-11 06:55:29.700224+00:00, run_duration=0.233802, state=success, executor_state=success, try_number=1, max_tries=0, job_id=205, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:26.388950+00:00, queued_by_job_id=2, pid=42720[0m
[[34m2023-09-11T06:55:30.418+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-22T00:00:00+00:00, run_after=2023-07-23T00:00:00+00:00[0m
[[34m2023-09-11T06:55:30.456+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-20 00:00:00+00:00: scheduled__2023-07-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:26.334644+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:30.456+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-20 00:00:00+00:00, run_id=scheduled__2023-07-20T00:00:00+00:00, run_start_date=2023-09-11 06:55:26.352150+00:00, run_end_date=2023-09-11 06:55:30.456412+00:00, run_duration=4.104262, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-20 00:00:00+00:00, data_interval_end=2023-07-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:30.460+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-21T00:00:00+00:00, run_after=2023-07-22T00:00:00+00:00[0m
[[34m2023-09-11T06:55:30.477+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:30.478+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:30.478+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:30.480+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:30.481+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:30.481+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:30.484+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:32.490+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:32.622+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:32.795+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:32.830+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:33.313+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:33.314+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:33.387+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:33.451+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:34.218+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:34.229+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:33.539189+00:00, run_end_date=2023-09-11 06:55:33.815394+00:00, run_duration=0.276205, state=success, executor_state=success, try_number=1, max_tries=0, job_id=206, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:30.479015+00:00, queued_by_job_id=2, pid=42729[0m
[[34m2023-09-11T06:55:34.482+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-22T00:00:00+00:00, run_after=2023-07-23T00:00:00+00:00[0m
[[34m2023-09-11T06:55:34.505+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-21 00:00:00+00:00: scheduled__2023-07-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:30.413049+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:34.506+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-21 00:00:00+00:00, run_id=scheduled__2023-07-21T00:00:00+00:00, run_start_date=2023-09-11 06:55:30.431894+00:00, run_end_date=2023-09-11 06:55:34.506187+00:00, run_duration=4.074293, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-21 00:00:00+00:00, data_interval_end=2023-07-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:34.510+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-22T00:00:00+00:00, run_after=2023-07-23T00:00:00+00:00[0m
[[34m2023-09-11T06:55:35.405+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-23T00:00:00+00:00, run_after=2023-07-24T00:00:00+00:00[0m
[[34m2023-09-11T06:55:35.451+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:35.451+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:35.451+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:35.453+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:35.454+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:35.454+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:35.458+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:37.418+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:37.556+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:37.742+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:37.777+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:38.277+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:38.278+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:38.350+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:38.412+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:39.211+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:39.222+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:38.502151+00:00, run_end_date=2023-09-11 06:55:38.764470+00:00, run_duration=0.262319, state=success, executor_state=success, try_number=1, max_tries=0, job_id=207, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:35.452384+00:00, queued_by_job_id=2, pid=42737[0m
[[34m2023-09-11T06:55:39.491+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-24T00:00:00+00:00, run_after=2023-07-25T00:00:00+00:00[0m
[[34m2023-09-11T06:55:39.529+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-22 00:00:00+00:00: scheduled__2023-07-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:35.400529+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:39.529+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-22 00:00:00+00:00, run_id=scheduled__2023-07-22T00:00:00+00:00, run_start_date=2023-09-11 06:55:35.417858+00:00, run_end_date=2023-09-11 06:55:39.529524+00:00, run_duration=4.111666, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-22 00:00:00+00:00, data_interval_end=2023-07-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:39.533+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-23T00:00:00+00:00, run_after=2023-07-24T00:00:00+00:00[0m
[[34m2023-09-11T06:55:39.550+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:39.550+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:39.550+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:39.553+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:39.554+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:39.554+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:39.557+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:41.614+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:41.765+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:41.948+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:41.983+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:42.474+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:42.475+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:42.549+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:42.609+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:43.351+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:43.363+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:42.704420+00:00, run_end_date=2023-09-11 06:55:42.936300+00:00, run_duration=0.23188, state=success, executor_state=success, try_number=1, max_tries=0, job_id=208, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:39.551507+00:00, queued_by_job_id=2, pid=42746[0m
[[34m2023-09-11T06:55:43.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-24T00:00:00+00:00, run_after=2023-07-25T00:00:00+00:00[0m
[[34m2023-09-11T06:55:43.698+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-23 00:00:00+00:00: scheduled__2023-07-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:39.485936+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:43.699+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-23 00:00:00+00:00, run_id=scheduled__2023-07-23T00:00:00+00:00, run_start_date=2023-09-11 06:55:39.504182+00:00, run_end_date=2023-09-11 06:55:43.699310+00:00, run_duration=4.195128, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-23 00:00:00+00:00, data_interval_end=2023-07-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:43.703+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-24T00:00:00+00:00, run_after=2023-07-25T00:00:00+00:00[0m
[[34m2023-09-11T06:55:44.486+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-25T00:00:00+00:00, run_after=2023-07-26T00:00:00+00:00[0m
[[34m2023-09-11T06:55:44.535+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:44.535+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:44.536+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:44.538+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:44.538+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:44.538+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:44.542+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:46.470+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:46.601+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:46.778+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:46.814+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:47.310+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:47.310+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:47.382+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:47.443+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:48.267+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:48.278+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:47.532567+00:00, run_end_date=2023-09-11 06:55:47.839668+00:00, run_duration=0.307101, state=success, executor_state=success, try_number=1, max_tries=0, job_id=209, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:44.536897+00:00, queued_by_job_id=2, pid=42756[0m
[[34m2023-09-11T06:55:48.538+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-26T00:00:00+00:00, run_after=2023-07-27T00:00:00+00:00[0m
[[34m2023-09-11T06:55:48.576+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-24 00:00:00+00:00: scheduled__2023-07-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:44.481968+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:48.577+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-24 00:00:00+00:00, run_id=scheduled__2023-07-24T00:00:00+00:00, run_start_date=2023-09-11 06:55:44.501676+00:00, run_end_date=2023-09-11 06:55:48.577145+00:00, run_duration=4.075469, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-24 00:00:00+00:00, data_interval_end=2023-07-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:48.580+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-25T00:00:00+00:00, run_after=2023-07-26T00:00:00+00:00[0m
[[34m2023-09-11T06:55:48.596+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:48.597+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:48.597+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:48.599+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:48.600+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:48.600+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:48.603+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:50.616+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:50.754+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:50.932+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:50.968+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:51.467+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:51.468+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:51.546+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:51.611+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:52.394+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:52.405+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:51.703033+00:00, run_end_date=2023-09-11 06:55:51.940769+00:00, run_duration=0.237736, state=success, executor_state=success, try_number=1, max_tries=0, job_id=210, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:48.598049+00:00, queued_by_job_id=2, pid=42765[0m
[[34m2023-09-11T06:55:52.647+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-26T00:00:00+00:00, run_after=2023-07-27T00:00:00+00:00[0m
[[34m2023-09-11T06:55:52.671+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-25 00:00:00+00:00: scheduled__2023-07-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:48.533156+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:52.671+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-25 00:00:00+00:00, run_id=scheduled__2023-07-25T00:00:00+00:00, run_start_date=2023-09-11 06:55:48.550761+00:00, run_end_date=2023-09-11 06:55:52.671657+00:00, run_duration=4.120896, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-25 00:00:00+00:00, data_interval_end=2023-07-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:52.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-26T00:00:00+00:00, run_after=2023-07-27T00:00:00+00:00[0m
[[34m2023-09-11T06:55:54.308+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-27T00:00:00+00:00, run_after=2023-07-28T00:00:00+00:00[0m
[[34m2023-09-11T06:55:54.355+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:54.355+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:54.355+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:54.358+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:54.359+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:54.359+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:54.362+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:56.320+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:55:56.458+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:56.644+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:56.678+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:55:57.214+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:55:57.214+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:55:57.291+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:55:57.358+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:55:58.305+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:55:58.317+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:55:57.449518+00:00, run_end_date=2023-09-11 06:55:57.805579+00:00, run_duration=0.356061, state=success, executor_state=success, try_number=1, max_tries=0, job_id=211, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:54.356554+00:00, queued_by_job_id=2, pid=42775[0m
[[34m2023-09-11T06:55:58.596+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-28T00:00:00+00:00, run_after=2023-07-29T00:00:00+00:00[0m
[[34m2023-09-11T06:55:58.649+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-26 00:00:00+00:00: scheduled__2023-07-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:54.303787+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:55:58.650+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-26 00:00:00+00:00, run_id=scheduled__2023-07-26T00:00:00+00:00, run_start_date=2023-09-11 06:55:54.321707+00:00, run_end_date=2023-09-11 06:55:58.649870+00:00, run_duration=4.328163, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-26 00:00:00+00:00, data_interval_end=2023-07-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:55:58.657+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-27T00:00:00+00:00, run_after=2023-07-28T00:00:00+00:00[0m
[[34m2023-09-11T06:55:58.686+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:58.686+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:55:58.687+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:55:58.690+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:55:58.692+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:55:58.694+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:55:58.700+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:01.013+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:01.149+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:01.332+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:01.366+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:01.885+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:01.886+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:01.966+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:02.022+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:02.787+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:02.798+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:02.110797+00:00, run_end_date=2023-09-11 06:56:02.338804+00:00, run_duration=0.228007, state=success, executor_state=success, try_number=1, max_tries=0, job_id=212, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:55:58.688572+00:00, queued_by_job_id=2, pid=42784[0m
[[34m2023-09-11T06:56:03.052+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-28T00:00:00+00:00, run_after=2023-07-29T00:00:00+00:00[0m
[[34m2023-09-11T06:56:03.077+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-27 00:00:00+00:00: scheduled__2023-07-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:55:58.588156+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:03.077+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-27 00:00:00+00:00, run_id=scheduled__2023-07-27T00:00:00+00:00, run_start_date=2023-09-11 06:55:58.614558+00:00, run_end_date=2023-09-11 06:56:03.077572+00:00, run_duration=4.463014, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-27 00:00:00+00:00, data_interval_end=2023-07-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:03.081+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-28T00:00:00+00:00, run_after=2023-07-29T00:00:00+00:00[0m
[[34m2023-09-11T06:56:03.700+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-29T00:00:00+00:00, run_after=2023-07-30T00:00:00+00:00[0m
[[34m2023-09-11T06:56:03.777+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:03.778+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:03.778+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:03.781+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:03.782+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:03.783+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:03.786+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:05.836+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:05.980+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:06.181+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:06.214+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:06.725+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:06.726+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:06.799+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:06.861+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:07.628+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:07.638+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:06.949700+00:00, run_end_date=2023-09-11 06:56:07.181916+00:00, run_duration=0.232216, state=success, executor_state=success, try_number=1, max_tries=0, job_id=213, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:03.779837+00:00, queued_by_job_id=2, pid=42794[0m
[[34m2023-09-11T06:56:07.911+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-30T00:00:00+00:00, run_after=2023-07-31T00:00:00+00:00[0m
[[34m2023-09-11T06:56:07.947+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-28 00:00:00+00:00: scheduled__2023-07-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:03.695261+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:07.948+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-28 00:00:00+00:00, run_id=scheduled__2023-07-28T00:00:00+00:00, run_start_date=2023-09-11 06:56:03.714862+00:00, run_end_date=2023-09-11 06:56:07.948099+00:00, run_duration=4.233237, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-28 00:00:00+00:00, data_interval_end=2023-07-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:07.951+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-29T00:00:00+00:00, run_after=2023-07-30T00:00:00+00:00[0m
[[34m2023-09-11T06:56:07.967+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:07.967+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:07.968+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:07.970+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:07.970+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:07.971+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:07.974+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:09.928+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:10.078+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:10.286+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:10.326+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:10.796+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:10.797+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:10.874+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:10.941+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:12.212+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:12.224+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:11.028068+00:00, run_end_date=2023-09-11 06:56:11.764967+00:00, run_duration=0.736899, state=success, executor_state=success, try_number=1, max_tries=0, job_id=214, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:07.968788+00:00, queued_by_job_id=2, pid=42803[0m
[[34m2023-09-11T06:56:12.383+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-30T00:00:00+00:00, run_after=2023-07-31T00:00:00+00:00[0m
[[34m2023-09-11T06:56:12.409+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-29 00:00:00+00:00: scheduled__2023-07-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:07.905530+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:12.409+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-29 00:00:00+00:00, run_id=scheduled__2023-07-29T00:00:00+00:00, run_start_date=2023-09-11 06:56:07.924346+00:00, run_end_date=2023-09-11 06:56:12.409696+00:00, run_duration=4.48535, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-29 00:00:00+00:00, data_interval_end=2023-07-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:12.413+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-30T00:00:00+00:00, run_after=2023-07-31T00:00:00+00:00[0m
[[34m2023-09-11T06:56:12.955+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-31T00:00:00+00:00, run_after=2023-08-01T00:00:00+00:00[0m
[[34m2023-09-11T06:56:13.006+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:13.006+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:13.006+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:13.009+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:13.009+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:13.010+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:13.012+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:14.881+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:15.009+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:15.182+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:15.216+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:15.698+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:15.699+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:15.771+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:15.833+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:16.586+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:16.596+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:15.945284+00:00, run_end_date=2023-09-11 06:56:16.174434+00:00, run_duration=0.22915, state=success, executor_state=success, try_number=1, max_tries=0, job_id=215, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:13.007757+00:00, queued_by_job_id=2, pid=42813[0m
[[34m2023-09-11T06:56:16.875+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-01T00:00:00+00:00, run_after=2023-08-02T00:00:00+00:00[0m
[[34m2023-09-11T06:56:16.929+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-30 00:00:00+00:00: scheduled__2023-07-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:12.950316+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:16.930+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-30 00:00:00+00:00, run_id=scheduled__2023-07-30T00:00:00+00:00, run_start_date=2023-09-11 06:56:12.970854+00:00, run_end_date=2023-09-11 06:56:16.929962+00:00, run_duration=3.959108, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-30 00:00:00+00:00, data_interval_end=2023-07-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:16.933+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-07-31T00:00:00+00:00, run_after=2023-08-01T00:00:00+00:00[0m
[[34m2023-09-11T06:56:16.948+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:16.948+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:16.948+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-07-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:16.951+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:16.951+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:16.951+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:16.954+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:19.031+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:19.189+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:19.538+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:19.578+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:20.233+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:20.233+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:20.337+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-07-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:20.422+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-07-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:21.396+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:21.408+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-07-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:20.514137+00:00, run_end_date=2023-09-11 06:56:20.848814+00:00, run_duration=0.334677, state=success, executor_state=success, try_number=1, max_tries=0, job_id=216, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:16.949565+00:00, queued_by_job_id=2, pid=42822[0m
[[34m2023-09-11T06:56:21.748+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-01T00:00:00+00:00, run_after=2023-08-02T00:00:00+00:00[0m
[[34m2023-09-11T06:56:21.771+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-07-31 00:00:00+00:00: scheduled__2023-07-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:16.870609+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:21.772+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-07-31 00:00:00+00:00, run_id=scheduled__2023-07-31T00:00:00+00:00, run_start_date=2023-09-11 06:56:16.889503+00:00, run_end_date=2023-09-11 06:56:21.772450+00:00, run_duration=4.882947, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-31 00:00:00+00:00, data_interval_end=2023-08-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:21.779+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-01T00:00:00+00:00, run_after=2023-08-02T00:00:00+00:00[0m
[[34m2023-09-11T06:56:22.758+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-02T00:00:00+00:00, run_after=2023-08-03T00:00:00+00:00[0m
[[34m2023-09-11T06:56:23.078+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:23.078+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:23.078+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:23.082+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:23.083+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:23.083+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:23.089+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:26.032+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:26.199+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:26.843+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:26.979+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:27.970+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:27.970+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:28.070+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:28.163+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:29.105+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:29.120+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:28.288214+00:00, run_end_date=2023-09-11 06:56:28.610638+00:00, run_duration=0.322424, state=success, executor_state=success, try_number=1, max_tries=0, job_id=217, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:23.080116+00:00, queued_by_job_id=2, pid=42832[0m
[[34m2023-09-11T06:56:29.403+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-03T00:00:00+00:00, run_after=2023-08-04T00:00:00+00:00[0m
[[34m2023-09-11T06:56:29.439+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-01 00:00:00+00:00: scheduled__2023-08-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:22.749624+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:29.440+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-01 00:00:00+00:00, run_id=scheduled__2023-08-01T00:00:00+00:00, run_start_date=2023-09-11 06:56:22.995080+00:00, run_end_date=2023-09-11 06:56:29.440045+00:00, run_duration=6.444965, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-01 00:00:00+00:00, data_interval_end=2023-08-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:29.444+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-02T00:00:00+00:00, run_after=2023-08-03T00:00:00+00:00[0m
[[34m2023-09-11T06:56:29.460+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:29.461+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:29.461+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:29.463+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:29.463+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:29.464+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:29.466+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:31.343+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:31.478+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:31.658+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:31.702+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:32.181+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:32.182+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:32.256+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:32.322+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:33.065+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:33.076+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:32.408941+00:00, run_end_date=2023-09-11 06:56:32.676321+00:00, run_duration=0.26738, state=success, executor_state=success, try_number=1, max_tries=0, job_id=218, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:29.462035+00:00, queued_by_job_id=2, pid=42841[0m
[[34m2023-09-11T06:56:33.319+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-03T00:00:00+00:00, run_after=2023-08-04T00:00:00+00:00[0m
[[34m2023-09-11T06:56:33.343+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-02 00:00:00+00:00: scheduled__2023-08-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:29.397859+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:33.343+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-02 00:00:00+00:00, run_id=scheduled__2023-08-02T00:00:00+00:00, run_start_date=2023-09-11 06:56:29.415989+00:00, run_end_date=2023-09-11 06:56:33.343514+00:00, run_duration=3.927525, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-02 00:00:00+00:00, data_interval_end=2023-08-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:33.347+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-03T00:00:00+00:00, run_after=2023-08-04T00:00:00+00:00[0m
[[34m2023-09-11T06:56:34.482+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-04T00:00:00+00:00, run_after=2023-08-05T00:00:00+00:00[0m
[[34m2023-09-11T06:56:34.526+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:34.527+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:34.527+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:34.529+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:34.530+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:34.530+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:34.533+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:36.703+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:36.861+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:37.060+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:37.107+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:37.742+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:37.743+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:37.947+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:38.053+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:38.931+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:38.947+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:38.156239+00:00, run_end_date=2023-09-11 06:56:38.425193+00:00, run_duration=0.268954, state=success, executor_state=success, try_number=1, max_tries=0, job_id=219, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:34.528069+00:00, queued_by_job_id=2, pid=42850[0m
[[34m2023-09-11T06:56:39.270+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-05T00:00:00+00:00, run_after=2023-08-06T00:00:00+00:00[0m
[[34m2023-09-11T06:56:39.316+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-03 00:00:00+00:00: scheduled__2023-08-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:34.478241+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:39.317+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-03 00:00:00+00:00, run_id=scheduled__2023-08-03T00:00:00+00:00, run_start_date=2023-09-11 06:56:34.495331+00:00, run_end_date=2023-09-11 06:56:39.317398+00:00, run_duration=4.822067, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-03 00:00:00+00:00, data_interval_end=2023-08-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:39.321+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-04T00:00:00+00:00, run_after=2023-08-05T00:00:00+00:00[0m
[[34m2023-09-11T06:56:39.338+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:39.338+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:39.339+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:39.342+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:39.343+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:39.343+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:39.346+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:41.896+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:42.132+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:42.370+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:42.438+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:43.060+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:43.061+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:43.192+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:43.257+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:44.193+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:44.205+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:43.355426+00:00, run_end_date=2023-09-11 06:56:43.660484+00:00, run_duration=0.305058, state=success, executor_state=success, try_number=1, max_tries=0, job_id=220, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:39.341103+00:00, queued_by_job_id=2, pid=42859[0m
[[34m2023-09-11T06:56:44.631+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-05T00:00:00+00:00, run_after=2023-08-06T00:00:00+00:00[0m
[[34m2023-09-11T06:56:44.667+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-04 00:00:00+00:00: scheduled__2023-08-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:39.263765+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:44.668+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-04 00:00:00+00:00, run_id=scheduled__2023-08-04T00:00:00+00:00, run_start_date=2023-09-11 06:56:39.287544+00:00, run_end_date=2023-09-11 06:56:44.668093+00:00, run_duration=5.380549, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-04 00:00:00+00:00, data_interval_end=2023-08-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:44.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-05T00:00:00+00:00, run_after=2023-08-06T00:00:00+00:00[0m
[[34m2023-09-11T06:56:45.955+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-06T00:00:00+00:00, run_after=2023-08-07T00:00:00+00:00[0m
[[34m2023-09-11T06:56:46.003+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:46.004+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:46.004+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:46.008+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:46.009+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:46.010+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:46.012+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:48.419+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:48.553+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:48.744+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:48.780+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:49.310+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:49.310+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:49.414+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:49.472+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:50.199+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:50.212+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:49.568042+00:00, run_end_date=2023-09-11 06:56:49.800128+00:00, run_duration=0.232086, state=success, executor_state=success, try_number=1, max_tries=0, job_id=221, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:46.006173+00:00, queued_by_job_id=2, pid=42871[0m
[[34m2023-09-11T06:56:50.556+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-07T00:00:00+00:00, run_after=2023-08-08T00:00:00+00:00[0m
[[34m2023-09-11T06:56:50.602+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-05 00:00:00+00:00: scheduled__2023-08-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:45.950544+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:50.602+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-05 00:00:00+00:00, run_id=scheduled__2023-08-05T00:00:00+00:00, run_start_date=2023-09-11 06:56:45.968014+00:00, run_end_date=2023-09-11 06:56:50.602743+00:00, run_duration=4.634729, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-05 00:00:00+00:00, data_interval_end=2023-08-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:50.608+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-06T00:00:00+00:00, run_after=2023-08-07T00:00:00+00:00[0m
[[34m2023-09-11T06:56:50.626+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:50.626+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:50.627+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:50.629+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:50.630+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:50.630+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:50.633+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:52.975+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:53.144+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:53.332+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:53.369+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:53.980+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:53.981+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:54.061+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:54.145+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:56:54.985+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:56:54.998+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:56:54.238033+00:00, run_end_date=2023-09-11 06:56:54.477784+00:00, run_duration=0.239751, state=success, executor_state=success, try_number=1, max_tries=0, job_id=222, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:50.627840+00:00, queued_by_job_id=2, pid=42878[0m
[[34m2023-09-11T06:56:55.246+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-07T00:00:00+00:00, run_after=2023-08-08T00:00:00+00:00[0m
[[34m2023-09-11T06:56:55.268+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-06 00:00:00+00:00: scheduled__2023-08-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:50.549295+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:56:55.269+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-06 00:00:00+00:00, run_id=scheduled__2023-08-06T00:00:00+00:00, run_start_date=2023-09-11 06:56:50.573532+00:00, run_end_date=2023-09-11 06:56:55.269055+00:00, run_duration=4.695523, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-06 00:00:00+00:00, data_interval_end=2023-08-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:56:55.272+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-07T00:00:00+00:00, run_after=2023-08-08T00:00:00+00:00[0m
[[34m2023-09-11T06:56:56.600+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-08T00:00:00+00:00, run_after=2023-08-09T00:00:00+00:00[0m
[[34m2023-09-11T06:56:56.653+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:56.653+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:56:56.653+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:56:56.657+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:56:56.658+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:56:56.658+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:56.661+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:56:58.910+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:56:59.070+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:59.314+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:59.356+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:56:59.852+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:56:59.852+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:56:59.932+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:56:59.998+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:00.781+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:00.792+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:00.088958+00:00, run_end_date=2023-09-11 06:57:00.346640+00:00, run_duration=0.257682, state=success, executor_state=success, try_number=1, max_tries=0, job_id=223, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:56:56.655526+00:00, queued_by_job_id=2, pid=42890[0m
[[34m2023-09-11T06:57:00.955+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-09T00:00:00+00:00, run_after=2023-08-10T00:00:00+00:00[0m
[[34m2023-09-11T06:57:00.993+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-07 00:00:00+00:00: scheduled__2023-08-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:56:56.595392+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:00.993+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-07 00:00:00+00:00, run_id=scheduled__2023-08-07T00:00:00+00:00, run_start_date=2023-09-11 06:56:56.616813+00:00, run_end_date=2023-09-11 06:57:00.993658+00:00, run_duration=4.376845, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-07 00:00:00+00:00, data_interval_end=2023-08-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:00.997+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-08T00:00:00+00:00, run_after=2023-08-09T00:00:00+00:00[0m
[[34m2023-09-11T06:57:01.012+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:01.013+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:01.013+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:01.015+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:01.015+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:01.016+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:01.018+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:02.881+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:03.008+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:03.178+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:03.210+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:03.773+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:03.774+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:03.869+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:03.950+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:04.882+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:04.897+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:04.053849+00:00, run_end_date=2023-09-11 06:57:04.335093+00:00, run_duration=0.281244, state=success, executor_state=success, try_number=1, max_tries=0, job_id=224, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:01.013984+00:00, queued_by_job_id=2, pid=42897[0m
[[34m2023-09-11T06:57:05.158+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-09T00:00:00+00:00, run_after=2023-08-10T00:00:00+00:00[0m
[[34m2023-09-11T06:57:05.181+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-08 00:00:00+00:00: scheduled__2023-08-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:00.949620+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:05.182+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-08 00:00:00+00:00, run_id=scheduled__2023-08-08T00:00:00+00:00, run_start_date=2023-09-11 06:57:00.968846+00:00, run_end_date=2023-09-11 06:57:05.182230+00:00, run_duration=4.213384, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-08 00:00:00+00:00, data_interval_end=2023-08-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:05.185+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-09T00:00:00+00:00, run_after=2023-08-10T00:00:00+00:00[0m
[[34m2023-09-11T06:57:05.959+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-10T00:00:00+00:00, run_after=2023-08-11T00:00:00+00:00[0m
[[34m2023-09-11T06:57:06.016+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:06.016+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:06.017+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:06.019+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:06.020+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:06.020+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:06.044+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:08.261+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:08.386+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:08.567+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:08.604+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:09.058+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:09.059+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:09.130+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:09.191+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:09.898+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:09.910+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:09.276833+00:00, run_end_date=2023-09-11 06:57:09.497629+00:00, run_duration=0.220796, state=success, executor_state=success, try_number=1, max_tries=0, job_id=225, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:06.017776+00:00, queued_by_job_id=2, pid=42907[0m
[[34m2023-09-11T06:57:10.074+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-11T00:00:00+00:00, run_after=2023-08-12T00:00:00+00:00[0m
[[34m2023-09-11T06:57:10.108+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-09 00:00:00+00:00: scheduled__2023-08-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:05.952105+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:10.108+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-09 00:00:00+00:00, run_id=scheduled__2023-08-09T00:00:00+00:00, run_start_date=2023-09-11 06:57:05.975550+00:00, run_end_date=2023-09-11 06:57:10.108732+00:00, run_duration=4.133182, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-09 00:00:00+00:00, data_interval_end=2023-08-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:10.112+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-10T00:00:00+00:00, run_after=2023-08-11T00:00:00+00:00[0m
[[34m2023-09-11T06:57:10.128+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:10.128+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:10.128+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:10.131+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:10.131+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:10.132+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:10.134+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:12.046+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:12.187+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:12.365+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:12.396+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:12.867+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:12.868+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:12.939+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:12.994+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:13.724+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:13.735+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:13.078541+00:00, run_end_date=2023-09-11 06:57:13.296303+00:00, run_duration=0.217762, state=success, executor_state=success, try_number=1, max_tries=0, job_id=226, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:10.129676+00:00, queued_by_job_id=2, pid=42916[0m
[[34m2023-09-11T06:57:13.910+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-11T00:00:00+00:00, run_after=2023-08-12T00:00:00+00:00[0m
[[34m2023-09-11T06:57:13.933+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-10 00:00:00+00:00: scheduled__2023-08-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:10.068148+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:13.933+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-10 00:00:00+00:00, run_id=scheduled__2023-08-10T00:00:00+00:00, run_start_date=2023-09-11 06:57:10.086144+00:00, run_end_date=2023-09-11 06:57:13.933870+00:00, run_duration=3.847726, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-10 00:00:00+00:00, data_interval_end=2023-08-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:13.937+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-11T00:00:00+00:00, run_after=2023-08-12T00:00:00+00:00[0m
[[34m2023-09-11T06:57:15.207+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-12T00:00:00+00:00, run_after=2023-08-13T00:00:00+00:00[0m
[[34m2023-09-11T06:57:15.251+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:15.251+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:15.251+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-11T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:15.254+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:15.254+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:15.254+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:15.257+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:17.267+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:17.427+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:17.632+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:17.670+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:18.349+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:18.350+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:18.427+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-11T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:18.491+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-11T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:19.482+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:19.498+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-11T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:18.594472+00:00, run_end_date=2023-09-11 06:57:18.912499+00:00, run_duration=0.318027, state=success, executor_state=success, try_number=1, max_tries=0, job_id=227, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:15.252471+00:00, queued_by_job_id=2, pid=42926[0m
[[34m2023-09-11T06:57:19.832+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-13T00:00:00+00:00, run_after=2023-08-14T00:00:00+00:00[0m
[[34m2023-09-11T06:57:19.876+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-11 00:00:00+00:00: scheduled__2023-08-11T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:15.202788+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:19.877+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-11 00:00:00+00:00, run_id=scheduled__2023-08-11T00:00:00+00:00, run_start_date=2023-09-11 06:57:15.220449+00:00, run_end_date=2023-09-11 06:57:19.877181+00:00, run_duration=4.656732, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-11 00:00:00+00:00, data_interval_end=2023-08-12 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:19.880+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-12T00:00:00+00:00, run_after=2023-08-13T00:00:00+00:00[0m
[[34m2023-09-11T06:57:19.897+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:19.898+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:19.899+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-12T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:19.905+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:19.906+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:19.906+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:19.908+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:22.048+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:22.238+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:22.440+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:22.475+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:22.965+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:22.965+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:23.037+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-12T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:23.096+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-12T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:23.858+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:23.869+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-12T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:23.190204+00:00, run_end_date=2023-09-11 06:57:23.450985+00:00, run_duration=0.260781, state=success, executor_state=success, try_number=1, max_tries=0, job_id=228, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:19.901288+00:00, queued_by_job_id=2, pid=42935[0m
[[34m2023-09-11T06:57:24.314+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-13T00:00:00+00:00, run_after=2023-08-14T00:00:00+00:00[0m
[[34m2023-09-11T06:57:24.339+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-12 00:00:00+00:00: scheduled__2023-08-12T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:19.825343+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:24.340+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-12 00:00:00+00:00, run_id=scheduled__2023-08-12T00:00:00+00:00, run_start_date=2023-09-11 06:57:19.848267+00:00, run_end_date=2023-09-11 06:57:24.339935+00:00, run_duration=4.491668, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-12 00:00:00+00:00, data_interval_end=2023-08-13 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:24.343+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-13T00:00:00+00:00, run_after=2023-08-14T00:00:00+00:00[0m
[[34m2023-09-11T06:57:25.187+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-14T00:00:00+00:00, run_after=2023-08-15T00:00:00+00:00[0m
[[34m2023-09-11T06:57:25.244+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:25.245+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:25.245+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-13T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:25.247+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:25.248+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:25.248+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:25.251+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:27.379+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:27.527+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:27.782+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:27.825+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:28.650+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:28.651+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:28.727+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-13T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:28.788+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-13T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:29.796+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:29.807+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-13T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:28.924348+00:00, run_end_date=2023-09-11 06:57:29.257682+00:00, run_duration=0.333334, state=success, executor_state=success, try_number=1, max_tries=0, job_id=229, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:25.246244+00:00, queued_by_job_id=2, pid=42945[0m
[[34m2023-09-11T06:57:30.285+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-15T00:00:00+00:00, run_after=2023-08-16T00:00:00+00:00[0m
[[34m2023-09-11T06:57:30.345+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-13 00:00:00+00:00: scheduled__2023-08-13T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:25.182345+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:30.346+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-13 00:00:00+00:00, run_id=scheduled__2023-08-13T00:00:00+00:00, run_start_date=2023-09-11 06:57:25.211806+00:00, run_end_date=2023-09-11 06:57:30.346234+00:00, run_duration=5.134428, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-13 00:00:00+00:00, data_interval_end=2023-08-14 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:30.352+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-14T00:00:00+00:00, run_after=2023-08-15T00:00:00+00:00[0m
[[34m2023-09-11T06:57:30.373+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:30.373+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:30.374+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-14T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:30.377+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:30.378+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:30.378+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:30.381+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:32.500+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:32.632+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:32.865+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:32.899+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:33.440+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:33.441+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:33.514+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-14T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:33.573+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-14T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:34.314+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:34.329+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-14T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:33.662626+00:00, run_end_date=2023-09-11 06:57:33.882916+00:00, run_duration=0.22029, state=success, executor_state=success, try_number=1, max_tries=0, job_id=230, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:30.375236+00:00, queued_by_job_id=2, pid=42954[0m
[[34m2023-09-11T06:57:34.582+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-15T00:00:00+00:00, run_after=2023-08-16T00:00:00+00:00[0m
[[34m2023-09-11T06:57:34.609+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-14 00:00:00+00:00: scheduled__2023-08-14T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:30.277097+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:34.609+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-14 00:00:00+00:00, run_id=scheduled__2023-08-14T00:00:00+00:00, run_start_date=2023-09-11 06:57:30.313677+00:00, run_end_date=2023-09-11 06:57:34.609787+00:00, run_duration=4.29611, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-14 00:00:00+00:00, data_interval_end=2023-08-15 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:34.614+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-15T00:00:00+00:00, run_after=2023-08-16T00:00:00+00:00[0m
[[34m2023-09-11T06:57:35.073+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-16T00:00:00+00:00, run_after=2023-08-17T00:00:00+00:00[0m
[[34m2023-09-11T06:57:35.121+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:35.122+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:35.122+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-15T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:35.124+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:35.125+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:35.125+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:35.128+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:37.185+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:37.327+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:37.539+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:37.572+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:38.088+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:38.089+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:38.162+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-15T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:38.231+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-15T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:39.036+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:39.047+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-15T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:38.332293+00:00, run_end_date=2023-09-11 06:57:38.586419+00:00, run_duration=0.254126, state=success, executor_state=success, try_number=1, max_tries=0, job_id=231, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:35.123135+00:00, queued_by_job_id=2, pid=42964[0m
[[34m2023-09-11T06:57:39.326+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-17T00:00:00+00:00, run_after=2023-08-18T00:00:00+00:00[0m
[[34m2023-09-11T06:57:39.361+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-15 00:00:00+00:00: scheduled__2023-08-15T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:35.067298+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:39.362+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-15 00:00:00+00:00, run_id=scheduled__2023-08-15T00:00:00+00:00, run_start_date=2023-09-11 06:57:35.085451+00:00, run_end_date=2023-09-11 06:57:39.361999+00:00, run_duration=4.276548, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-15 00:00:00+00:00, data_interval_end=2023-08-16 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:39.367+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-16T00:00:00+00:00, run_after=2023-08-17T00:00:00+00:00[0m
[[34m2023-09-11T06:57:39.385+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:39.386+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:39.387+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-16T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:39.396+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:39.397+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:39.397+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:39.400+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:41.630+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:41.782+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:41.981+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:42.017+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:42.672+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:42.674+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:42.760+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-16T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:42.825+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-16T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:43.860+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:43.871+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-16T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:42.985217+00:00, run_end_date=2023-09-11 06:57:43.289337+00:00, run_duration=0.30412, state=success, executor_state=success, try_number=1, max_tries=0, job_id=232, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:39.394458+00:00, queued_by_job_id=2, pid=42973[0m
[[34m2023-09-11T06:57:44.563+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-17T00:00:00+00:00, run_after=2023-08-18T00:00:00+00:00[0m
[[34m2023-09-11T06:57:44.614+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-16 00:00:00+00:00: scheduled__2023-08-16T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:39.321634+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:44.614+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-16 00:00:00+00:00, run_id=scheduled__2023-08-16T00:00:00+00:00, run_start_date=2023-09-11 06:57:39.338677+00:00, run_end_date=2023-09-11 06:57:44.614845+00:00, run_duration=5.276168, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-16 00:00:00+00:00, data_interval_end=2023-08-17 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:44.620+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-17T00:00:00+00:00, run_after=2023-08-18T00:00:00+00:00[0m
[[34m2023-09-11T06:57:46.144+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-18T00:00:00+00:00, run_after=2023-08-19T00:00:00+00:00[0m
[[34m2023-09-11T06:57:46.227+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:46.227+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:46.227+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-17T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:46.229+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:46.230+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:46.230+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:46.233+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:48.282+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:48.456+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:48.641+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:48.677+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:49.187+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:49.187+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:49.260+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-17T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:49.319+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-17T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:50.215+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:50.229+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-17T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:49.428399+00:00, run_end_date=2023-09-11 06:57:49.715193+00:00, run_duration=0.286794, state=success, executor_state=success, try_number=1, max_tries=0, job_id=233, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:46.228411+00:00, queued_by_job_id=2, pid=42983[0m
[[34m2023-09-11T06:57:50.590+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-19T00:00:00+00:00, run_after=2023-08-20T00:00:00+00:00[0m
[[34m2023-09-11T06:57:50.625+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-17 00:00:00+00:00: scheduled__2023-08-17T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:46.139979+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:50.625+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-17 00:00:00+00:00, run_id=scheduled__2023-08-17T00:00:00+00:00, run_start_date=2023-09-11 06:57:46.192554+00:00, run_end_date=2023-09-11 06:57:50.625441+00:00, run_duration=4.432887, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-17 00:00:00+00:00, data_interval_end=2023-08-18 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:50.628+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-18T00:00:00+00:00, run_after=2023-08-19T00:00:00+00:00[0m
[[34m2023-09-11T06:57:50.643+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:50.643+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:50.643+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-18T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:50.645+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:50.646+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:50.646+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:50.649+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:52.930+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:53.059+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:53.266+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:53.315+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:53.821+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:57:53.822+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:53.894+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-18T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:57:53.950+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-18T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:57:55.015+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:57:55.028+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-18T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:57:54.112341+00:00, run_end_date=2023-09-11 06:57:54.522926+00:00, run_duration=0.410585, state=success, executor_state=success, try_number=1, max_tries=0, job_id=234, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:50.644318+00:00, queued_by_job_id=2, pid=42992[0m
[[34m2023-09-11T06:57:55.477+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-19T00:00:00+00:00, run_after=2023-08-20T00:00:00+00:00[0m
[[34m2023-09-11T06:57:55.504+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-18 00:00:00+00:00: scheduled__2023-08-18T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:50.585076+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:57:55.505+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-18 00:00:00+00:00, run_id=scheduled__2023-08-18T00:00:00+00:00, run_start_date=2023-09-11 06:57:50.602626+00:00, run_end_date=2023-09-11 06:57:55.505020+00:00, run_duration=4.902394, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-18 00:00:00+00:00, data_interval_end=2023-08-19 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:57:55.510+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-19T00:00:00+00:00, run_after=2023-08-20T00:00:00+00:00[0m
[[34m2023-09-11T06:57:56.989+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-20T00:00:00+00:00, run_after=2023-08-21T00:00:00+00:00[0m
[[34m2023-09-11T06:57:57.051+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:57.052+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:57:57.053+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-19T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:57:57.056+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:57:57.057+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:57:57.057+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:57.060+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:57:59.142+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:57:59.270+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:57:59.435+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:57:59.469+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:00.076+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:00.078+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:00.187+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-19T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:00.257+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-19T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:00.985+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:00.998+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-19T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:00.344518+00:00, run_end_date=2023-09-11 06:58:00.578565+00:00, run_duration=0.234047, state=success, executor_state=success, try_number=1, max_tries=0, job_id=235, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:57:57.054503+00:00, queued_by_job_id=2, pid=43004[0m
[[34m2023-09-11T06:58:01.519+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-21T00:00:00+00:00, run_after=2023-08-22T00:00:00+00:00[0m
[[34m2023-09-11T06:58:01.571+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-19 00:00:00+00:00: scheduled__2023-08-19T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:57:56.981565+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:01.572+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-19 00:00:00+00:00, run_id=scheduled__2023-08-19T00:00:00+00:00, run_start_date=2023-09-11 06:57:57.007159+00:00, run_end_date=2023-09-11 06:58:01.572277+00:00, run_duration=4.565118, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-19 00:00:00+00:00, data_interval_end=2023-08-20 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:01.578+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-20T00:00:00+00:00, run_after=2023-08-21T00:00:00+00:00[0m
[[34m2023-09-11T06:58:01.599+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:01.600+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:01.600+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-20T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:01.604+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:01.605+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:01.605+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:01.608+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:03.885+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:04.015+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:04.194+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:04.234+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:04.792+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:04.792+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:04.866+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-20T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:04.930+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-20T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:05.820+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:05.833+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-20T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:05.046235+00:00, run_end_date=2023-09-11 06:58:05.362739+00:00, run_duration=0.316504, state=success, executor_state=success, try_number=1, max_tries=0, job_id=236, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:01.602056+00:00, queued_by_job_id=2, pid=43013[0m
[[34m2023-09-11T06:58:06.194+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-21T00:00:00+00:00, run_after=2023-08-22T00:00:00+00:00[0m
[[34m2023-09-11T06:58:06.219+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-20 00:00:00+00:00: scheduled__2023-08-20T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:01.511915+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:06.220+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-20 00:00:00+00:00, run_id=scheduled__2023-08-20T00:00:00+00:00, run_start_date=2023-09-11 06:58:01.537388+00:00, run_end_date=2023-09-11 06:58:06.220493+00:00, run_duration=4.683105, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-20 00:00:00+00:00, data_interval_end=2023-08-21 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:06.224+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-21T00:00:00+00:00, run_after=2023-08-22T00:00:00+00:00[0m
[[34m2023-09-11T06:58:07.713+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-22T00:00:00+00:00, run_after=2023-08-23T00:00:00+00:00[0m
[[34m2023-09-11T06:58:07.756+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:07.756+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:07.756+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-21T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:07.758+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:07.759+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:07.759+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:07.762+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:09.953+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:10.172+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:10.380+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:10.424+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:10.973+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:10.973+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:11.057+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-21T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:11.119+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-21T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:12.037+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:12.053+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-21T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:11.214721+00:00, run_end_date=2023-09-11 06:58:11.527253+00:00, run_duration=0.312532, state=success, executor_state=success, try_number=1, max_tries=0, job_id=237, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:07.757270+00:00, queued_by_job_id=2, pid=43025[0m
[[34m2023-09-11T06:58:12.537+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-23T00:00:00+00:00, run_after=2023-08-24T00:00:00+00:00[0m
[[34m2023-09-11T06:58:12.577+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-21 00:00:00+00:00: scheduled__2023-08-21T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:07.708990+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:12.578+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-21 00:00:00+00:00, run_id=scheduled__2023-08-21T00:00:00+00:00, run_start_date=2023-09-11 06:58:07.725960+00:00, run_end_date=2023-09-11 06:58:12.577975+00:00, run_duration=4.852015, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-21 00:00:00+00:00, data_interval_end=2023-08-22 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:12.582+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-22T00:00:00+00:00, run_after=2023-08-23T00:00:00+00:00[0m
[[34m2023-09-11T06:58:12.598+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:12.599+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:12.599+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-22T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:12.602+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:12.603+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:12.603+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:12.606+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:14.559+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:14.688+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:14.856+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:14.888+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:15.361+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:15.362+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:15.441+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-22T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:15.500+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-22T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:16.206+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:16.217+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-22T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:15.587469+00:00, run_end_date=2023-09-11 06:58:15.813048+00:00, run_duration=0.225579, state=success, executor_state=success, try_number=1, max_tries=0, job_id=238, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:12.600377+00:00, queued_by_job_id=2, pid=43033[0m
[[34m2023-09-11T06:58:16.573+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-23T00:00:00+00:00, run_after=2023-08-24T00:00:00+00:00[0m
[[34m2023-09-11T06:58:16.596+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-22 00:00:00+00:00: scheduled__2023-08-22T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:12.531209+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:16.596+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-22 00:00:00+00:00, run_id=scheduled__2023-08-22T00:00:00+00:00, run_start_date=2023-09-11 06:58:12.550785+00:00, run_end_date=2023-09-11 06:58:16.596489+00:00, run_duration=4.045704, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-22 00:00:00+00:00, data_interval_end=2023-08-23 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:16.600+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-23T00:00:00+00:00, run_after=2023-08-24T00:00:00+00:00[0m
[[34m2023-09-11T06:58:17.652+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-24T00:00:00+00:00, run_after=2023-08-25T00:00:00+00:00[0m
[[34m2023-09-11T06:58:17.707+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:17.707+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:17.707+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-23T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:17.710+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:17.711+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:17.711+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:17.714+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:19.748+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:19.875+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:20.055+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:20.087+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:20.561+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:20.562+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:20.645+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-23T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:20.713+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-23T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:21.451+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:21.463+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-23T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:20.812383+00:00, run_end_date=2023-09-11 06:58:21.063065+00:00, run_duration=0.250682, state=success, executor_state=success, try_number=1, max_tries=0, job_id=239, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:17.708560+00:00, queued_by_job_id=2, pid=43043[0m
[[34m2023-09-11T06:58:21.821+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-25T00:00:00+00:00, run_after=2023-08-26T00:00:00+00:00[0m
[[34m2023-09-11T06:58:21.857+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-23 00:00:00+00:00: scheduled__2023-08-23T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:17.647047+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:21.857+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-23 00:00:00+00:00, run_id=scheduled__2023-08-23T00:00:00+00:00, run_start_date=2023-09-11 06:58:17.668516+00:00, run_end_date=2023-09-11 06:58:21.857683+00:00, run_duration=4.189167, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-23 00:00:00+00:00, data_interval_end=2023-08-24 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:21.861+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-24T00:00:00+00:00, run_after=2023-08-25T00:00:00+00:00[0m
[[34m2023-09-11T06:58:21.876+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:21.876+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:21.876+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-24T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:21.878+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:21.879+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:21.879+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:21.882+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:23.966+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:24.127+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:24.346+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:24.396+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:24.956+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:24.957+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:25.025+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-24T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:25.084+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-24T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:25.806+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:25.817+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-24T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:25.176775+00:00, run_end_date=2023-09-11 06:58:25.408029+00:00, run_duration=0.231254, state=success, executor_state=success, try_number=1, max_tries=0, job_id=240, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:21.877409+00:00, queued_by_job_id=2, pid=43052[0m
[[34m2023-09-11T06:58:26.027+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-25T00:00:00+00:00, run_after=2023-08-26T00:00:00+00:00[0m
[[34m2023-09-11T06:58:26.049+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-24 00:00:00+00:00: scheduled__2023-08-24T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:21.816035+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:26.050+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-24 00:00:00+00:00, run_id=scheduled__2023-08-24T00:00:00+00:00, run_start_date=2023-09-11 06:58:21.834676+00:00, run_end_date=2023-09-11 06:58:26.050413+00:00, run_duration=4.215737, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-24 00:00:00+00:00, data_interval_end=2023-08-25 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:26.053+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-25T00:00:00+00:00, run_after=2023-08-26T00:00:00+00:00[0m
[[34m2023-09-11T06:58:26.680+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-26T00:00:00+00:00, run_after=2023-08-27T00:00:00+00:00[0m
[[34m2023-09-11T06:58:26.741+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:26.741+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:26.741+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-25T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:26.743+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:26.744+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:26.745+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:26.747+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:28.584+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:28.717+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:28.880+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:28.920+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:29.386+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:29.386+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:29.455+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-25T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:29.514+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-25T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:30.233+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:30.243+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-25T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:29.603366+00:00, run_end_date=2023-09-11 06:58:29.816618+00:00, run_duration=0.213252, state=success, executor_state=success, try_number=1, max_tries=0, job_id=241, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:26.742341+00:00, queued_by_job_id=2, pid=43060[0m
[[34m2023-09-11T06:58:30.603+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-27T00:00:00+00:00, run_after=2023-08-28T00:00:00+00:00[0m
[[34m2023-09-11T06:58:30.643+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-25 00:00:00+00:00: scheduled__2023-08-25T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:26.675410+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:30.644+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-25 00:00:00+00:00, run_id=scheduled__2023-08-25T00:00:00+00:00, run_start_date=2023-09-11 06:58:26.708844+00:00, run_end_date=2023-09-11 06:58:30.644001+00:00, run_duration=3.935157, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-25 00:00:00+00:00, data_interval_end=2023-08-26 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:30.649+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-26T00:00:00+00:00, run_after=2023-08-27T00:00:00+00:00[0m
[[34m2023-09-11T06:58:30.669+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:30.670+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:30.670+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:30.673+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:30.673+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:30.673+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:30.676+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:32.547+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:32.674+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:32.870+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:32.901+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:33.361+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:33.361+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:33.434+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-26T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:33.490+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-26T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:34.206+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:34.217+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-26T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:33.574419+00:00, run_end_date=2023-09-11 06:58:33.789032+00:00, run_duration=0.214613, state=success, executor_state=success, try_number=1, max_tries=0, job_id=242, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:30.671589+00:00, queued_by_job_id=2, pid=43069[0m
[[34m2023-09-11T06:58:34.942+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-27T00:00:00+00:00, run_after=2023-08-28T00:00:00+00:00[0m
[[34m2023-09-11T06:58:34.970+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-26 00:00:00+00:00: scheduled__2023-08-26T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:30.598161+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:34.971+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-26 00:00:00+00:00, run_id=scheduled__2023-08-26T00:00:00+00:00, run_start_date=2023-09-11 06:58:30.615691+00:00, run_end_date=2023-09-11 06:58:34.971064+00:00, run_duration=4.355373, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-26 00:00:00+00:00, data_interval_end=2023-08-27 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:34.975+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-27T00:00:00+00:00, run_after=2023-08-28T00:00:00+00:00[0m
[[34m2023-09-11T06:58:35.907+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-28T00:00:00+00:00, run_after=2023-08-29T00:00:00+00:00[0m
[[34m2023-09-11T06:58:35.954+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:35.954+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:35.954+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-27T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:35.957+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:35.958+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:35.958+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:35.960+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:37.786+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:37.918+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:38.091+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:38.125+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:38.587+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:38.587+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:38.657+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-27T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:38.715+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-27T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:39.434+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:39.445+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-27T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:38.799612+00:00, run_end_date=2023-09-11 06:58:39.029149+00:00, run_duration=0.229537, state=success, executor_state=success, try_number=1, max_tries=0, job_id=243, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:35.955659+00:00, queued_by_job_id=2, pid=43079[0m
[[34m2023-09-11T06:58:39.928+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-29T00:00:00+00:00, run_after=2023-08-30T00:00:00+00:00[0m
[[34m2023-09-11T06:58:39.968+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-27 00:00:00+00:00: scheduled__2023-08-27T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:35.902733+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:39.968+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-27 00:00:00+00:00, run_id=scheduled__2023-08-27T00:00:00+00:00, run_start_date=2023-09-11 06:58:35.920118+00:00, run_end_date=2023-09-11 06:58:39.968495+00:00, run_duration=4.048377, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-27 00:00:00+00:00, data_interval_end=2023-08-28 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:39.973+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-28T00:00:00+00:00, run_after=2023-08-29T00:00:00+00:00[0m
[[34m2023-09-11T06:58:39.991+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:39.991+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:39.991+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-28T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:39.994+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:39.994+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:39.994+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:39.997+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:42.339+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:42.518+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:42.730+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:42.768+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:43.397+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:43.397+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:43.522+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-28T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:43.590+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-28T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:44.317+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:44.329+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-28T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:43.696714+00:00, run_end_date=2023-09-11 06:58:43.921859+00:00, run_duration=0.225145, state=success, executor_state=success, try_number=1, max_tries=0, job_id=244, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:39.992522+00:00, queued_by_job_id=2, pid=43088[0m
[[34m2023-09-11T06:58:45.044+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-29T00:00:00+00:00, run_after=2023-08-30T00:00:00+00:00[0m
[[34m2023-09-11T06:58:45.077+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-28 00:00:00+00:00: scheduled__2023-08-28T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:39.923468+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:45.078+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-28 00:00:00+00:00, run_id=scheduled__2023-08-28T00:00:00+00:00, run_start_date=2023-09-11 06:58:39.941999+00:00, run_end_date=2023-09-11 06:58:45.078517+00:00, run_duration=5.136518, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-28 00:00:00+00:00, data_interval_end=2023-08-29 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:45.084+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-29T00:00:00+00:00, run_after=2023-08-30T00:00:00+00:00[0m
[[34m2023-09-11T06:58:46.571+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-30T00:00:00+00:00, run_after=2023-08-31T00:00:00+00:00[0m
[[34m2023-09-11T06:58:46.618+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:46.618+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:46.618+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-29T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:46.620+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:46.621+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:46.621+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:46.624+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:48.571+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:48.704+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:48.869+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:48.901+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:49.423+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:49.423+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:49.496+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-29T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:49.556+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-29T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:50.339+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:50.350+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-29T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:49.642181+00:00, run_end_date=2023-09-11 06:58:49.896194+00:00, run_duration=0.254013, state=success, executor_state=success, try_number=1, max_tries=0, job_id=245, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:46.619383+00:00, queued_by_job_id=2, pid=43098[0m
[[34m2023-09-11T06:58:50.934+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-31T00:00:00+00:00, run_after=2023-09-01T00:00:00+00:00[0m
[[34m2023-09-11T06:58:50.968+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-29 00:00:00+00:00: scheduled__2023-08-29T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:46.566820+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:50.969+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-29 00:00:00+00:00, run_id=scheduled__2023-08-29T00:00:00+00:00, run_start_date=2023-09-11 06:58:46.584238+00:00, run_end_date=2023-09-11 06:58:50.969226+00:00, run_duration=4.384988, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-29 00:00:00+00:00, data_interval_end=2023-08-30 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:50.972+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-30T00:00:00+00:00, run_after=2023-08-31T00:00:00+00:00[0m
[[34m2023-09-11T06:58:50.989+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:50.989+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:50.990+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-30T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:50.992+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:50.992+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:50.993+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:50.996+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:52.914+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:53.055+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:53.262+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:53.303+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:53.813+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:58:53.813+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:53.884+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-30T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:58:53.954+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-30T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:58:55.294+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:58:55.305+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-30T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:58:54.042620+00:00, run_end_date=2023-09-11 06:58:54.292338+00:00, run_duration=0.249718, state=success, executor_state=success, try_number=1, max_tries=0, job_id=246, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:50.990721+00:00, queued_by_job_id=2, pid=43107[0m
[[34m2023-09-11T06:58:55.878+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-31T00:00:00+00:00, run_after=2023-09-01T00:00:00+00:00[0m
[[34m2023-09-11T06:58:55.905+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-30 00:00:00+00:00: scheduled__2023-08-30T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:50.928837+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:58:55.905+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-30 00:00:00+00:00, run_id=scheduled__2023-08-30T00:00:00+00:00, run_start_date=2023-09-11 06:58:50.946563+00:00, run_end_date=2023-09-11 06:58:55.905502+00:00, run_duration=4.958939, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-30 00:00:00+00:00, data_interval_end=2023-08-31 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:58:55.908+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-08-31T00:00:00+00:00, run_after=2023-09-01T00:00:00+00:00[0m
[[34m2023-09-11T06:58:57.268+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-01T00:00:00+00:00, run_after=2023-09-02T00:00:00+00:00[0m
[[34m2023-09-11T06:58:57.316+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:57.317+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:58:57.317+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-08-31T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:58:57.319+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:58:57.320+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:58:57.320+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:57.323+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:58:59.233+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:58:59.361+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:58:59.533+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:58:59.564+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:00.041+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:00.042+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:00.114+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-08-31T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:00.172+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-08-31T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:01.048+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:01.058+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-08-31T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:00.256163+00:00, run_end_date=2023-09-11 06:59:00.480089+00:00, run_duration=0.223926, state=success, executor_state=success, try_number=1, max_tries=0, job_id=247, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:58:57.318155+00:00, queued_by_job_id=2, pid=43119[0m
[[34m2023-09-11T06:59:01.329+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00[0m
[[34m2023-09-11T06:59:01.364+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-08-31 00:00:00+00:00: scheduled__2023-08-31T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:58:57.262809+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:01.364+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-08-31 00:00:00+00:00, run_id=scheduled__2023-08-31T00:00:00+00:00, run_start_date=2023-09-11 06:58:57.282677+00:00, run_end_date=2023-09-11 06:59:01.364459+00:00, run_duration=4.081782, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-31 00:00:00+00:00, data_interval_end=2023-09-01 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:01.368+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-01T00:00:00+00:00, run_after=2023-09-02T00:00:00+00:00[0m
[[34m2023-09-11T06:59:01.383+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:01.383+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:01.384+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:01.386+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:01.386+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:01.387+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:01.400+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:03.353+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:03.492+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:03.670+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:03.706+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:04.482+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:04.484+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:04.612+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-01T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:04.681+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-01T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:05.698+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:05.711+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-01T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:04.809214+00:00, run_end_date=2023-09-11 06:59:05.051655+00:00, run_duration=0.242441, state=success, executor_state=success, try_number=1, max_tries=0, job_id=248, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:01.384904+00:00, queued_by_job_id=2, pid=43126[0m
[[34m2023-09-11T06:59:05.976+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00[0m
[[34m2023-09-11T06:59:06.002+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-01 00:00:00+00:00: scheduled__2023-09-01T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:01.323766+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:06.003+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-01 00:00:00+00:00, run_id=scheduled__2023-09-01T00:00:00+00:00, run_start_date=2023-09-11 06:59:01.341818+00:00, run_end_date=2023-09-11 06:59:06.003371+00:00, run_duration=4.661553, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-01 00:00:00+00:00, data_interval_end=2023-09-02 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:06.007+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-02T00:00:00+00:00, run_after=2023-09-03T00:00:00+00:00[0m
[[34m2023-09-11T06:59:06.425+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-03T00:00:00+00:00, run_after=2023-09-04T00:00:00+00:00[0m
[[34m2023-09-11T06:59:06.470+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:06.470+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:06.470+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-02T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:06.473+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:06.474+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:06.474+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:06.479+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:08.672+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:08.871+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:09.075+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:09.119+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:09.630+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:09.631+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:09.708+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-02T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:09.774+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-02T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:10.533+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:10.544+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-02T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:09.869480+00:00, run_end_date=2023-09-11 06:59:10.104398+00:00, run_duration=0.234918, state=success, executor_state=success, try_number=1, max_tries=0, job_id=249, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:06.471419+00:00, queued_by_job_id=2, pid=43136[0m
[[34m2023-09-11T06:59:10.704+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-04T00:00:00+00:00, run_after=2023-09-05T00:00:00+00:00[0m
[[34m2023-09-11T06:59:10.743+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-02 00:00:00+00:00: scheduled__2023-09-02T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:06.420757+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:10.743+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-02 00:00:00+00:00, run_id=scheduled__2023-09-02T00:00:00+00:00, run_start_date=2023-09-11 06:59:06.438298+00:00, run_end_date=2023-09-11 06:59:10.743454+00:00, run_duration=4.305156, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-02 00:00:00+00:00, data_interval_end=2023-09-03 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:10.746+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-03T00:00:00+00:00, run_after=2023-09-04T00:00:00+00:00[0m
[[34m2023-09-11T06:59:10.762+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:10.762+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:10.763+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-03T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:10.765+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:10.766+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:10.767+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:10.770+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:12.792+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:12.934+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:13.130+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:13.166+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:13.651+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:13.652+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:13.728+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-03T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:13.796+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-03T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:14.544+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:14.555+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-03T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:13.886880+00:00, run_end_date=2023-09-11 06:59:14.118903+00:00, run_duration=0.232023, state=success, executor_state=success, try_number=1, max_tries=0, job_id=250, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:10.763773+00:00, queued_by_job_id=2, pid=43145[0m
[[34m2023-09-11T06:59:14.805+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-04T00:00:00+00:00, run_after=2023-09-05T00:00:00+00:00[0m
[[34m2023-09-11T06:59:14.828+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-03 00:00:00+00:00: scheduled__2023-09-03T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:10.699534+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:14.828+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-03 00:00:00+00:00, run_id=scheduled__2023-09-03T00:00:00+00:00, run_start_date=2023-09-11 06:59:10.719041+00:00, run_end_date=2023-09-11 06:59:14.828729+00:00, run_duration=4.109688, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-03 00:00:00+00:00, data_interval_end=2023-09-04 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:14.832+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-04T00:00:00+00:00, run_after=2023-09-05T00:00:00+00:00[0m
[[34m2023-09-11T06:59:15.701+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-05T00:00:00+00:00, run_after=2023-09-06T00:00:00+00:00[0m
[[34m2023-09-11T06:59:15.749+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:15.749+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:15.750+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-04T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:15.752+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:15.753+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:15.753+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:15.756+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:17.704+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:17.845+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:18.041+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:18.077+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:18.591+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:18.591+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:18.668+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-04T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:18.728+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-04T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:19.452+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:19.464+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-04T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:18.817067+00:00, run_end_date=2023-09-11 06:59:19.045019+00:00, run_duration=0.227952, state=success, executor_state=success, try_number=1, max_tries=0, job_id=251, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:15.750913+00:00, queued_by_job_id=2, pid=43155[0m
[[34m2023-09-11T06:59:19.632+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-06T00:00:00+00:00, run_after=2023-09-07T00:00:00+00:00[0m
[[34m2023-09-11T06:59:19.668+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-04 00:00:00+00:00: scheduled__2023-09-04T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:15.695198+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:19.669+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-04 00:00:00+00:00, run_id=scheduled__2023-09-04T00:00:00+00:00, run_start_date=2023-09-11 06:59:15.714745+00:00, run_end_date=2023-09-11 06:59:19.669232+00:00, run_duration=3.954487, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-04 00:00:00+00:00, data_interval_end=2023-09-05 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:19.673+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-05T00:00:00+00:00, run_after=2023-09-06T00:00:00+00:00[0m
[[34m2023-09-11T06:59:19.689+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:19.690+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:19.690+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-05T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:19.692+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:19.693+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:19.693+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:19.696+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:21.701+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:21.839+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:22.041+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:22.078+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:22.563+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:22.564+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:22.638+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-05T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:22.697+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-05T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:23.461+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:23.473+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-05T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:22.786616+00:00, run_end_date=2023-09-11 06:59:23.034985+00:00, run_duration=0.248369, state=success, executor_state=success, try_number=1, max_tries=0, job_id=252, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:19.691180+00:00, queued_by_job_id=2, pid=43164[0m
[[34m2023-09-11T06:59:23.646+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-06T00:00:00+00:00, run_after=2023-09-07T00:00:00+00:00[0m
[[34m2023-09-11T06:59:23.670+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-05 00:00:00+00:00: scheduled__2023-09-05T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:19.626682+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:23.671+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-05 00:00:00+00:00, run_id=scheduled__2023-09-05T00:00:00+00:00, run_start_date=2023-09-11 06:59:19.645012+00:00, run_end_date=2023-09-11 06:59:23.671179+00:00, run_duration=4.026167, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-05 00:00:00+00:00, data_interval_end=2023-09-06 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:23.675+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-06T00:00:00+00:00, run_after=2023-09-07T00:00:00+00:00[0m
[[34m2023-09-11T06:59:24.658+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00[0m
[[34m2023-09-11T06:59:24.708+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:24.708+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:24.708+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-06T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:24.711+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:24.711+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:24.711+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:24.715+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:26.722+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:26.873+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:27.079+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:27.120+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:27.674+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:27.674+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:27.754+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-06T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:27.814+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-06T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:28.625+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:28.637+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-06T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:27.905557+00:00, run_end_date=2023-09-11 06:59:28.153444+00:00, run_duration=0.247887, state=success, executor_state=success, try_number=1, max_tries=0, job_id=253, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:24.709369+00:00, queued_by_job_id=2, pid=43174[0m
[[34m2023-09-11T06:59:28.908+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00[0m
[[34m2023-09-11T06:59:28.963+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-06 00:00:00+00:00: scheduled__2023-09-06T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:24.652383+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:28.964+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-06 00:00:00+00:00, run_id=scheduled__2023-09-06T00:00:00+00:00, run_start_date=2023-09-11 06:59:24.672294+00:00, run_end_date=2023-09-11 06:59:28.963979+00:00, run_duration=4.291685, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-06 00:00:00+00:00, data_interval_end=2023-09-07 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:28.968+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-07T00:00:00+00:00, run_after=2023-09-08T00:00:00+00:00[0m
[[34m2023-09-11T06:59:28.983+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:28.984+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:28.984+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-07T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:28.986+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:28.987+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:28.987+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:28.990+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:31.008+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:31.147+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:31.328+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:31.364+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:31.882+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:31.882+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:31.959+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-07T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:32.021+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-07T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:32.816+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:32.827+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-07T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:32.112721+00:00, run_end_date=2023-09-11 06:59:32.350579+00:00, run_duration=0.237858, state=success, executor_state=success, try_number=1, max_tries=0, job_id=254, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:28.985015+00:00, queued_by_job_id=2, pid=43183[0m
[[34m2023-09-11T06:59:33.074+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00[0m
[[34m2023-09-11T06:59:33.098+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-07 00:00:00+00:00: scheduled__2023-09-07T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:28.903434+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:33.099+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-07 00:00:00+00:00, run_id=scheduled__2023-09-07T00:00:00+00:00, run_start_date=2023-09-11 06:59:28.922245+00:00, run_end_date=2023-09-11 06:59:33.099333+00:00, run_duration=4.177088, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-07 00:00:00+00:00, data_interval_end=2023-09-08 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:33.105+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-08T00:00:00+00:00, run_after=2023-09-09T00:00:00+00:00[0m
[[34m2023-09-11T06:59:33.992+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-09T00:00:00+00:00, run_after=2023-09-10T00:00:00+00:00[0m
[[34m2023-09-11T06:59:34.041+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:34.041+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:34.041+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-08T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:34.043+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:34.044+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:34.045+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:34.048+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:36.086+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:36.234+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:36.415+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:36.451+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:36.960+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:36.961+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:37.039+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-08T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:37.105+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-08T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:37.894+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:37.905+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-08T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:37.199637+00:00, run_end_date=2023-09-11 06:59:37.454572+00:00, run_duration=0.254935, state=success, executor_state=success, try_number=1, max_tries=0, job_id=255, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:34.042596+00:00, queued_by_job_id=2, pid=43193[0m
[[34m2023-09-11T06:59:38.164+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00[0m
[[34m2023-09-11T06:59:38.202+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-08 00:00:00+00:00: scheduled__2023-09-08T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:33.988047+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:38.203+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-08 00:00:00+00:00, run_id=scheduled__2023-09-08T00:00:00+00:00, run_start_date=2023-09-11 06:59:34.007064+00:00, run_end_date=2023-09-11 06:59:38.202993+00:00, run_duration=4.195929, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-08 00:00:00+00:00, data_interval_end=2023-09-09 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:38.206+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-09T00:00:00+00:00, run_after=2023-09-10T00:00:00+00:00[0m
[[34m2023-09-11T06:59:38.222+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:38.223+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:38.223+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-09T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:38.225+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:38.226+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:38.226+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:38.229+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:40.157+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:40.295+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:40.490+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:40.524+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:41.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:41.010+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:41.088+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-09T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:41.159+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-09T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:41.977+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:41.990+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-09T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:41.251785+00:00, run_end_date=2023-09-11 06:59:41.491433+00:00, run_duration=0.239648, state=success, executor_state=success, try_number=1, max_tries=0, job_id=256, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:38.224155+00:00, queued_by_job_id=2, pid=43200[0m
[[34m2023-09-11T06:59:42.239+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00[0m
[[34m2023-09-11T06:59:42.265+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-09 00:00:00+00:00: scheduled__2023-09-09T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:38.158972+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:42.265+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-09 00:00:00+00:00, run_id=scheduled__2023-09-09T00:00:00+00:00, run_start_date=2023-09-11 06:59:38.178530+00:00, run_end_date=2023-09-11 06:59:42.265608+00:00, run_duration=4.087078, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-09 00:00:00+00:00, data_interval_end=2023-09-10 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:42.270+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-10T00:00:00+00:00, run_after=2023-09-11T00:00:00+00:00[0m
[[34m2023-09-11T06:59:43.166+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-11T00:00:00+00:00, run_after=2023-09-12T00:00:00+00:00[0m
[[34m2023-09-11T06:59:43.211+0000[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:43.212+0000[0m] {[34mscheduler_job_runner.py:[0m477} INFO[0m - DAG etl_workflow has 0/16 running and queued tasks[0m
[[34m2023-09-11T06:59:43.212+0000[0m] {[34mscheduler_job_runner.py:[0m593} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_workflow.Random_number scheduled__2023-09-10T00:00:00+00:00 [scheduled]>[0m
[[34m2023-09-11T06:59:43.214+0000[0m] {[34mtaskinstance.py:[0m1439} WARNING[0m - cannot record scheduled_duration for task Random_number because previous state change time has not been saved[0m
[[34m2023-09-11T06:59:43.215+0000[0m] {[34mscheduler_job_runner.py:[0m636} INFO[0m - Sending TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-09-11T06:59:43.215+0000[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:43.218+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_workflow', 'Random_number', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/0-first_dag.py'][0m
[[34m2023-09-11T06:59:45.129+0000[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /root/airflow/dags/0-first_dag.py[0m
[[34m2023-09-11T06:59:45.291+0000[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:45.507+0000[0m] {[34mexample_python_operator.py:[0m89} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:45.542+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2023-09-11T06:59:46.023+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-09-11T06:59:46.024+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-09-11T06:59:46.103+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
Changing /root/airflow/logs/dag_id=etl_workflow/run_id=scheduled__2023-09-10T00:00:00+00:00/task_id=Random_number permission to 509
[[34m2023-09-11T06:59:46.177+0000[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: etl_workflow.Random_number scheduled__2023-09-10T00:00:00+00:00 [queued]> on host ubuntu-focal[0m
[[34m2023-09-11T06:59:46.969+0000[0m] {[34mscheduler_job_runner.py:[0m686} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_workflow', task_id='Random_number', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2023-09-11T06:59:46.982+0000[0m] {[34mscheduler_job_runner.py:[0m723} INFO[0m - TaskInstance Finished: dag_id=etl_workflow, task_id=Random_number, run_id=scheduled__2023-09-10T00:00:00+00:00, map_index=-1, run_start_date=2023-09-11 06:59:46.272386+00:00, run_end_date=2023-09-11 06:59:46.528545+00:00, run_duration=0.256159, state=success, executor_state=success, try_number=1, max_tries=0, job_id=257, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-09-11 06:59:43.212994+00:00, queued_by_job_id=2, pid=43212[0m
[[34m2023-09-11T06:59:47.275+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun etl_workflow @ 2023-09-10 00:00:00+00:00: scheduled__2023-09-10T00:00:00+00:00, state:running, queued_at: 2023-09-11 06:59:43.161610+00:00. externally triggered: False> successful[0m
[[34m2023-09-11T06:59:47.275+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=etl_workflow, execution_date=2023-09-10 00:00:00+00:00, run_id=scheduled__2023-09-10T00:00:00+00:00, run_start_date=2023-09-11 06:59:43.179052+00:00, run_end_date=2023-09-11 06:59:47.275521+00:00, run_duration=4.096469, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-10 00:00:00+00:00, data_interval_end=2023-09-11 00:00:00+00:00, dag_hash=4357bf60e57abc991dba8b901e2ba260[0m
[[34m2023-09-11T06:59:47.279+0000[0m] {[34mdag.py:[0m3696} INFO[0m - Setting next_dagrun for etl_workflow to 2023-09-11T00:00:00+00:00, run_after=2023-09-12T00:00:00+00:00[0m
[[34m2023-09-11T07:00:01.385+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:05:01.671+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:10:02.072+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:15:02.245+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:20:02.350+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:25:02.630+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:30:02.973+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:35:03.294+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:40:03.558+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2023-09-11T07:45:03.724+0000[0m] {[34mscheduler_job_runner.py:[0m1586} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
